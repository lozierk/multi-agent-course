# -*- coding: utf-8 -*-
"""speculative-decoding.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1c8_JIRnNWZxH0U-oo7RsXy_x_Ps70dxR
"""

# This notebook demonstrates Speculative Decoding, a technique to speed up large language model (LLM) inference.
# It involves using a smaller, faster "draft" model to generate a sequence of tokens, which is then verified
# by a larger, more accurate "main" model. If the draft is accurate, the main model can accept multiple tokens
# at once, rather than generating them one by one, leading to significant speed improvements.

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed

# Set a random seed for reproducibility of results
set_seed(42)

# Determine the device (GPU or CPU) to run the models on
device = "cuda" if torch.cuda.is_available() else "cpu"

# Load the smaller Gemma2 model (2B parameters) for draft generation
# device_map="auto" automatically distributes the model across available devices (e.g., GPU).
# torch_dtype=torch.bfloat16 uses bfloat16 precision for faster computation and lower memory usage.
small_tokenizer = AutoTokenizer.from_pretrained("google/gemma-2-2b-it", device_map="auto")
small_model = AutoModelForCausalLM.from_pretrained("google/gemma-2-2b-it", device_map="auto", torch_dtype=torch.bfloat16)

# Load the larger Gemma2 model (9B parameters) for verification and normal inference
# This model is more accurate but slower for token-by-token generation.
big_tokenizer = AutoTokenizer.from_pretrained("google/gemma-2-9b-it", device_map="auto")
big_model = AutoModelForCausalLM.from_pretrained("google/gemma-2-9b-it", device_map="auto", torch_dtype=torch.bfloat16)

def normal_inference(big_model, big_tokenizer, prompt, max_new_tokens=50):
    """Generates text using only the large model (standard inference)."""
    # Tokenize the input prompt and move it to the computational device
    inputs = big_tokenizer(prompt, return_tensors='pt').to(device)

    # Generate output tokens using the large model
    # max_new_tokens specifies the maximum number of tokens to generate.
    outputs = big_model.generate(inputs['input_ids'], max_new_tokens=max_new_tokens)

    # Decode the generated token IDs back into readable text, skipping special tokens
    return big_tokenizer.decode(outputs[0], skip_special_tokens=True)

def speculative_decoding(small_model, big_model, small_tokenizer, big_tokenizer, prompt, max_new_tokens=50):
    """Demonstrates speculative decoding concept: uses a small model to draft, then a large model to verify.

    Note: This is a simplified demonstration that calculates a quality metric (log-likelihood) for the draft.
    A full implementation would perform token-by-token accept/reject to actually speed up generation by
    allowing the big model to accept multiple tokens at once when the draft is accurate.
    """
    # Step 1: Use the small (draft) model to generate a complete draft of the response
    # This model is faster but potentially less accurate than the big model.
    inputs = small_tokenizer(prompt, return_tensors='pt').to(device)
    small_outputs = small_model.generate(inputs['input_ids'], max_new_tokens=max_new_tokens)
    draft = small_tokenizer.decode(small_outputs[0], skip_special_tokens=True)

    # Step 2: Prepare the draft for verification by the big model
    # Tokenize the entire draft using the big model's tokenizer
    big_inputs = big_tokenizer(draft, return_tensors='pt').to(device)

    # Step 3: Verify the draft with the big model by calculating its log-likelihood
    # This shows how well the draft aligns with what the big model would have generated.
    # torch.no_grad() disables gradient calculation, reducing memory consumption and speeding up computation.
    with torch.no_grad():
        # Get the logits (raw prediction scores) from the big model for each position in the draft
        outputs = big_model(big_inputs['input_ids'])
        # Convert logits to log probabilities using softmax across the vocabulary dimension
        log_probs = torch.log_softmax(outputs.logits, dim=-1)

    # Calculate the log-likelihood of the draft under the big model's probability distribution
    # This serves as a 'verification score' indicating how likely the big model would have generated the draft.
    # Higher (less negative) scores indicate better alignment between the draft and big model.
    draft_token_ids = big_inputs['input_ids']
    log_likelihood = 0
    # Iterate through the generated tokens (excluding the first token which is part of the prompt context)
    for i in range(draft_token_ids.size(1) - 1):
        token_id = draft_token_ids[0, i + 1] # Get the current token's ID
        log_likelihood += log_probs[0, i, token_id].item() # Add its log-probability from the big model

    # Calculate the average log-likelihood per token for the generated draft
    # This normalized score allows comparison across drafts of different lengths
    avg_log_likelihood = log_likelihood / (draft_token_ids.size(1) - 1)

    # Return the generated draft and its verification score (average log-likelihood)
    return draft, avg_log_likelihood

import time

def measure_latency(small_model, big_model, small_tokenizer, big_tokenizer, prompt, max_new_tokens=50):
    """Measures and compares the inference latency and tokens per second for normal vs. speculative decoding."""
    # --- Measure latency for normal inference (using only the big model) ---
    start_time = time.time()
    normal_output = normal_inference(big_model, big_tokenizer, prompt, max_new_tokens)
    normal_inference_latency = time.time() - start_time
    # Calculate tokens per second for normal inference
    normal_tokens_per_sec = max_new_tokens / normal_inference_latency if normal_inference_latency > 0 else 0
    print(f"Normal Inference Output: {normal_output}")
    print(f"Normal Inference Latency: {normal_inference_latency:.4f} seconds")
    print(f"Normal Inference Tokens per second: {normal_tokens_per_sec:.2f} tokens/second")
    print("\n\n")

    # --- Measure latency for speculative decoding ---
    start_time = time.time()
    speculative_output, log_likelihood = speculative_decoding(
        small_model, big_model, small_tokenizer, big_tokenizer, prompt, max_new_tokens
    )
    speculative_decoding_latency = time.time() - start_time
    # Calculate tokens per second for speculative decoding
    speculative_tokens_per_sec = max_new_tokens / speculative_decoding_latency if speculative_decoding_latency > 0 else 0
    print(f"Speculative Decoding Output: {speculative_output}")
    print(f"Speculative Decoding Latency: {speculative_decoding_latency:.4f} seconds")
    print(f"Log Likelihood (Verification Score): {log_likelihood:.4f}")
    print(f"Speculative Decoding Tokens per second: {speculative_tokens_per_sec:.2f} tokens/second")

    # Return the measured latencies and tokens per second for further analysis
    return normal_inference_latency, speculative_decoding_latency, normal_tokens_per_sec, speculative_tokens_per_sec

# Define a list of prompts to test the models with
prompts = [
    "The future of artificial intelligence is ",
    "Machine learning is transforming the world by ",
    "Natural language processing enables computers to understand ",
    "Generative models like GPT-3 can create ",
    "AI ethics and fairness are important considerations for "
]

# Set the maximum number of new tokens to generate for each prompt
max_new_tokens = 200

# Initialize accumulators for total tokens per second and total latency for both methods
total_tokens_per_sec_normal = 0
total_tokens_per_sec_speculative = 0
total_normal_latency = 0
total_speculative_latency = 0

# Iterate through each prompt to perform inference and accumulate results
for prompt in prompts:
    # Call the measure_latency function to get performance metrics for the current prompt
    normal_latency, speculative_latency, tokens_per_sec_normal, tokens_per_sec_speculative = measure_latency(
        small_model, big_model, small_tokenizer, big_tokenizer, prompt, max_new_tokens
    )
    # Add current prompt's metrics to the total accumulators
    total_tokens_per_sec_normal += tokens_per_sec_normal
    total_tokens_per_sec_speculative += tokens_per_sec_speculative
    total_normal_latency += normal_latency
    total_speculative_latency += speculative_latency

# Calculate the average performance metrics across all prompts
average_tokens_per_sec_normal = total_tokens_per_sec_normal / len(prompts)
average_tokens_per_sec_speculative = total_tokens_per_sec_speculative / len(prompts)
average_normal_latency = total_normal_latency / len(prompts)
average_speculative_latency = total_speculative_latency / len(prompts)

# Print the calculated average performance metrics
print(f"Average Normal Inference Latency: {average_normal_latency:.4f} seconds")
print(f"Average Speculative Decoding Latency: {average_speculative_latency:.4f} seconds")
print(f"Average Normal Inference Tokens per second: {average_tokens_per_sec_normal:.2f} tokens/second")
print(f"Average Speculative Decoding Tokens per second: {average_tokens_per_sec_speculative:.2f} tokens/second")

# Calculate and print the percentage improvement of speculative decoding over normal inference
if average_tokens_per_sec_normal > 0:
    improvement_percentage = ((average_tokens_per_sec_speculative - average_tokens_per_sec_normal) / average_tokens_per_sec_normal) * 100
    print(f"\nSpeculative Decoding Speed Improvement: {improvement_percentage:.2f}%")
else:
    print("\nCannot calculate improvement: Average normal inference tokens per second is zero.")

"""The **Log Likelihood (Verification Score)** is a metric used in speculative decoding to quantify how well the 'draft' generated by a smaller, faster model aligns with the expectations of a larger, more accurate model.

Here's a breakdown:

1.  **The Core Idea of Speculative Decoding:**
    *   You have two models: a small, fast **'draft' model** and a large, accurate **'main' model**.
    *   Instead of the main model generating tokens one-by-one, the draft model quickly proposes a sequence of tokens.
    *   The main model then *verifies* this proposed sequence.

2.  **How the Verification Score is Calculated:**
    *   After the draft model generates a sequence (e.g., a few words), the *entire drafted sequence* is fed to the main model.
    *   The main model then predicts the likelihood of each token in that sequence. Specifically, it calculates the **log-probability** for each token in the draft, given the preceding tokens.
    *   These individual log-probabilities are summed up and then typically averaged over the number of drafted tokens. This average is what we refer to as the "Log Likelihood (Verification Score)".

3.  **What the Value Signifies (Interpretation):**
    *   **Always Negative (or Zero):** Log probabilities are always negative (or zero for a token predicted with 100% certainty, which is rare). Therefore, the log likelihood score will also be negative.
    *   **Closer to 0 (less negative) = Better Alignment:** A score closer to 0 indicates that the tokens in the draft were highly probable according to the main model. This means the main model was 'expecting' those tokens and can accept more of the draft at once, leading to significant speedups.
    *   **Farther from 0 (more negative) = Poorer Alignment:** A very negative score suggests that the draft model proposed tokens that the main model considered highly unlikely. In such cases, the main model might have to reject many of the drafted tokens and generate new ones from scratch, reducing the efficiency gains of speculative decoding.

4.  **Why it's Important:**
    *   This score acts as a **measure of confidence** for the main model in the draft. It tells us how 'good' the draft was.
    *   A high verification score directly correlates with better performance. When the draft is accurate (high log likelihood), the main model spends less time correcting and more time accepting chunks of text, thus accelerating the generation process.

In summary, a **Log Likelihood (Verification Score)** of, for example, `-0.7859` indicates that, on average, each token proposed by the draft model had a reasonable, but not extremely high, probability under the main model. The effectiveness of this score is ultimately determined by the actual speedup achieved and the quality of the generated text.
"""