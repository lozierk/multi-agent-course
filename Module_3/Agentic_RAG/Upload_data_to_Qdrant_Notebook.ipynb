{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hamzafarooq/multi-agent-course/blob/main/Module_3/Agentic_RAG/Upload_data_to_Qdrant_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rq9fUQqW6fkx"
      },
      "source": [
        "# Uploading PDF Data to Qdrant with Embeddings\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This notebook demonstrates how to process unstructured document data (such as PDF files) and store it in a local vector database using Qdrant. This workflow is useful for building applications like intelligent document search, semantic search engines, or AI-based question-answering systems.\n",
        "\n",
        "We will start by extracting text content from PDF files, convert that text into numerical representations called embeddings, and finally upload those embeddings into a Qdrant database for efficient retrieval and future use.\n",
        "\n",
        "In this example, we will use two types of PDF data:\n",
        "- **OpenAI documentation about Agents** (covering tools, APIs, and usage guidelines).\n",
        "- **10-K financial filings** (official company reports and financial statements by Uber and Lyft).\n",
        "\n",
        "## Data access\n",
        "\n",
        "**OpenAI documentation can be found here:** https://cdn.openai.com/business-guides-and-resources/a-practical-guide-to-building-agents.pdf\n",
        "\n",
        "**10-K financial filings for Uber and Lyft can be found here:**\n",
        "1. Uber 2021: https://www.sec.gov/Archives/edgar/data/1543151/000154315122000008/uber-20211231.htm\n",
        "\n",
        "2. Lyft 2024: https://www.sec.gov/ix?doc=/Archives/edgar/data/0001759509/000175950925000025/lyft-20241231.htm\n",
        "\n",
        "## Objectives\n",
        "\n",
        "- **Extract text from PDF files** using the PyMuPDF library.\n",
        "- **Generate semantic embeddings** for each chunk of text using a model from Nomic from Hugging Face.\n",
        "- **Store the embeddings in Qdrant**, a vector database running locally.\n",
        "\n",
        "By the end of this notebook, you'll have a working pipeline that reads documents, encodes them into meaningful vector representations, and persists them in a local database that can be queried later.\n",
        "\n",
        "For more information about Qdrant, please refer to the following sources:\n",
        "\n",
        "https://api.qdrant.tech/v-1-13-x/api-reference \\\n",
        "https://api.qdrant.tech/v-1-13-x/api-reference/search/query-points \\\n",
        "https://api.qdrant.tech/v-1-13-x/api-reference/collections/create-collection\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZLVukPv8LpW"
      },
      "source": [
        "## Setup and Dependencies\n",
        "\n",
        "Before we begin, ensure the necessary libraries are installed and imported\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/10k_files\n",
        "!mkdir /content/agents"
      ],
      "metadata": {
        "id": "MQ5Pw5NyI3B3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://cdn.openai.com/business-guides-and-resources/a-practical-guide-to-building-agents.pdf\" -q -O ./agents/agent-guide.pdf"
      ],
      "metadata": {
        "id": "DTxxj4TZIrrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://www.dropbox.com/scl/fi/ywc29qvt66s8i97h1taci/lyft-10k-2020.pdf?rlkey=d7bru2jno7398imeirn09fey5&dl=0\" -q -O ./10k_files/lyft_10k_2020.pdf\n",
        "!wget \"https://www.dropbox.com/scl/fi/lpmmki7a9a14s1l5ef7ep/lyft-10k-2021.pdf?rlkey=ud5cwlfotrii6r5jjag1o3hvm&dl=0\" -q -O ./10k_files/lyft_10k_2021.pdf\n",
        "!wget \"https://www.dropbox.com/scl/fi/iffbbnbw9h7shqnnot5es/lyft-10k-2022.pdf?rlkey=grkdgxcrib60oegtp4jn8hpl8&dl=0\" -q -O ./10k_files/lyft_10k_2022.pdf"
      ],
      "metadata": {
        "id": "W-QZSpwqHU41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://www.google.com/url?q=https%3A%2F%2Fwww.sec.gov%2FArchives%2Fedgar%2Fdata%2F1543151%2F000154315122000008%2Fuber-20211231.htm\" -q -O ./10k_files/uber_10k_2021.pdf"
      ],
      "metadata": {
        "id": "11EMMYXGIFJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "HciDI6OpSKJN"
      },
      "outputs": [],
      "source": [
        "# Install the Qdrant client, which allows you to connect to and interact with a Qdrant vector database.\n",
        "# Qdrant is often used for similarity search like semantic search or recommendation systems.\n",
        "!pip install qdrant_client\n",
        "\n",
        "# Install the Hugging Face Transformers library.\n",
        "# This library provides pre-trained models for tasks like text embeddings, classification, translation, summarization, and more.\n",
        "!pip install transformers\n",
        "\n",
        "# Install the PyMuPDF library (also known as Fitz), which is used for working with PDF files.\n",
        "# It allows you to extract text, images, and metadata from PDFs,\n",
        "!pip install PyMuPDF\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKrnG6z5REv7"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "\n",
        "# Import the PyMuPDF library, which is installed as 'fitz'\n",
        "# This library is used for reading and extracting content from PDF documents.\n",
        "import fitz\n",
        "\n",
        "# Import the 'os' module for handling file paths and operating system interactions (not used directly here but often useful)\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swim7SqVW7iC"
      },
      "source": [
        "## 1. Extract Data from PDF Files\n",
        "\n",
        "In this step, we will use the PyMuPDF library to extract text from our PDF documents. This will allow us to process the content and prepare it for embedding.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yaQO3vGva71L"
      },
      "outputs": [],
      "source": [
        "def read_text_pymupdf(path):\n",
        "    \"\"\"\n",
        "    Extracts text from a PDF file using the PyMuPDF (fitz) library.\n",
        "\n",
        "    Parameters:\n",
        "        path (str): The file path to the PDF document.\n",
        "\n",
        "    Returns:\n",
        "        str: A single string containing all the text extracted from the PDF.\n",
        "    \"\"\"\n",
        "\n",
        "    # Open the PDF document using the provided file path\n",
        "    # This returns a Document object that allows access to each page\n",
        "    doc = fitz.open(path)\n",
        "\n",
        "    # Initialize an empty string to collect text from all pages\n",
        "    text_results = ''\n",
        "\n",
        "    # Loop through each page in the PDF document\n",
        "    for page in doc:\n",
        "        # Extract text content from the current page\n",
        "        text = page.get_text()\n",
        "\n",
        "        # Append the extracted text to the cumulative result\n",
        "        text_results += text\n",
        "\n",
        "    # Return the complete text from the PDF\n",
        "    return text_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mf0_uV6kbDpZ"
      },
      "outputs": [],
      "source": [
        "# Define the path to the folder containing OpenAI-related PDF documents\n",
        "document_path_opnai = \"/content/agents\" #make sure to add your folder path here, this data is available in the github repository for the course\n",
        "\n",
        "# Define the path to the folder containing 10-K filing PDF documents\n",
        "document_path_10k = \"/content/10k_files\" #make sure to add your folder path here, this data is available in the github repository for the course\n",
        "\n",
        "# Initialize an empty list to hold the extracted text from the OpenAI documents\n",
        "openai_docs = []\n",
        "\n",
        "# Initialize an empty list to hold the extracted text from the 10-K documents\n",
        "docs_10k = []\n",
        "\n",
        "# Sets to track processed filenames (avoid duplicates)\n",
        "seen_files_opnai = set()\n",
        "seen_files_10k = set()\n",
        "\n",
        "# Loop through each file in the OpenAI documents folder\n",
        "for _f in os.listdir(document_path_opnai):\n",
        "    # Process only PDF files and skip duplicate filenames\n",
        "    if _f.lower().endswith(\".pdf\") and _f not in seen_files_opnai:\n",
        "        path = os.path.join(document_path_opnai, _f)\n",
        "        openai_docs.append(read_text_pymupdf(path))\n",
        "        seen_files_opnai.add(_f)\n",
        "\n",
        "# Loop through each file in the 10-K documents folder\n",
        "for _f in os.listdir(document_path_10k):\n",
        "    # Process only PDF files and skip duplicate filenames\n",
        "    if _f.lower().endswith(\".pdf\") and _f not in seen_files_10k:\n",
        "        path = os.path.join(document_path_10k, _f)\n",
        "        docs_10k.append(read_text_pymupdf(path))\n",
        "        seen_files_10k.add(_f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "VXD8vvHQdIKU"
      },
      "outputs": [],
      "source": [
        "docs_10k"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-3b8pckDzY6"
      },
      "source": [
        "## 2. Prepare Document Chunks with Metadata for Vector Storage\n",
        "\n",
        "Before we store our documents in a vector database like Qdrant, it's important to organize the data in a meaningful way. This section assigns unique identifiers and metadata to each document chunk.\n",
        "\n",
        "### Why this step is important:\n",
        "\n",
        "- **Chunk-Level Tracking**: Each document is split into smaller text chunks to fit the input size of embedding models. Assigning metadata to each chunk helps trace it back to its original source.\n",
        "  \n",
        "- **UUID Generation**: By attaching a universally unique identifier (UUID) to each chunk, we ensure every piece of data can be reliably referenced or retrieved later.\n",
        "\n",
        "- **Metadata Enrichment**: Adding metadata such as the original document path enables better filtering, searching, and organization within the vector database.\n",
        "\n",
        "This process ensures that once the chunks are embedded and stored, they remain well-organized and easily searchable in downstream applications such as semantic search or document-based Q&A systems.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "7yqrqkXLdSCG"
      },
      "outputs": [],
      "source": [
        "# Import a text splitter that breaks large texts into smaller, overlapping chunks\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Create the text splitter with settings for chunk size and overlap\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=2048,          # Max characters per chunk\n",
        "    chunk_overlap=50,         # Overlap between chunks to preserve context\n",
        "    length_function=len,      # Use Python's len() to measure text length\n",
        "    is_separator_regex=False, # Treat separators as plain text, not regex\n",
        "    separators=[              # Preferred breakpoints for splitting\n",
        "        \"\\n\\n\", \"\\n\", \" \", \".\", \",\",\n",
        "        \"\\u200b\", \"\\uff0c\", \"\\u3001\", \"\\uff0e\", \"\\u3002\", \"\"\n",
        "    ],\n",
        ")\n",
        "\n",
        "# Split the extracted OpenAI document text into chunks\n",
        "opnai_chunks = text_splitter.create_documents(openai_docs)\n",
        "\n",
        "# Split the 10-K documents into chunks as well\n",
        "chunks_10k = text_splitter.create_documents(docs_10k)\n",
        "\n",
        "# View the result (list of text chunks)\n",
        "opnai_chunks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Haph5n9eihN3"
      },
      "outputs": [],
      "source": [
        "# Import the uuid module to generate unique identifiers\n",
        "import uuid\n",
        "\n",
        "# Loop through each chunk of the OpenAI documents\n",
        "for i in range(len(opnai_chunks)):\n",
        "    # Generate a unique ID for each chunk (helps track and reference later)\n",
        "    unique_id = str(uuid.uuid4())\n",
        "\n",
        "    # Add metadata to the chunk:\n",
        "    # 'document_info' stores the source path (where the document came from)\n",
        "    # 'uuid' stores a unique identifier for the chunk\n",
        "    opnai_chunks[i].metadata['document_info'] = document_path_opnai\n",
        "    opnai_chunks[i].metadata['uuid'] = unique_id\n",
        "\n",
        "# Display the first 5 chunks with their metadata\n",
        "opnai_chunks[:5]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(opnai_chunks)"
      ],
      "metadata": {
        "id": "Bk2zIwrXMFot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "brsB1FhufrCp"
      },
      "outputs": [],
      "source": [
        "# Loop through each chunk of the 10-K documents\n",
        "for i in range(len(chunks_10k)):\n",
        "    # Generate a unique identifier (UUID) for each chunk\n",
        "    unique_id = str(uuid.uuid4())\n",
        "\n",
        "    # Assign metadata to each chunk:\n",
        "    # 'document_info' stores the path of the folder where the 10-K files are located\n",
        "    # 'uuid' stores the unique identifier for the chunk\n",
        "    chunks_10k[i].metadata['document_info'] = document_path_10k\n",
        "    chunks_10k[i].metadata['uuid'] = unique_id\n",
        "\n",
        "# Display the metadata and content of the 6th chunk (index 5) from the 10-K documents\n",
        "chunks_10k[5]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCKR7-IzEqW2"
      },
      "source": [
        "## 3. Embed Chunks for Vector Database\n",
        "\n",
        "### Purpose:\n",
        "\n",
        "Embedding the chunks of text is a crucial step in preparing the data for storage in a **vector database** like Qdrant. In this step, we convert each text chunk into a **numerical representation** (vector) that captures its semantic meaning. This allows us to perform advanced operations like **semantic search**, **similarity comparison**, and **retrieval** based on meaning, rather than just keyword matching.\n",
        "\n",
        "### Why we Embed?\n",
        "\n",
        "- **Vector Representation**: Embedding transforms text into vectors (lists of numbers) that machine learning models can understand. These vectors represent the **semantic meaning** of the text, allowing similar texts to be grouped together, even if they donâ€™t share exact words.\n",
        "  \n",
        "- **Efficient Search**: Storing these vectors in a vector database enables **fast similarity searches**. For example, if you ask a question or search for a document, the vector database can quickly find and return the most relevant results based on the meaning of the text, not just exact matches.\n",
        "  \n",
        "- **Contextual Understanding**: The embedding process allows the system to \"understand\" the context of words, which improves the relevance and accuracy of search results. For example, it helps understand that \"machine learning\" and \"artificial intelligence\" are related concepts, even if they don't appear together in the same document.\n",
        "\n",
        "### What it Does?\n",
        "\n",
        "- **Transforms text** into dense vectors, capturing the **semantic essence** of the content.\n",
        "- These vectors are then stored in a **vector database** (Qdrant in our case) for fast retrieval and comparison based on similarity, enabling features like document search or answering questions.\n",
        "  \n",
        "In the following steps, we will embed the text chunks using a pre-trained nomic text embed model, and store the resulting vectors in Qdrant for future use.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vIA6DUb3hG3I"
      },
      "outputs": [],
      "source": [
        "# Import the necessary libraries from the Hugging Face Transformers library\n",
        "# AutoTokenizer is used to tokenize the input text into a format that the model can process.\n",
        "# AutoModel loads the pre-trained model for generating embeddings.\n",
        "import torch # Import torch for device handling\n",
        "import numpy as np # For concatenating embeddings\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from tqdm.auto import tqdm # For progress bar\n",
        "\n",
        "# Load the tokenizer and model from Hugging Face\n",
        "# 'nomic-ai/nomic-embed-text-v1.5' is a pre-trained model designed for text embeddings\n",
        "# The `trust_remote_code=True` argument allows the use of the model's code from the remote repository\n",
        "text_tokenizer = AutoTokenizer.from_pretrained(\"nomic-ai/nomic-embed-text-v1.5\", trust_remote_code=True)\n",
        "text_model = AutoModel.from_pretrained(\"nomic-ai/nomic-embed-text-v1.5\", trust_remote_code=True)\n",
        "\n",
        "# Move model to GPU if available for faster inference\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "text_model.to(device)\n",
        "text_model.eval() # Set model to evaluation mode\n",
        "\n",
        "# Define a function to get embeddings for a given batch of text inputs\n",
        "def _get_embeddings_batch(texts): # Helper function for a single batch\n",
        "    \"\"\"\n",
        "    Generates embeddings for a single batch of text inputs.\n",
        "\n",
        "    Parameters:\n",
        "        texts (list of str): A list of text strings to embed.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: A 2D numpy array where each row is the embedding for a corresponding text.\n",
        "    \"\"\"\n",
        "    if not texts:\n",
        "        return np.array([])\n",
        "\n",
        "    with torch.no_grad(): # Disable gradient calculation for inference\n",
        "        # Tokenize the batch of text inputs\n",
        "        inputs = text_tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "        # Move tokenized inputs to the same device as the model (GPU if available)\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        # Pass the tokenized inputs into the model to get embeddings\n",
        "        outputs = text_model(**inputs)\n",
        "\n",
        "        # We use the mean of the last hidden state (output of the final layer) for each token\n",
        "        # This gives us a fixed-size vector for each text in the batch\n",
        "        embeddings = outputs.last_hidden_state.mean(dim=1)\n",
        "\n",
        "        # Return the embeddings as a numpy array, moving them back to CPU if they were on GPU\n",
        "        return embeddings.cpu().numpy()\n",
        "\n",
        "def get_document_embeddings_batched(document_chunks, batch_size=32):\n",
        "    \"\"\"\n",
        "    Generates embeddings for a list of document chunks in batches, showing progress.\n",
        "\n",
        "    Parameters:\n",
        "        document_chunks (list of Document): A list of Document objects from Langchain.\n",
        "        batch_size (int): The number of document chunks to process in each batch.\n",
        "\n",
        "    Returns:\n",
        "        list of numpy.ndarray: A list where each element is the embedding for a corresponding document chunk.\n",
        "    \"\"\"\n",
        "    all_embeddings = []\n",
        "    # Extract page_content for batching\n",
        "    contents = [doc.page_content for doc in document_chunks]\n",
        "\n",
        "    # Process in batches with a progress bar\n",
        "    for i in tqdm(range(0, len(contents), batch_size), desc=\"Generating Embeddings\"):\n",
        "        batch_contents = contents[i:i + batch_size]\n",
        "        batch_embeds = _get_embeddings_batch(batch_contents)\n",
        "        all_embeddings.extend(batch_embeds)\n",
        "    return all_embeddings\n",
        "\n",
        "# Example usage (updated for batch processing and progress bar)\n",
        "# This example now simulates processing a list of Document objects.\n",
        "# In actual usage, this `get_document_embeddings_batched` function will be called\n",
        "# directly with `opnai_chunks` and `chunks_10k` in cell `5ms4hviWkf37`.\n",
        "class MockDocument: # A mock class to simulate langchain Document objects\n",
        "    def __init__(self, page_content):\n",
        "        self.page_content = page_content\n",
        "\n",
        "sample_docs = [\n",
        "    MockDocument(\"This is the first document chunk.\"),\n",
        "    MockDocument(\"This is the second document chunk, with more detail.\"),\n",
        "    MockDocument(\"The third chunk is here.\"),\n",
        "    MockDocument(\"And finally, the fourth document chunk comes into play.\"),\n",
        "    MockDocument(\"Another chunk to demonstrate batching.\"),\n",
        "    MockDocument(\"The final piece of text.\"),\n",
        "]\n",
        "print(\"--- Example of batched embedding generation ---\")\n",
        "embeddings_example = get_document_embeddings_batched(sample_docs, batch_size=2)\n",
        "\n",
        "# Print the shape of the embeddings (should be num_docs x embedding_dim)\n",
        "print(f\"Shape of example embeddings: {np.array(embeddings_example).shape}\")\n",
        "if embeddings_example:\n",
        "    print(f\"First 5 values of the first example embedding: {embeddings_example[0][:5]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ms4hviWkf37"
      },
      "outputs": [],
      "source": [
        "# Embed the OpenAI document chunks into vectors using the `get_document_embeddings_batched` function\n",
        "# This function processes chunks in batches and shows a progress bar.\n",
        "opnai_texts_embeded = get_document_embeddings_batched(opnai_chunks)\n",
        "\n",
        "# Embed the 10-K document chunks into vectors using the `get_document_embeddings_batched` function\n",
        "# This also processes chunks in batches and shows a progress bar.\n",
        "texts_embeded_10k = get_document_embeddings_batched(chunks_10k)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrkE-V4PlClH"
      },
      "source": [
        "##  4. Initialize Qdrant\n",
        "\n",
        "### Purpose:\n",
        "\n",
        "This step initializes **Qdrant**, a vector database, to store the text embeddings generated earlier. Qdrant is designed for high-performance similarity search and retrieval of vector data, making it ideal for tasks like semantic search, recommendation systems, or nearest neighbor searches.\n",
        "\n",
        "### Why this is important?\n",
        "\n",
        "- **Qdrant Initialization**: This code sets up the Qdrant database to store vectors efficiently.\n",
        "- **Creating a Collection**: Collections are like tables in a relational database, and in this case, the collection stores vectors representing text embeddings.\n",
        "- **Cosine Similarity**: Using cosine similarity enables the database to quickly identify vectors (texts) that are semantically similar to a given query.\n",
        "  \n",
        "Once the collection is created and initialized, it will be ready to store the embeddings from the OpenAI documents and 10-K filings, making it possible to perform fast similarity searches later.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IpMMBShOk-Fd"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries from the Qdrant client\n",
        "# QdrantClient is used to interact with the Qdrant database.\n",
        "# models provides predefined parameters like vector configurations.\n",
        "from qdrant_client import models\n",
        "from qdrant_client import QdrantClient, models\n",
        "\n",
        "import qdrant_client\n",
        "import os\n",
        "\n",
        "qdrant_data_dir = '/content/qdrant_data'\n",
        "\n",
        "# Remove the directory if it exists\n",
        "if os.path.exists(qdrant_data_dir):\n",
        "    shutil.rmtree(qdrant_data_dir)\n",
        "    print(f\"Removed existing Qdrant data directory: {qdrant_data_dir}\")\n",
        "else:\n",
        "    print(f\"Qdrant data directory does not exist: {qdrant_data_dir}\")\n",
        "\n",
        "\n",
        "# Create the directory if it doesn't already exist\n",
        "# `exist_ok=True` ensures no error is raised if the directory already exists\n",
        "os.makedirs(qdrant_data_dir, exist_ok=True)\n",
        "\n",
        "# Initialize the Qdrant client, passing the path where the database will be stored\n",
        "# The Qdrant client will manage operations like adding vectors, creating collections, and querying data\n",
        "client = qdrant_client.QdrantClient(path=qdrant_data_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QAo5IyCElIbj"
      },
      "outputs": [],
      "source": [
        "# Determine the size (dimensionality) of the embeddings by checking the length of the first embedding vector\n",
        "# The embedding size is the number of values in each vector representation of the text (e.g., 768, 1024, etc.)\n",
        "text_embeddings_size = len(opnai_texts_embeded[0])\n",
        "\n",
        "# Check if the collection \"opnai_data\" already exists in Qdrant\n",
        "# If it does not exist, the code proceeds to create a new collection\n",
        "if not client.collection_exists(\"opnai_data\"):\n",
        "    # Create a new collection in Qdrant to store the OpenAI document embeddings\n",
        "    client.create_collection(\n",
        "        # Name of the collection, which helps identify it in the database\n",
        "        collection_name=\"opnai_data\",\n",
        "\n",
        "        # Define the vector configuration (size and distance metric) for the collection\n",
        "        vectors_config=models.VectorParams(\n",
        "            size=text_embeddings_size,  # Set the vector size to match the embeddings' dimensionality\n",
        "            distance=models.Distance.COSINE,  # Use Cosine similarity to measure distance between vectors\n",
        "        ),\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29OrYi5ulNQx"
      },
      "outputs": [],
      "source": [
        "# Check if the collection \"10k_data\" already exists in Qdrant\n",
        "# If it does not exist, the code proceeds to create a new collection\n",
        "if not client.collection_exists(\"10k_data\"):\n",
        "    # Create a new collection in Qdrant to store the 10-K document embeddings\n",
        "    client.create_collection(\n",
        "        # Name of the collection, which will hold the 10-K document embeddings\n",
        "        collection_name=\"10k_data\",\n",
        "\n",
        "        # Define the vector configuration (size and distance metric) for the collection\n",
        "        vectors_config=models.VectorParams(\n",
        "            size=text_embeddings_size,  # Set the vector size to match the embeddings' dimensionality\n",
        "            distance=models.Distance.COSINE,  # Use Cosine similarity to measure distance between vectors\n",
        "        ),\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuuPLjUcltrQ"
      },
      "source": [
        "## 5. Store Embeddings in Qdrant\n",
        "\n",
        "### Purpose:\n",
        "\n",
        "In this step, we insert the embeddings (numerical representations of text) for both the OpenAI documents and the 10-K filings into their respective collections in Qdrant. By storing these embeddings in Qdrant, we can enable efficient semantic search functionality, which is a key part of the **Agentic RAG** (Retriever-Augmented Generation) project.\n",
        "\n",
        "Once the embeddings are stored, we can leverage Qdrant's similarity search capabilities to retrieve semantically relevant documents based on a given query.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjax3ybnlmcJ"
      },
      "outputs": [],
      "source": [
        "# Define the names of the Qdrant collections we're working with\n",
        "# These are the collections where document embeddings will be stored\n",
        "clusters = [\"opnai_data\", \"10k_data\"]\n",
        "\n",
        "# Import numpy to handle vector data as arrays\n",
        "import numpy as np\n",
        "\n",
        "# Upload (store) the embeddings of OpenAI documents into the 'opnai_data' collection in Qdrant\n",
        "client.upload_points(\n",
        "    collection_name=\"opnai_data\",  # Name of the target collection in Qdrant\n",
        "\n",
        "    # Create a list of PointStruct objects, one for each embedded document chunk\n",
        "    points=[\n",
        "        models.PointStruct(\n",
        "            id=doc.metadata['uuid'],  # Use the pre-generated UUID as a unique ID for each point (chunk)\n",
        "\n",
        "            vector=np.array(opnai_texts_embeded[idx]),  # The embedding vector for the chunk, converted to a NumPy array\n",
        "\n",
        "            payload={  # Payload stores extra information (metadata and content) along with the vector\n",
        "                \"metadata\": doc.metadata,       # Include metadata like document path and UUID\n",
        "                \"content\": doc.page_content     # Store the original text content of the chunk\n",
        "            }\n",
        "        )\n",
        "        for idx, doc in enumerate(opnai_chunks)  # Loop through all OpenAI chunks and embed them\n",
        "    ]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rmRcz7pdmd9N"
      },
      "outputs": [],
      "source": [
        "# Upload (store) the embeddings of 10-K documents into the '10k_data' collection in Qdrant\n",
        "client.upload_points(\n",
        "    collection_name=\"10k_data\",  # Target collection name in Qdrant\n",
        "\n",
        "    # Create a list of points, one for each chunk in the 10-K documents\n",
        "    points=[\n",
        "        models.PointStruct(\n",
        "            id=doc.metadata['uuid'],  # Unique ID for each vector (from previously assigned UUID)\n",
        "\n",
        "            vector=np.array(texts_embeded_10k[idx]),  # The embedding vector for the chunk, converted to NumPy array\n",
        "\n",
        "            payload={  # Payload contains additional information for each vector\n",
        "                \"metadata\": doc.metadata,       # Include metadata such as source path and UUID\n",
        "                \"content\": doc.page_content     # Include the original chunk text for retrieval/display\n",
        "            }\n",
        "        )\n",
        "        for idx, doc in enumerate(chunks_10k)  # Loop through all 10-K chunks to create PointStructs\n",
        "    ]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def search_qdrant(query_text, collection_name, top_k=5):\n",
        "    \"\"\"\n",
        "    Performs a semantic search on a specified Qdrant collection.\n",
        "\n",
        "    Args:\n",
        "        query_text (str): The text of the query.\n",
        "        collection_name (str): The name of the Qdrant collection to search.\n",
        "        top_k (int): The number of top results to return.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of search results from Qdrant.\n",
        "    \"\"\"\n",
        "    # Generate embedding for the query text\n",
        "    query_embedding = _get_embeddings_batch([query_text])[0]\n",
        "\n",
        "    # Perform search using the Qdrant client\n",
        "    # The 'query' parameter now directly accepts the vector.\n",
        "    search_results = client.query_points(\n",
        "        collection_name=collection_name,\n",
        "        query=query_embedding,\n",
        "        limit=top_k,\n",
        "        with_payload=True,\n",
        "    )\n",
        "    # Access the 'points' attribute as suggested by the user's sample code\n",
        "    return search_results.points\n",
        "\n",
        "# --- Example Usage ---\n",
        "search_query = \"What is an AI agent?\"\n",
        "\n",
        "print(f\"\\nSearching OpenAI documentation for: '{search_query}'\")\n",
        "openai_results = search_qdrant(search_query, \"opnai_data\", top_k=3)\n",
        "for i, hit in enumerate(openai_results):\n",
        "    print(f\"Result {i+1} (score: {hit.score:.2f}):\")\n",
        "    print(f\"  Content: {hit.payload['content'][:200]}...\") # Print first 200 chars\n",
        "    print(f\"  Source: {hit.payload['metadata']['document_info']}\")\n",
        "    print(\"---\")\n",
        "\n",
        "search_query_10k = \"What are the main risks for Lyft?\"\n",
        "\n",
        "print(f\"\\nSearching 10-K filings for: '{search_query_10k}'\")\n",
        "k10_results = search_qdrant(search_query_10k, \"10k_data\", top_k=3)\n",
        "for i, hit in enumerate(k10_results):\n",
        "    print(f\"Result {i+1} (score: {hit.score:.2f}):\")\n",
        "    print(f\"  Content: {hit.payload['content'][:200]}...\") # Print first 200 chars\n",
        "    print(f\"  Source: {hit.payload['metadata']['document_info']}\")\n",
        "    print(\"---\")"
      ],
      "metadata": {
        "id": "3iGi_m19NKKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Zip the qdrant_data folder for easy download\n",
        "!zip -r /content/qdrant_data.zip /content/qdrant_data\n",
        "\n",
        "print(\"The qdrant_data folder has been zipped to /content/qdrant_data.zip\")\n",
        "print(\"You can download it from the Colab file browser or use the following Python command:\")\n",
        "print(\"from google.colab import files\")\n",
        "print(\"files.download('/content/qdrant_data.zip')\")"
      ],
      "metadata": {
        "id": "4U6EGi30O0TA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('/content/qdrant_data.zip')"
      ],
      "metadata": {
        "id": "o3oUIx01O-ov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vp1r2GnnJ1rv"
      },
      "source": [
        "## Final Notes\n",
        "\n",
        "At this point, we've successfully completed the full pipeline for preparing and storing document embeddings:\n",
        "\n",
        "1. Extracted text from PDFs using PyMuPDF.\n",
        "2. Split the documents into manageable text chunks.\n",
        "3. Embedded each chunk using the `nomic-embed-text` model.\n",
        "4. Stored the resulting vectors, along with metadata and original content, into Qdrant collections (`opnai_data` and `10k_data`).\n",
        "\n",
        "These embeddings are now stored in a local Qdrant vector database located at:\n",
        "\n",
        "/content/qdrant_data\n",
        "\n",
        "\n",
        "You can reuse this vector database in any other project by pointing to the same path. In our case, we will be using it as the retrieval layer in the **Agentic RAG** system, where relevant chunks will be fetched based on user queries and passed to an LLM to generate meaningful, grounded responses.\n",
        "\n",
        "This setup forms the foundation for building retrieval-augmented applications.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSfLvsFAKce3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}