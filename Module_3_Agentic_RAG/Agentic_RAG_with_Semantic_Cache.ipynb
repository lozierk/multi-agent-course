{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "id": "title-cell",
      "metadata": {
        "id": "title-cell"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hamzafarooq/multi-agent-course/blob/main/Module_3_Agentic_RAG/Agentic_RAG_with_Semantic_Cache.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "intro-cell",
      "metadata": {
        "id": "intro-cell"
      },
      "source": [
        "# Agentic RAG with Semantic Cache\n",
        "\n",
        "This notebook combines two powerful concepts:\n",
        "\n",
        "- **Semantic Cache** â€” A FAISS-backed cache that stores previous query embeddings and their answers. When a new query is semantically similar to a cached one, the stored answer is returned instantly â€” no LLM or API call needed.\n",
        "- **Agentic RAG** â€” An intelligent retrieval system that routes queries to the right knowledge source: OpenAI documentation (via Qdrant), 10-K financial filings (via Qdrant), or live internet search (via SerpApi).\n",
        "\n",
        "## Architecture\n",
        "\n",
        "```\n",
        "User Query\n",
        "    â”‚\n",
        "    â–¼\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚  Is query time-sensitive?   â”‚  â”€â”€YESâ”€â”€â–¶  Agentic RAG (no caching)\n",
        "â”‚  (current events, \"today\",  â”‚\n",
        "â”‚   live data, etc.)          â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "               â”‚ NO\n",
        "               â–¼\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚   Semantic Cache Lookup     â”‚  â”€â”€HITâ”€â”€â–¶  Return cached answer âš¡\n",
        "â”‚   (FAISS similarity search) â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "               â”‚ MISS\n",
        "               â–¼\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚      Agentic RAG Router     â”‚\n",
        "â”‚   (GPT-4o classifies query) â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "       â”‚          â”‚           â”‚\n",
        "  OPENAI      10K_DOC    INTERNET\n",
        "  QUERY       QUERY       QUERY\n",
        "    â”‚            â”‚            â”‚\n",
        "  Qdrant      Qdrant      SerpApi\n",
        "  (RAG)       (RAG)      (live web)\n",
        "       â”‚          â”‚           â”‚\n",
        "       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "               â”‚\n",
        "               â–¼\n",
        "    Store answer in cache ğŸ’¾\n",
        "               â”‚\n",
        "               â–¼\n",
        "          Return answer\n",
        "```\n",
        "\n",
        "## Why combine them?\n",
        "\n",
        "- **Speed**: Cached answers return in milliseconds vs. 2â€“5 seconds for full RAG.\n",
        "- **Cost**: Fewer LLM and API calls for repeated or similar questions.\n",
        "- **Correctness**: Time-sensitive queries (e.g., *\"What happened today?\"*) always bypass the cache to ensure fresh answers."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "setup-md",
      "metadata": {
        "id": "setup-md"
      },
      "source": [
        "## 1. Setup\n",
        "\n",
        "Install dependencies, clone the course repository (which contains `rag_helpers.py` and the pre-built Qdrant vector database), and import the helper module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "install-cell",
      "metadata": {
        "id": "install-cell"
      },
      "outputs": [],
      "source": [
        "!pip install -U faiss-cpu sentence_transformers transformers openai qdrant_client python-dotenv nest_asyncio -q\n",
        "\n",
        "import os, sys, nest_asyncio\n",
        "\n",
        "# â”€â”€ Colab: clone the course repo if not already present â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "try:\n",
        "    import google.colab\n",
        "    _REPO = \"/content/multi-agent-course\"\n",
        "    if not os.path.exists(_REPO):\n",
        "        os.system(f\"git clone https://github.com/hamzafarooq/multi-agent-course.git\")\n",
        "        print(\"Repository cloned âœ…\")\n",
        "    else:\n",
        "        print(\"Repository already present âœ…\")\n",
        "    _MODULE_DIR = f\"{_REPO}/Module_3_Agentic_RAG\"\n",
        "except ImportError:\n",
        "    # Running locally â€” rag_helpers.py lives in Module_3_Agentic_RAG/\n",
        "    _MODULE_DIR = os.path.dirname(os.path.abspath(\"__file__\"))\n",
        "    print(f\"Running locally â€” helpers path: {_MODULE_DIR}\")\n",
        "\n",
        "sys.path.insert(0, _MODULE_DIR)\n",
        "nest_asyncio.apply()  # Required for asyncio.run() inside Jupyter/Colab\n",
        "\n",
        "from rag_helpers import init_rag, SemanticCaching, agentic_rag_with_cache\n",
        "print(\"âœ… Helpers imported from rag_helpers.py\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "api-keys-md",
      "metadata": {
        "id": "api-keys-md"
      },
      "source": [
        "## 2. API Keys\n",
        "\n",
        "**On Google Colab** â€” store keys in the Secrets panel (`ğŸ”‘` icon, left sidebar):\n",
        "| Secret name | Where to get it |\n",
        "|---|---|\n",
        "| `SERP_API_KEY` | [serpapi.com](https://serpapi.com) |\n",
        "| `OPENAI_API_KEY` | [platform.openai.com](https://platform.openai.com) |\n",
        "\n",
        "**Running locally** â€” add keys to `Module_3_Agentic_RAG/.env`:\n",
        "```\n",
        "serp_api_key=<your_key>\n",
        "openai_api_key=<your_key>\n",
        "```\n",
        "The cell below detects the environment automatically and loads from the right source."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "api-keys-cell",
      "metadata": {
        "id": "api-keys-cell"
      },
      "outputs": [],
      "source": [
        "# â”€â”€ Load API keys â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    serp_api_key   = userdata.get('SERP_API_KEY')\n",
        "    openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "    QDRANT_PATH    = f\"{_REPO}/Module_3_Agentic_RAG/Agentic_RAG/qdrant_data\"\n",
        "    print(\"Colab: credentials loaded from Secrets.\")\n",
        "except ImportError:\n",
        "    from dotenv import load_dotenv, find_dotenv\n",
        "    load_dotenv(find_dotenv())\n",
        "    serp_api_key   = os.getenv(\"serp_api_key\")   or os.getenv(\"SERP_API_KEY\")\n",
        "    openai_api_key = os.getenv(\"openai_api_key\") or os.getenv(\"OPENAI_API_KEY\")\n",
        "    QDRANT_PATH    = os.path.join(_MODULE_DIR, \"Agentic_RAG\", \"qdrant_data\")\n",
        "    print(\"Local: credentials loaded from .env.\")\n",
        "\n",
        "print(f\"SerpApi key:    {'âœ…' if serp_api_key else 'âŒ MISSING'}\")\n",
        "print(f\"OpenAI API key: {'âœ…' if openai_api_key else 'âŒ MISSING'}\")\n",
        "\n",
        "# â”€â”€ Initialise the RAG pipeline (loads models + connects to Qdrant) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "init_rag(openai_api_key=openai_api_key, serp_api_key=serp_api_key, qdrant_path=QDRANT_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cache-md",
      "metadata": {
        "id": "cache-md"
      },
      "source": [
        "## 3. Create the Semantic Cache\n",
        "\n",
        "`SemanticCaching` is defined in `rag_helpers.py`. It provides:\n",
        "\n",
        "| Method | Purpose |\n",
        "|---|---|\n",
        "| `is_time_sensitive(q)` | Returns `True` for questions with temporal keywords â€” these always bypass the cache |\n",
        "| `check_cache(q)` | Embeds the query and runs a FAISS nearest-neighbour search; returns hit/miss + pre-computed embedding |\n",
        "| `add_to_cache(q, answer, embedding)` | Persists a new entry to FAISS + JSON after a RAG call |\n",
        "\n",
        "**Similarity threshold** (`threshold=0.2`): distance â‰¤ 0.2 (Euclidean) counts as a hit. Lower = stricter matching. Try `0.1` for exact-ish matches or `0.35` for a looser hit rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "init-cache-cell",
      "metadata": {
        "id": "init-cache-cell"
      },
      "outputs": [],
      "source": [
        "# Instantiate the semantic cache\n",
        "# Set clear_on_init=True to wipe any previously stored entries\n",
        "cache = SemanticCaching(json_file='rag_cache.json', threshold=0.2, clear_on_init=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rag-components-md",
      "metadata": {
        "id": "rag-components-md"
      },
      "source": [
        "## 4. Agentic RAG Pipeline (from `rag_helpers.py`)\n",
        "\n",
        "The pipeline components are all defined in `rag_helpers.py` â€” see the file for full implementations.\n",
        "\n",
        "| Function | What it does |\n",
        "|---|---|\n",
        "| `get_internet_content(query)` | Live Google search via SerpApi |\n",
        "| `route_query(query)` | GPT-4o classifies into `OPENAI_QUERY`, `10K_DOCUMENT_QUERY`, or `INTERNET_QUERY` |\n",
        "| `_retrieve_and_respond(query, action)` | Embeds query â†’ searches the right Qdrant collection â†’ generates a cited RAG answer |\n",
        "| `_run_rag_pipeline(query)` | Orchestrates routing + handler dispatch; returns the answer string |\n",
        "| **`agentic_rag_with_cache(query, cache)`** | **Main entry point** â€” applies the cache layer on top of the full pipeline |\n",
        "\n",
        "**Qdrant collections loaded by `init_rag()`:**\n",
        "- `opnai_data` â€” OpenAI Agents documentation  \n",
        "- `10k_data`   â€” Uber 2021 & Lyft 2024 10-K filings"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "combine-md",
      "metadata": {
        "id": "combine-md"
      },
      "source": [
        "## 5. Demo â€” Semantic Cache + Agentic RAG in Action\n",
        "\n",
        "`agentic_rag_with_cache(query, cache)` is the single function to call. It handles all routing, retrieval, caching, and display automatically."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tests-md",
      "metadata": {
        "id": "tests-md"
      },
      "source": [
        "### Test queries â€” three cache paths\n",
        "\n",
        "| Query | Expected path |\n",
        "|---|---|\n",
        "| *\"What was Uber's revenue in 2021?\"* | Cache MISS â†’ 10K RAG â†’ stored |\n",
        "| *\"How much did Uber earn in 2021?\"* | Cache HIT (semantically similar) |\n",
        "| *\"How do I build an agent with the OpenAI Agents SDK?\"* | Cache MISS â†’ OpenAI RAG â†’ stored |\n",
        "| *\"What are the best AI tools this week?\"* | Time-sensitive â†’ bypass cache â†’ SerpApi |\n",
        "| *\"What is the current stock price of Apple?\"* | Time-sensitive â†’ bypass cache â†’ SerpApi |\n",
        "| *\"What are the most popular open-source LLMs?\"* | Cache MISS â†’ INTERNET â†’ SerpApi â†’ stored |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "test1-cell",
      "metadata": {
        "id": "test1-cell"
      },
      "outputs": [],
      "source": [
        "result = agentic_rag_with_cache(\"What was Uber's revenue in 2021?\", cache)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "test2-cell",
      "metadata": {
        "id": "test2-cell"
      },
      "outputs": [],
      "source": [
        "# Test 2: Cache HIT â€” semantically similar to Test 1, returns instantly from cache\n",
        "result = agentic_rag_with_cache(\"How much did Uber earn in fiscal year 2021?\", cache)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "test3-cell",
      "metadata": {
        "id": "test3-cell"
      },
      "outputs": [],
      "source": [
        "# Test 3: Cache MISS â€” routes to OPENAI_QUERY and stores result\n",
        "result = agentic_rag_with_cache(\"How do I build an agent with the OpenAI Agents SDK?\", cache)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "test4-cell",
      "metadata": {
        "id": "test4-cell"
      },
      "outputs": [],
      "source": [
        "# Test 4: Time-sensitive query â€” BYPASSES cache, calls Ares API for live answer\n",
        "result = agentic_rag_with_cache(\"What are the best AI tools this week?\", cache)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "test5-cell",
      "metadata": {
        "id": "test5-cell"
      },
      "outputs": [],
      "source": [
        "# Test 5: Time-sensitive query â€” stock price, always fetched live\n",
        "result = agentic_rag_with_cache(\"What is the current stock price of Apple?\", cache)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "test6-cell",
      "metadata": {
        "id": "test6-cell"
      },
      "outputs": [],
      "source": [
        "# Test 6: Cache MISS â€” INTERNET_QUERY, stored after Ares API call\n",
        "result = agentic_rag_with_cache(\"What are the most popular open-source LLMs?\", cache)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "test7-cell",
      "metadata": {
        "id": "test7-cell"
      },
      "outputs": [],
      "source": [
        "# Test 7: Cache HIT â€” similar to Test 6\n",
        "result = agentic_rag_with_cache(\"Which open-source large language models are most widely used?\", cache)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "inspect-md",
      "metadata": {
        "id": "inspect-md"
      },
      "source": [
        "## 6. Inspect the Cache\n",
        "\n",
        "View all entries currently stored in the semantic cache."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "inspect-cell",
      "metadata": {
        "id": "inspect-cell"
      },
      "outputs": [],
      "source": [
        "print(f\"Total cached entries: {len(cache.cache['questions'])}\")\n",
        "print(f\"FAISS index size: {cache.index.ntotal}\\n\")\n",
        "\n",
        "for i, (q, a) in enumerate(zip(cache.cache['questions'], cache.cache['response_text'])):\n",
        "    print(f\"[{i}] Q: {q}\")\n",
        "    print(f\"    A: {a[:120]}...\\n\" if len(a) > 120 else f\"    A: {a}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "assignment-md",
      "metadata": {
        "id": "assignment-md"
      },
      "source": [
        "## Assignment: Extend the System\n",
        "\n",
        "Try one or more of these extensions:\n",
        "\n",
        "1. **Adjustable similarity threshold** â€” Experiment with `threshold=0.1` (stricter) vs `threshold=0.35` (looser). How does it affect hit rate and answer quality?\n",
        "\n",
        "2. **Cache TTL (Time-To-Live)** â€” Add an expiry timestamp to each cache entry. Stale entries (e.g., older than 7 days) should be evicted and re-fetched.\n",
        "\n",
        "3. **Sub-query division** â€” Before checking the cache, use a GPT call to split compound questions (e.g., *\"What was Uber and Lyft revenue in 2021?\"*) into sub-queries. Check and populate the cache per sub-query.\n",
        "\n",
        "4. **Cache analytics** â€” Track and display cache hit rate, average latency for hits vs misses, and the most-queried topics over a session."
      ]
    }
  ]
}