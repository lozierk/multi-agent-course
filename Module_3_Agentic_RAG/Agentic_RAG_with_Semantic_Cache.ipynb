{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "id": "title-cell",
      "metadata": {
        "id": "title-cell"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hamzafarooq/multi-agent-course/blob/main/Module_3_Agentic_RAG/Agentic_RAG_with_Semantic_Cache.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "intro-cell",
      "metadata": {
        "id": "intro-cell"
      },
      "source": [
        "# Agentic RAG with Semantic Cache\n",
        "\n",
        "This notebook combines two powerful concepts:\n",
        "\n",
        "- **Semantic Cache** â€” A FAISS-backed cache that stores previous query embeddings and their answers. When a new query is semantically similar to a cached one, the stored answer is returned instantly â€” no LLM or API call needed.\n",
        "- **Agentic RAG** â€” An intelligent retrieval system that routes queries to the right knowledge source: OpenAI documentation (via Qdrant), 10-K financial filings (via Qdrant), or live internet search (via SerpApi).\n",
        "\n",
        "## Architecture\n",
        "\n",
        "```\n",
        "User Query\n",
        "    â”‚\n",
        "    â–¼\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚  Is query time-sensitive?   â”‚  â”€â”€YESâ”€â”€â–¶  Agentic RAG (no caching)\n",
        "â”‚  (current events, \"today\",  â”‚\n",
        "â”‚   live data, etc.)          â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "               â”‚ NO\n",
        "               â–¼\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚   Semantic Cache Lookup     â”‚  â”€â”€HITâ”€â”€â–¶  Return cached answer âš¡\n",
        "â”‚   (FAISS similarity search) â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "               â”‚ MISS\n",
        "               â–¼\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚      Agentic RAG Router     â”‚\n",
        "â”‚   (GPT-4o classifies query) â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "       â”‚          â”‚           â”‚\n",
        "  OPENAI      10K_DOC    INTERNET\n",
        "  QUERY       QUERY       QUERY\n",
        "    â”‚            â”‚            â”‚\n",
        "  Qdrant      Qdrant      SerpApi\n",
        "  (RAG)       (RAG)      (live web)\n",
        "       â”‚          â”‚           â”‚\n",
        "       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "               â”‚\n",
        "               â–¼\n",
        "    Store answer in cache ğŸ’¾\n",
        "               â”‚\n",
        "               â–¼\n",
        "          Return answer\n",
        "```\n",
        "\n",
        "## Why combine them?\n",
        "\n",
        "- **Speed**: Cached answers return in milliseconds vs. 2â€“5 seconds for full RAG.\n",
        "- **Cost**: Fewer LLM and API calls for repeated or similar questions.\n",
        "- **Correctness**: Time-sensitive queries (e.g., *\"What happened today?\"*) always bypass the cache to ensure fresh answers."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "setup-md",
      "metadata": {
        "id": "setup-md"
      },
      "source": [
        "## 1. Setup and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "install-cell",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "install-cell",
        "outputId": "b6acb5d5-2e9b-46a3-c8a4-30e73fa1e2e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.13.2)\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.12/dist-packages (5.2.3)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (5.2.0)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (2.21.0)\n",
            "Requirement already satisfied: qdrant_client in /usr/local/lib/python3.12/dist-packages (1.17.0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (26.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (1.4.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (2.10.0+cpu)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (1.16.3)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (4.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (4.67.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from transformers) (0.24.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.12.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.13.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.12.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: grpcio>=1.41.0 in /usr/local/lib/python3.12/dist-packages (from qdrant_client) (1.78.0)\n",
            "Requirement already satisfied: portalocker<4.0,>=2.7.0 in /usr/local/lib/python3.12/dist-packages (from qdrant_client) (3.2.0)\n",
            "Requirement already satisfied: protobuf>=3.20.0 in /usr/local/lib/python3.12/dist-packages (from qdrant_client) (5.29.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.14 in /usr/local/lib/python3.12/dist-packages (from qdrant_client) (2.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.12/dist-packages (from httpx[http2]>=0.20.0->qdrant_client) (4.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.24.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (1.2.0)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (1.5.4)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.6)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence_transformers) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence_transformers) (3.6.0)\n",
            "Requirement already satisfied: typer>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->transformers) (0.24.0)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant_client) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant_client) (4.1.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Requirement already satisfied: click>=8.2.1 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim->transformers) (8.3.1)\n",
            "Requirement already satisfied: rich>=12.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim->transformers) (13.9.4)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim->transformers) (0.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim->transformers) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim->transformers) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=12.3.0->typer>=0.24.0->typer-slim->transformers) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U faiss-cpu sentence_transformers transformers openai qdrant_client python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "imports-cell",
      "metadata": {
        "id": "imports-cell"
      },
      "outputs": [],
      "source": [
        "import faiss                          # Efficient vector similarity search\n",
        "import json                           # JSON persistence for cache\n",
        "import numpy as np                    # Numerical operations on embeddings\n",
        "import requests                       # HTTP calls to SerpApi and Qdrant\n",
        "import re                             # Regex for parsing LLM JSON responses\n",
        "import time                           # Latency measurement\n",
        "import asyncio                        # Async support for Qdrant retrieval\n",
        "import nest_asyncio                   # Allow nested event loops in Colab/Jupyter\n",
        "import os\n",
        "\n",
        "from sentence_transformers import SentenceTransformer      # Nomic embed model wrapper\n",
        "from transformers import AutoTokenizer, AutoModel          # Tokenizer + model for RAG embeddings\n",
        "from openai import OpenAI, OpenAIError                     # OpenAI client for routing + generation\n",
        "import qdrant_client                                       # Qdrant vector database client\n",
        "from qdrant_client import models\n",
        "\n",
        "nest_asyncio.apply()  # Allow asyncio.run() inside Jupyter/Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "api-keys-md",
      "metadata": {
        "id": "api-keys-md"
      },
      "source": [
        "## 2. API Keys\n",
        "\n",
        "**On Google Colab** â€” store keys in the Secrets panel (`ğŸ”‘` icon, left sidebar):\n",
        "| Secret name | Where to get it |\n",
        "|---|---|\n",
        "| `SERP_API_KEY` | [serpapi.com](https://serpapi.com) |\n",
        "| `OPENAI_API_KEY` | [platform.openai.com](https://platform.openai.com) |\n",
        "\n",
        "**Running locally** â€” add keys to `Module_3_Agentic_RAG/.env`:\n",
        "```\n",
        "serp_api_key=<your_key>\n",
        "openai_api_key=<your_key>\n",
        "```\n",
        "The cell below detects the environment automatically and loads from the right source."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "api-keys-cell",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "api-keys-cell",
        "outputId": "d7cb30f9-7b08-4bfe-d263-421c47f7c0e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on Colab â€” credentials loaded from Secrets.\n",
            "SerpApi key loaded:    âœ…\n",
            "OpenAI API key loaded: âœ…\n"
          ]
        }
      ],
      "source": [
        "# â”€â”€ Credential loading â€” works on Colab and locally â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    serp_api_key   = userdata.get('SERP_API_KEY')\n",
        "    openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "    print(\"Running on Colab â€” credentials loaded from Secrets.\")\n",
        "except ImportError:\n",
        "    from dotenv import load_dotenv, find_dotenv\n",
        "    load_dotenv(find_dotenv())  # walks up the directory tree to find .env\n",
        "    serp_api_key   = os.getenv(\"serp_api_key\")   or os.getenv(\"SERP_API_KEY\")\n",
        "    openai_api_key = os.getenv(\"openai_api_key\") or os.getenv(\"OPENAI_API_KEY\")\n",
        "    print(\"Running locally â€” credentials loaded from .env file.\")\n",
        "\n",
        "print(f\"SerpApi key loaded:    {'âœ…' if serp_api_key else 'âŒ MISSING'}\")\n",
        "print(f\"OpenAI API key loaded: {'âœ…' if openai_api_key else 'âŒ MISSING'}\")\n",
        "\n",
        "# Initialize OpenAI client â€” used for query routing and RAG response generation\n",
        "openaiclient = OpenAI(api_key=openai_api_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cache-md",
      "metadata": {
        "id": "cache-md"
      },
      "source": [
        "## 3. Semantic Cache with Time-Sensitivity Filter\n",
        "\n",
        "The `SemanticCaching` class provides:\n",
        "\n",
        "- **`is_time_sensitive(question)`** â€” Detects questions that should never be cached because their answers change over time (e.g., *\"What is the weather today?\"*, *\"Latest AI news this week\"*). These go directly to the Agentic RAG pipeline for a fresh answer.\n",
        "- **`check_cache(question)`** â€” Embeds the query and checks FAISS for a near-neighbor within the similarity threshold. Returns hit/miss, the cached answer if available, and the computed embedding (reused on cache miss to avoid re-encoding).\n",
        "- **`add_to_cache(question, answer, embedding)`** â€” Stores a new entry after a successful RAG response.\n",
        "\n",
        "### What counts as time-sensitive?\n",
        "\n",
        "A question is flagged as time-sensitive if it contains temporal indicators such as:\n",
        "> `today`, `now`, `current`, `latest`, `this week`, `yesterday`, `upcoming`, `live`, `breaking`, `stock price`, `weather`, `forecast`, `real-time`, `last month`, `this quarter` â€¦\n",
        "\n",
        "These questions are intentionally excluded from caching because a cached answer from last week would be incorrect today."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "cache-class-cell",
      "metadata": {
        "id": "cache-class-cell"
      },
      "outputs": [],
      "source": [
        "class SemanticCaching:\n",
        "    \"\"\"\n",
        "    A semantic cache backed by FAISS that:\n",
        "    - Detects and bypasses time-sensitive queries\n",
        "    - Returns cached answers for semantically similar past queries\n",
        "    - Persists cache entries to disk as JSON\n",
        "    \"\"\"\n",
        "\n",
        "    # Keywords that indicate a question is time-sensitive and should NOT be cached\n",
        "    TIME_SENSITIVE_KEYWORDS = [\n",
        "        \"today\", \"tonight\", \"now\", \"currently\", \"current\",\n",
        "        \"latest\", \"recent\", \"recently\", \"right now\", \"at the moment\",\n",
        "        \"at present\", \"as of now\", \"this week\", \"this month\", \"this year\",\n",
        "        \"this quarter\", \"this season\", \"this morning\", \"this afternoon\",\n",
        "        \"this evening\", \"this weekend\", \"yesterday\", \"tomorrow\",\n",
        "        \"last week\", \"last month\", \"last year\", \"upcoming\", \"live\",\n",
        "        \"breaking\", \"just happened\", \"what time\", \"what day\", \"what date\",\n",
        "        \"happening now\", \"events today\", \"news today\", \"news this week\",\n",
        "        \"stock price\", \"share price\", \"weather\", \"forecast\", \"temperature\",\n",
        "        \"real-time\", \"realtime\", \"right now\", \"schedule today\",\n",
        "    ]\n",
        "\n",
        "    def __init__(self, json_file='rag_cache.json', threshold=0.2, clear_on_init=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            json_file (str): Path to the JSON file for cache persistence.\n",
        "            threshold (float): Max Euclidean distance to count as a cache hit (lower = stricter).\n",
        "            clear_on_init (bool): If True, wipe existing cache on startup.\n",
        "        \"\"\"\n",
        "        self.embedding_dim = 768\n",
        "        self.index = faiss.IndexFlatL2(self.embedding_dim)\n",
        "        self.euclidean_threshold = threshold\n",
        "        self.json_file = json_file\n",
        "\n",
        "        # Load the Nomic embedding model for query encoding\n",
        "        print(\"Loading Nomic embedding model...\")\n",
        "        self.encoder = SentenceTransformer(\n",
        "            'nomic-ai/nomic-embed-text-v1.5',\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "        print(\"Embedding model ready.\")\n",
        "\n",
        "        if clear_on_init:\n",
        "            self.clear_cache()\n",
        "        else:\n",
        "            self.load_cache()\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # Time-sensitivity detection\n",
        "    # ------------------------------------------------------------------\n",
        "\n",
        "    def is_time_sensitive(self, question: str) -> bool:\n",
        "        \"\"\"\n",
        "        Returns True if the question references time-dependent information\n",
        "        that should NOT be cached (answers change frequently).\n",
        "\n",
        "        Examples that return True:\n",
        "            'What is the weather today?'\n",
        "            'Who won the game last night?'\n",
        "            'What are the latest AI news this week?'\n",
        "            'What is the current stock price of Apple?'\n",
        "\n",
        "        Examples that return False (safe to cache):\n",
        "            'What is the capital of France?'\n",
        "            'Who founded Apple?'\n",
        "            'What is machine learning?'\n",
        "            'What was Uber revenue in 2021?'\n",
        "        \"\"\"\n",
        "        question_lower = question.lower()\n",
        "        return any(keyword in question_lower for keyword in self.TIME_SENSITIVE_KEYWORDS)\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # Cache persistence\n",
        "    # ------------------------------------------------------------------\n",
        "\n",
        "    def clear_cache(self):\n",
        "        \"\"\"Reset all in-memory state and overwrite the JSON file.\"\"\"\n",
        "        self.cache = {'questions': [], 'embeddings': [], 'response_text': []}\n",
        "        self.index = faiss.IndexFlatL2(self.embedding_dim)\n",
        "        self.save_cache()\n",
        "        print(\"Semantic cache cleared.\")\n",
        "\n",
        "    def load_cache(self):\n",
        "        \"\"\"Load cache from JSON and rebuild the FAISS index from stored embeddings.\"\"\"\n",
        "        try:\n",
        "            with open(self.json_file, 'r') as f:\n",
        "                self.cache = json.load(f)\n",
        "            # Rebuild FAISS index so lookups work after a cold start\n",
        "            if self.cache['embeddings']:\n",
        "                embeddings = np.array(self.cache['embeddings'], dtype=np.float32)\n",
        "                self.index.add(embeddings)\n",
        "            print(f\"Cache loaded: {len(self.cache['questions'])} entries.\")\n",
        "        except FileNotFoundError:\n",
        "            self.cache = {'questions': [], 'embeddings': [], 'response_text': []}\n",
        "            print(\"No existing cache found â€” starting fresh.\")\n",
        "\n",
        "    def save_cache(self):\n",
        "        \"\"\"Persist the cache to disk.\"\"\"\n",
        "        with open(self.json_file, 'w') as f:\n",
        "            json.dump(self.cache, f)\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # Cache lookup and storage\n",
        "    # ------------------------------------------------------------------\n",
        "\n",
        "    def check_cache(self, question: str):\n",
        "        \"\"\"\n",
        "        Encode the question and search the FAISS index for a near-enough neighbor.\n",
        "\n",
        "        Returns:\n",
        "            tuple: (\n",
        "                hit (bool),           -- True if a cached answer was found\n",
        "                answer (str|None),    -- Cached answer text, or None on miss\n",
        "                embedding (np.array), -- The computed embedding (reuse on cache miss)\n",
        "                similarity (float|None), -- Similarity score [0,1], or None on miss\n",
        "                row_id (int|None),    -- Index of the matched row, or None on miss\n",
        "            )\n",
        "        \"\"\"\n",
        "        embedding = self.encoder.encode([question], normalize_embeddings=True)\n",
        "\n",
        "        # Nothing in the index yet â€” guaranteed miss\n",
        "        if self.index.ntotal == 0:\n",
        "            return False, None, embedding, None, None\n",
        "\n",
        "        D, I = self.index.search(embedding, 1)\n",
        "\n",
        "        if I[0][0] != -1 and D[0][0] <= self.euclidean_threshold:\n",
        "            row_id = int(I[0][0])\n",
        "            similarity = float(1.0 - D[0][0])  # Convert distance â†’ similarity score\n",
        "            return True, self.cache['response_text'][row_id], embedding, similarity, row_id\n",
        "\n",
        "        return False, None, embedding, None, None\n",
        "\n",
        "    def add_to_cache(self, question: str, answer: str, embedding: np.ndarray):\n",
        "        \"\"\"\n",
        "        Store a new question-answer pair.\n",
        "\n",
        "        Args:\n",
        "            question (str): The original user query.\n",
        "            answer (str): The answer from the Agentic RAG pipeline.\n",
        "            embedding (np.ndarray): Pre-computed query embedding (shape: [1, 768]).\n",
        "        \"\"\"\n",
        "        self.cache['questions'].append(question)\n",
        "        self.cache['embeddings'].append(embedding[0].tolist())\n",
        "        self.cache['response_text'].append(answer)\n",
        "        self.index.add(embedding)\n",
        "        self.save_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "init-cache-cell",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "init-cache-cell",
        "outputId": "f61f4142-c3d3-4ab0-aa00-aacaa9aa05bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Nomic embedding model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.nomic_hyphen_ai.nomic_hyphen_bert_hyphen_2048.7710840340a098cfb869c4f65e87cf2b1b70caca.modeling_hf_nomic_bert:<All keys matched successfully>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding model ready.\n",
            "Semantic cache cleared.\n"
          ]
        }
      ],
      "source": [
        "# Instantiate the semantic cache\n",
        "# Set clear_on_init=True to wipe any previously stored entries\n",
        "cache = SemanticCaching(json_file='rag_cache.json', threshold=0.2, clear_on_init=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rag-components-md",
      "metadata": {
        "id": "rag-components-md"
      },
      "source": [
        "## 4. Agentic RAG Components\n",
        "\n",
        "These are the building blocks of the Agentic RAG pipeline, adapted from the `Agentic_RAG_Notebook.ipynb`.\n",
        "\n",
        "### Components\n",
        "| Component | Purpose |\n",
        "|---|---|\n",
        "| `get_internet_content()` | Fetches real-time answers via SerpApi (Google search) |\n",
        "| `route_query()` | Uses GPT-4o to classify the query into `OPENAI_QUERY`, `10K_DOCUMENT_QUERY`, or `INTERNET_QUERY` |\n",
        "| `get_text_embeddings()` | Converts text to dense vectors using Nomic embed model |\n",
        "| `rag_formatted_response()` | Generates a grounded answer from retrieved Qdrant chunks |\n",
        "| `retrieve_and_response()` | Async: embeds query â†’ searches Qdrant â†’ calls RAG generator |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "internet-tool-cell",
      "metadata": {
        "id": "internet-tool-cell"
      },
      "outputs": [],
      "source": [
        "def get_internet_content(user_query: str, action: str = \"INTERNET_QUERY\") -> str:\n",
        "    \"\"\"\n",
        "    Search Google in real time using SerpApi and return a formatted answer.\n",
        "\n",
        "    Used for INTERNET_QUERY routes â€” queries that need live web information\n",
        "    beyond what is available in the Qdrant document collections.\n",
        "\n",
        "    Args:\n",
        "        user_query (str): The user's question.\n",
        "        action (str): Route label (always \"INTERNET_QUERY\" here).\n",
        "\n",
        "    Returns:\n",
        "        str: Formatted string combining Google's answer box and top organic\n",
        "             result snippets, or an error message.\n",
        "    \"\"\"\n",
        "    print(\"Getting your response from the internet ğŸŒ ...\")\n",
        "\n",
        "    if not serp_api_key:\n",
        "        return \"Error: Missing SERP_API_KEY â€” add it to Colab Secrets or .env\"\n",
        "\n",
        "    params = {\n",
        "        \"q\": user_query,\n",
        "        \"api_key\": serp_api_key,\n",
        "        \"engine\": \"google\",\n",
        "        \"num\": 5,\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(\"https://serpapi.com/search.json\", params=params)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "\n",
        "        parts = []\n",
        "\n",
        "        # Answer box â€” Google's highlighted direct answer (most relevant)\n",
        "        answer_box = data.get(\"answer_box\", {})\n",
        "        if answer_box.get(\"answer\"):\n",
        "            parts.append(f\"[Direct Answer] {answer_box['answer']}\")\n",
        "        elif answer_box.get(\"snippet\"):\n",
        "            parts.append(f\"[Direct Answer] {answer_box['snippet']}\")\n",
        "\n",
        "        # Top organic results â€” titles + snippets\n",
        "        for i, result in enumerate(data.get(\"organic_results\", [])[:5], start=1):\n",
        "            title   = result.get(\"title\", \"\")\n",
        "            snippet = result.get(\"snippet\", \"\")\n",
        "            link    = result.get(\"link\", \"\")\n",
        "            if snippet:\n",
        "                parts.append(f\"[{i}] {title}\\n    {snippet}\\n    Source: {link}\")\n",
        "\n",
        "        return \"\\n\\n\".join(parts) if parts else \"No results found.\"\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        return f\"SerpApi request error: {e}\"\n",
        "    except Exception as e:\n",
        "        return f\"Unexpected error: {e}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "router-cell",
      "metadata": {
        "id": "router-cell"
      },
      "outputs": [],
      "source": [
        "def route_query(user_query: str) -> dict:\n",
        "    \"\"\"\n",
        "    Classify the user query into one of three categories using GPT-4o.\n",
        "\n",
        "    Returns:\n",
        "        dict with keys:\n",
        "            'action': 'OPENAI_QUERY' | '10K_DOCUMENT_QUERY' | 'INTERNET_QUERY'\n",
        "            'reason': brief justification\n",
        "            'answer': short answer if applicable, else empty string\n",
        "    \"\"\"\n",
        "    router_system_prompt = f\"\"\"\n",
        "    As a professional query router, classify user input into one of three categories:\n",
        "    1. \"OPENAI_QUERY\": Questions about OpenAI's documentation â€” agents, tools, APIs, models, embeddings, guardrails.\n",
        "    2. \"10K_DOCUMENT_QUERY\": Questions about company financials, 10-K annual reports, Uber or Lyft filings.\n",
        "    3. \"INTERNET_QUERY\": Everything else â€” general knowledge, comparisons, trends, or anything not in internal docs.\n",
        "\n",
        "    Always respond in this valid JSON format:\n",
        "    {{\n",
        "        \"action\": \"OPENAI_QUERY\" or \"10K_DOCUMENT_QUERY\" or \"INTERNET_QUERY\",\n",
        "        \"reason\": \"brief justification\",\n",
        "        \"answer\": \"AT MAX 5 words. Leave empty if INTERNET_QUERY\"\n",
        "    }}\n",
        "\n",
        "    User: {user_query}\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = openaiclient.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[{\"role\": \"system\", \"content\": router_system_prompt}]\n",
        "        )\n",
        "        task_response = response.choices[0].message.content\n",
        "        json_match = re.search(r\"\\{.*\\}\", task_response, re.DOTALL)\n",
        "        return json.loads(json_match.group())\n",
        "    except (OpenAIError, json.JSONDecodeError, AttributeError) as err:\n",
        "        return {\"action\": \"INTERNET_QUERY\", \"reason\": f\"Routing error: {err}\", \"answer\": \"\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qdrant-md",
      "metadata": {
        "id": "qdrant-md"
      },
      "source": [
        "### Qdrant Vector Database Setup\n",
        "\n",
        "We load the pre-built Qdrant collections from the course repository. Two collections are available:\n",
        "- `opnai_data` â€” OpenAI Agents documentation\n",
        "- `10k_data` â€” Uber 2021 and Lyft 2024 10-K filings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "qdrant-setup-cell",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdrant-setup-cell",
        "outputId": "d3fb68ae-5826-4ce5-bf18-34b95fd381be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Repository already present.\n",
            "Qdrant data path: /content/multi-agent-course/Module_3_Agentic_RAG/Agentic_RAG/qdrant_data\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Resolve the path to the prebuilt Qdrant vector database.\n",
        "# On Colab: clone the repo if needed, then point to /content/multi-agent-course/...\n",
        "# Locally:  the qdrant_data folder is already present in the cloned repo next to this notebook.\n",
        "\n",
        "try:\n",
        "    from google.colab import userdata  # will only succeed on Colab\n",
        "    QDRANT_PATH = \"/content/multi-agent-course/Module_3_Agentic_RAG/Agentic_RAG/qdrant_data\"\n",
        "    if not os.path.exists(\"/content/multi-agent-course\"):\n",
        "        import subprocess\n",
        "        subprocess.run([\"git\", \"clone\", \"https://github.com/hamzafarooq/multi-agent-course.git\",\n",
        "                        \"/content/multi-agent-course\"], check=True)\n",
        "        print(\"Repository cloned.\")\n",
        "    else:\n",
        "        print(\"Repository already present.\")\n",
        "except ImportError:\n",
        "    # Running locally â€” qdrant_data is in the Agentic_RAG sub-folder\n",
        "    NOTEBOOK_DIR = os.path.dirname(os.path.abspath(\"__file__\"))\n",
        "    QDRANT_PATH = os.path.join(NOTEBOOK_DIR, \"Agentic_RAG\", \"qdrant_data\")\n",
        "    print(f\"Running locally â€” Qdrant path: {QDRANT_PATH}\")\n",
        "\n",
        "print(f\"Qdrant data path: {QDRANT_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "qdrant-client-cell",
      "metadata": {
        "id": "qdrant-client-cell"
      },
      "outputs": [],
      "source": [
        "# Initialize async Qdrant client using the resolved path from the cell above\n",
        "qdrant = qdrant_client.AsyncQdrantClient(path=QDRANT_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "embeddings-cell",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "embeddings-cell",
        "outputId": "12fb5619-3f9a-4393-ad80-fb13f03d4d69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.nomic_hyphen_ai.nomic_hyphen_bert_hyphen_2048.7710840340a098cfb869c4f65e87cf2b1b70caca.modeling_hf_nomic_bert:<All keys matched successfully>\n"
          ]
        }
      ],
      "source": [
        "# Load tokenizer and model for query embedding during Qdrant retrieval\n",
        "# Note: The Nomic model is already loaded inside SemanticCaching above.\n",
        "# We use AutoTokenizer/AutoModel here to match the Agentic RAG notebook's approach\n",
        "# (mean-pooling over token embeddings, without normalization â€” different from SentenceTransformer).\n",
        "text_tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"nomic-ai/nomic-embed-text-v1.5\", trust_remote_code=True\n",
        ")\n",
        "text_model = AutoModel.from_pretrained(\n",
        "    \"nomic-ai/nomic-embed-text-v1.5\", trust_remote_code=True\n",
        ")\n",
        "\n",
        "def get_text_embeddings(text: str) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Embed a text string using mean pooling over token embeddings.\n",
        "    Used for Qdrant retrieval queries.\n",
        "\n",
        "    Args:\n",
        "        text (str): Input text to embed.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: 1-D embedding vector of shape (768,).\n",
        "    \"\"\"\n",
        "    inputs = text_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    outputs = text_model(**inputs)\n",
        "    embeddings = outputs.last_hidden_state.mean(dim=1)\n",
        "    return embeddings[0].detach().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "rag-response-cell",
      "metadata": {
        "id": "rag-response-cell"
      },
      "outputs": [],
      "source": [
        "def rag_formatted_response(user_query: str, context: list) -> str:\n",
        "    \"\"\"\n",
        "    Generate a grounded answer from retrieved document chunks.\n",
        "    Citations are formatted as [1][2]... matching chunk order.\n",
        "\n",
        "    Args:\n",
        "        user_query (str): The user's original question.\n",
        "        context (list): List of text chunks retrieved from Qdrant.\n",
        "\n",
        "    Returns:\n",
        "        str: Model-generated answer with numbered citations.\n",
        "    \"\"\"\n",
        "    rag_prompt = f\"\"\"\n",
        "    Based on the given context, answer the user query: {user_query}\\nContext:\\n{context}\n",
        "    Employ references to the ID of articles provided [ID], ensuring their relevance to the query.\n",
        "    The referencing should always be in the format of [1][2]... etc.\n",
        "    \"\"\"\n",
        "    response = openaiclient.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[{\"role\": \"system\", \"content\": rag_prompt}]\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "retrieve-cell",
      "metadata": {
        "id": "retrieve-cell"
      },
      "outputs": [],
      "source": [
        "async def retrieve_and_response(user_query: str, action: str) -> str:\n",
        "    \"\"\"\n",
        "    Retrieve relevant chunks from the appropriate Qdrant collection\n",
        "    and generate a RAG response.\n",
        "\n",
        "    Args:\n",
        "        user_query (str): The user's question.\n",
        "        action (str): 'OPENAI_QUERY' or '10K_DOCUMENT_QUERY'.\n",
        "\n",
        "    Returns:\n",
        "        str: Generated answer grounded in retrieved document chunks.\n",
        "    \"\"\"\n",
        "    collections = {\n",
        "        \"OPENAI_QUERY\": \"opnai_data\",\n",
        "        \"10K_DOCUMENT_QUERY\": \"10k_data\",\n",
        "    }\n",
        "    if action not in collections:\n",
        "        return \"Invalid action type for retrieval.\"\n",
        "\n",
        "    try:\n",
        "        query_embedding = get_text_embeddings(user_query)\n",
        "    except Exception as e:\n",
        "        return f\"Embedding error: {e}\"\n",
        "\n",
        "    try:\n",
        "        text_hits = await qdrant.query_points(\n",
        "            collection_name=collections[action],\n",
        "            query=query_embedding,\n",
        "            limit=3\n",
        "        )\n",
        "    except Exception as e:\n",
        "        return f\"Qdrant query error: {e}\"\n",
        "\n",
        "    contents = [point.payload['content'] for point in text_hits.points]\n",
        "    if not contents:\n",
        "        return \"No relevant content found in the database.\"\n",
        "\n",
        "    try:\n",
        "        return rag_formatted_response(user_query, contents)\n",
        "    except Exception as e:\n",
        "        return f\"RAG generation error: {e}\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "combine-md",
      "metadata": {
        "id": "combine-md"
      },
      "source": [
        "## 5. Combining Agentic RAG with Semantic Cache\n",
        "\n",
        "The `agentic_rag_with_cache()` function is the main entry point. It:\n",
        "\n",
        "1. **Checks time-sensitivity** â€” time-dependent questions always get a fresh answer.\n",
        "2. **Checks the semantic cache** â€” similar past queries return instantly.\n",
        "3. **Runs full Agentic RAG** on cache misses via `_get_rag_result()`, which:\n",
        "   - Routes the query (GPT-4o â†’ `OPENAI_QUERY` / `10K_DOCUMENT_QUERY` / `INTERNET_QUERY`)\n",
        "   - Calls the appropriate retrieval or search function\n",
        "   - Returns the answer as a string\n",
        "4. **Stores the result** in the cache for future use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "routes-cell",
      "metadata": {
        "id": "routes-cell"
      },
      "outputs": [],
      "source": [
        "# Maps routing labels to their handler functions\n",
        "routes = {\n",
        "    \"OPENAI_QUERY\": retrieve_and_response,\n",
        "    \"10K_DOCUMENT_QUERY\": retrieve_and_response,\n",
        "    \"INTERNET_QUERY\": get_internet_content,\n",
        "}\n",
        "\n",
        "\n",
        "def _get_rag_result(user_query: str) -> str:\n",
        "    \"\"\"\n",
        "    Run the full Agentic RAG pipeline and return the answer string.\n",
        "\n",
        "    This is a helper used by agentic_rag_with_cache() so the result can be\n",
        "    captured, displayed, and stored in the semantic cache.\n",
        "\n",
        "    Args:\n",
        "        user_query (str): The user's question.\n",
        "\n",
        "    Returns:\n",
        "        str: The final answer from the chosen knowledge source.\n",
        "    \"\"\"\n",
        "    GREY = \"\\033[90m\"\n",
        "    RESET = \"\\033[0m\"\n",
        "\n",
        "    # Step 1: Route the query\n",
        "    route_response = route_query(user_query)\n",
        "    action = route_response.get(\"action\", \"INTERNET_QUERY\")\n",
        "    reason = route_response.get(\"reason\", \"\")\n",
        "\n",
        "    print(f\"{GREY}ğŸ“ Route: {action}\")\n",
        "    print(f\"ğŸ“ Reason: {reason}{RESET}\")\n",
        "\n",
        "    # Step 2: Call the appropriate handler\n",
        "    route_fn = routes.get(action)\n",
        "    if not route_fn:\n",
        "        return f\"Unsupported action: {action}\"\n",
        "\n",
        "    try:\n",
        "        if action in (\"OPENAI_QUERY\", \"10K_DOCUMENT_QUERY\"):\n",
        "            result = asyncio.run(route_fn(user_query, action))\n",
        "        else:\n",
        "            result = route_fn(user_query, action)\n",
        "    except Exception as e:\n",
        "        result = f\"Execution error: {e}\"\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "main-fn-cell",
      "metadata": {
        "id": "main-fn-cell"
      },
      "outputs": [],
      "source": [
        "def agentic_rag_with_cache(user_query: str) -> str:\n",
        "    \"\"\"\n",
        "    Main entry point: Agentic RAG with semantic cache layer.\n",
        "\n",
        "    Query flow:\n",
        "        1. If time-sensitive â†’ skip cache, run Agentic RAG directly\n",
        "        2. Check semantic cache â†’ if hit, return cached answer instantly\n",
        "        3. Cache miss â†’ run Agentic RAG, store answer, return answer\n",
        "\n",
        "    Args:\n",
        "        user_query (str): The user's question.\n",
        "\n",
        "    Returns:\n",
        "        str: The answer text.\n",
        "    \"\"\"\n",
        "    CYAN  = \"\\033[96m\"\n",
        "    GREEN = \"\\033[92m\"\n",
        "    YELLOW = \"\\033[93m\"\n",
        "    BOLD  = \"\\033[1m\"\n",
        "    RESET = \"\\033[0m\"\n",
        "\n",
        "    print(f\"{BOLD}{CYAN}ğŸ‘¤ User Query:{RESET} {user_query}\\n\")\n",
        "\n",
        "    # ------------------------------------------------------------------ #\n",
        "    # Step 1: Time-sensitivity check                                       #\n",
        "    # ------------------------------------------------------------------ #\n",
        "    if cache.is_time_sensitive(user_query):\n",
        "        print(\n",
        "            f\"{YELLOW}â° Time-sensitive query detected â€” bypassing semantic cache \"\n",
        "            f\"to ensure a fresh answer.{RESET}\\n\"\n",
        "        )\n",
        "        result = _get_rag_result(user_query)\n",
        "        print(f\"\\n{BOLD}{CYAN}ğŸ¤– Response (live):{RESET}\\n{result}\\n\")\n",
        "        return result\n",
        "\n",
        "    # ------------------------------------------------------------------ #\n",
        "    # Step 2: Semantic cache lookup                                        #\n",
        "    # ------------------------------------------------------------------ #\n",
        "    start_time = time.time()\n",
        "    hit, cached_answer, embedding, similarity, row_id = cache.check_cache(user_query)\n",
        "\n",
        "    if hit:\n",
        "        elapsed = time.time() - start_time\n",
        "        print(\n",
        "            f\"{GREEN}âœ… Semantic Cache HIT{RESET} \"\n",
        "            f\"(row {row_id}, similarity: {similarity:.3f}, time: {elapsed:.3f}s)\\n\"\n",
        "        )\n",
        "        print(f\"{BOLD}{CYAN}ğŸ¤– Response (cached):{RESET}\\n{cached_answer}\\n\")\n",
        "        return cached_answer\n",
        "\n",
        "    # ------------------------------------------------------------------ #\n",
        "    # Step 3: Cache miss â†’ run Agentic RAG                                 #\n",
        "    # ------------------------------------------------------------------ #\n",
        "    print(f\"{YELLOW}âŒ Semantic Cache MISS â€” running Agentic RAG pipeline...{RESET}\\n\")\n",
        "    result = _get_rag_result(user_query)\n",
        "\n",
        "    # ------------------------------------------------------------------ #\n",
        "    # Step 4: Store the answer for future cache hits                       #\n",
        "    # ------------------------------------------------------------------ #\n",
        "    cache.add_to_cache(user_query, result, embedding)\n",
        "    print(f\"\\n{GREEN}ğŸ’¾ Answer cached for future similar queries.{RESET}\")\n",
        "\n",
        "    print(f\"\\n{BOLD}{CYAN}ğŸ¤– Response:{RESET}\\n{result}\\n\")\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tests-md",
      "metadata": {
        "id": "tests-md"
      },
      "source": [
        "## 6. Testing the Combined System\n",
        "\n",
        "We run a series of queries to demonstrate all three pathways:\n",
        "\n",
        "| Query | Expected path |\n",
        "|---|---|\n",
        "| *\"What was Uber's revenue in 2021?\"* | Cache MISS â†’ 10K RAG â†’ stored |\n",
        "| *\"How much did Uber earn in 2021?\"* | Cache HIT (semantically similar to above) |\n",
        "| *\"How to use the OpenAI Agents SDK?\"* | Cache MISS â†’ OpenAI RAG â†’ stored |\n",
        "| *\"What are the best AI tools this week?\"* | Time-sensitive â†’ bypass cache â†’ SerpApi |\n",
        "| *\"What is the current stock price of Apple?\"* | Time-sensitive â†’ bypass cache â†’ SerpApi |\n",
        "| *\"List new LLMs in 2025\"* | Cache MISS â†’ INTERNET â†’ SerpApi â†’ stored |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "test1-cell",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "test1-cell",
        "outputId": "48d5cde1-0879-4446-a812-c0209e5c486a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\u001b[96mğŸ‘¤ User Query:\u001b[0m What was Uber's revenue in 2021?\n",
            "\n",
            "\u001b[93mâŒ Semantic Cache MISS â€” running Agentic RAG pipeline...\u001b[0m\n",
            "\n",
            "\u001b[90mğŸ“ Route: 10K_DOCUMENT_QUERY\n",
            "ğŸ“ Reason: The question relates to Uber's financials.\u001b[0m\n",
            "\n",
            "\u001b[92mğŸ’¾ Answer cached for future similar queries.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[96mğŸ¤– Response:\u001b[0m\n",
            "Uber's revenue in 2021 was $3,208,323,000 [1].\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Test 1: Cache MISS â€” should route to 10K_DOCUMENT_QUERY and store result\n",
        "result = agentic_rag_with_cache(\"What was Uber's revenue in 2021?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "test2-cell",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "test2-cell",
        "outputId": "0875d8f3-b114-43e9-bc16-20633b42fd13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\u001b[96mğŸ‘¤ User Query:\u001b[0m How much did Uber earn in fiscal year 2021?\n",
            "\n",
            "\u001b[92mâœ… Semantic Cache HIT\u001b[0m (row 0, similarity: 0.838, time: 0.123s)\n",
            "\n",
            "\u001b[1m\u001b[96mğŸ¤– Response (cached):\u001b[0m\n",
            "Uber's revenue in 2021 was $3,208,323,000 [1].\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Test 2: Cache HIT â€” semantically similar to Test 1, returns instantly from cache\n",
        "result = agentic_rag_with_cache(\"How much did Uber earn in fiscal year 2021?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "test3-cell",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "test3-cell",
        "outputId": "d3af2ba2-c046-49fb-cb82-8a0dc80a2835"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\u001b[96mğŸ‘¤ User Query:\u001b[0m How do I build an agent with the OpenAI Agents SDK?\n",
            "\n",
            "\u001b[93mâŒ Semantic Cache MISS â€” running Agentic RAG pipeline...\u001b[0m\n",
            "\n",
            "\u001b[90mğŸ“ Route: OPENAI_QUERY\n",
            "ğŸ“ Reason: Question about OpenAI's documentation on agents.\u001b[0m\n",
            "\n",
            "\u001b[92mğŸ’¾ Answer cached for future similar queries.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[96mğŸ¤– Response:\u001b[0m\n",
            "To build an agent with the OpenAI Agents SDK, you need to focus on three core components: Model, Tools, and Instructions. Here's a step-by-step guide using the SDK:\n",
            "\n",
            "1. **Model Selection**: Choose an appropriate LLM for your agent by evaluating task complexity, latency, and cost. Start with the most capable model to establish a performance baseline, then experiment with smaller models to optimize for cost and speed [1][2].\n",
            "\n",
            "2. **Define Tools**: Tools are external functions or APIs the agent can utilize to perform actions. Define these tools clearly for the agent to use, such as a `get_weather` function in the example provided [1].\n",
            "\n",
            "3. **Provide Instructions**: Write explicit guidelines and guardrails that define agent behavior. This could include instructions like \"You are a helpful agent who can talk to users about the weather\" [1].\n",
            "\n",
            "Here's what the agent definition could look like in Python:\n",
            "\n",
            "```python\n",
            "weather_agent = Agent(\n",
            "    name=\"Weather agent\",\n",
            "    instructions=\"You are a helpful agent who can talk to users about the weather.\",\n",
            "    tools=[get_weather],\n",
            ")\n",
            "```\n",
            "This code snippet creates an agent named \"Weather agent\" with specific instructions and a tool to get weather information [1].\n",
            "\n",
            "By following these principles and the foundational framework for agent design, you can start building efficient and effective agents using OpenAI's SDK [1].\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Test 3: Cache MISS â€” routes to OPENAI_QUERY and stores result\n",
        "result = agentic_rag_with_cache(\"How do I build an agent with the OpenAI Agents SDK?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "test4-cell",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "test4-cell",
        "outputId": "f449dc4f-5cb1-4c22-bdd9-1af909926cce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\u001b[96mğŸ‘¤ User Query:\u001b[0m What are the best AI tools this week?\n",
            "\n",
            "\u001b[93mâ° Time-sensitive query detected â€” bypassing semantic cache to ensure a fresh answer.\u001b[0m\n",
            "\n",
            "\u001b[90mğŸ“ Route: INTERNET_QUERY\n",
            "ğŸ“ Reason: General inquiry about AI tools.\u001b[0m\n",
            "Getting your response from the internet ğŸŒ ...\n",
            "\n",
            "\u001b[1m\u001b[96mğŸ¤– Response (live):\u001b[0m\n",
            "[1] The Best AI Tools for 2026\n",
            "    Without a doubt, ChatGPT, Gemini, and Claude are the best AI tools to date. They can provide answers to your everyday questions, do web searches ...\n",
            "    Source: https://medium.com/artificial-corner/the-best-ai-tools-for-2026-933535a44f8b\n",
            "\n",
            "[2] I tried 70+ best AI tools in 2026\n",
            "    I went deep into each tool, from image generation to email automation, chatbot building to scheduling assistants.\n",
            "    Source: https://www.techradar.com/best/best-ai-tools\n",
            "\n",
            "[3] I Tested 47 AI Tools in 30 Days â€” Here Are the Only 7 Worth ...\n",
            "    I Tested 47 AI Tools in 30 Days â€” Here Are the Only 7 Worth Paying For How I cut my workday from 10 hours to 6 without sacrificing quality ...\n",
            "    Source: https://levelup.gitconnected.com/i-tested-47-ai-tools-in-30-days-here-are-the-only-7-worth-paying-for-ad31b1a9c0ab\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Test 4: Time-sensitive query â€” BYPASSES cache, calls Ares API for live answer\n",
        "result = agentic_rag_with_cache(\"What are the best AI tools this week?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "test5-cell",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "test5-cell",
        "outputId": "4737d561-5ad9-4d19-fb53-641df6cd77b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\u001b[96mğŸ‘¤ User Query:\u001b[0m What is the current stock price of Apple?\n",
            "\n",
            "\u001b[93mâ° Time-sensitive query detected â€” bypassing semantic cache to ensure a fresh answer.\u001b[0m\n",
            "\n",
            "\u001b[90mğŸ“ Route: INTERNET_QUERY\n",
            "ğŸ“ Reason: Question on real-time stock price\u001b[0m\n",
            "Getting your response from the internet ğŸŒ ...\n",
            "\n",
            "\u001b[1m\u001b[96mğŸ¤– Response (live):\u001b[0m\n",
            "[1] Stock Price - Apple - Investor Relations\n",
            "    Stock Quote: NASDAQ: AAPL ; Day's Open262.60 ; Closing Price260.58 ; Volume30.8 ; Intraday High264.48 ; Intraday Low260.05.\n",
            "    Source: https://investor.apple.com/stock-price/default.aspx\n",
            "\n",
            "[2] AAPL: Apple Inc - Stock Price, Quote and News\n",
            "    Apple Inc AAPL:NASDAQ ; Open258.97 ; Day High264.75 ; Day Low258.16 ; Prev Close260.58 ; 52 Week High288.62 ...\n",
            "    Source: https://www.cnbc.com/quotes/AAPL\n",
            "\n",
            "[3] Buy or Sell Apple Stock - AAPL Stock Price Quote & News\n",
            "    Shares are currently priced at $261.38, which is +1.2% above the low and -1.3% below the high. Apple(AAPL) shares are trading with a volume of 42.05M, against a ...\n",
            "    Source: https://robinhood.com/us/en/stocks/AAPL/\n",
            "\n",
            "[4] Apple Inc. Stock Quote (U.S.: Nasdaq) - AAPL\n",
            "    264.57 ; Volume: 40.88M Â· 65 Day Avg: 48.26M ; 258.16 Day Range 264.75 ; 169.21 52 Week Range 288.62 ...\n",
            "    Source: https://www.marketwatch.com/investing/stock/aapl?gaa_at=eafs&gaa_n=AWEtsqcOslNE7J82ldssL4ASeXAF2bBFfW7dg6bdjyEmWnLCy8D24jYy3wEk&gaa_ts=6998f9ca&gaa_sig=4x3yowkl9IvIt9CyUmS-Xmnz-aGsUpX-8nHTKIpELs2VpgDZtGsNOi3miHD8PPZTYDD6VbVK868ogZs_EmKyaw%3D%3D\n",
            "\n",
            "[5] Apple Inc. (AAPL) Stock Price, News, Quote & History\n",
            "    Find the latest Apple Inc. (AAPL) stock quote, history, news and other vital information to help you with your stock trading and investing.\n",
            "    Source: https://finance.yahoo.com/quote/AAPL/\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Test 5: Time-sensitive query â€” stock price, always fetched live\n",
        "result = agentic_rag_with_cache(\"What is the current stock price of Apple?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "test6-cell",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "test6-cell",
        "outputId": "ed68ebb5-e8f9-46da-d837-48fb76d5d0a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\u001b[96mğŸ‘¤ User Query:\u001b[0m What are the most popular open-source LLMs?\n",
            "\n",
            "\u001b[93mâŒ Semantic Cache MISS â€” running Agentic RAG pipeline...\u001b[0m\n",
            "\n",
            "\u001b[90mğŸ“ Route: INTERNET_QUERY\n",
            "ğŸ“ Reason: General inquiry about open-source LLMs.\u001b[0m\n",
            "Getting your response from the internet ğŸŒ ...\n",
            "\n",
            "\u001b[92mğŸ’¾ Answer cached for future similar queries.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[96mğŸ¤– Response:\u001b[0m\n",
            "[1] Open LLM Leaderboard 2025\n",
            "    This LLM leaderboard displays the latest public benchmark performance for SOTA open-sourced model versions released after April 2024.\n",
            "    Source: https://www.vellum.ai/open-llm-leaderboard\n",
            "\n",
            "[2] Top 10 open source LLMs for 2025\n",
            "    Top open source LLMs in 2024 Â· 1. LLaMA 3 Â· 2. Google Gemma 2 Â· 3. Command R+ Â· 4. Mistral-8x22b Â· 5. Falcon 2 Â· 6. Grok 1.5 Â· 7. Qwen1.5 Â· 8. BLOOM.\n",
            "    Source: https://www.instaclustr.com/education/open-source-ai/top-10-open-source-llms-for-2025/\n",
            "\n",
            "[3] I locally benchmarked 41 open-source LLMs across 19 ...\n",
            "    Hello everyone! I benchmarked 41 open-source LLMs using lm-evaluation-harness. Here are the 19 tasks covered:.\n",
            "    Source: https://www.reddit.com/r/LocalLLaMA/comments/1n57hb8/i_locally_benchmarked_41_opensource_llms_across/\n",
            "\n",
            "[4] The best open source large language model\n",
            "    Best overall open source LLM: Llama 3.3 70B Instruct Â· Llama 3.3 70B only supports eight languages, while many models support 2-3x as many.\n",
            "    Source: https://www.baseten.co/blog/the-best-open-source-large-language-model/\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Test 6: Cache MISS â€” INTERNET_QUERY, stored after Ares API call\n",
        "result = agentic_rag_with_cache(\"What are the most popular open-source LLMs?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "test7-cell",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "test7-cell",
        "outputId": "2289624e-3de6-4f4a-8835-f3397d5801ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\u001b[96mğŸ‘¤ User Query:\u001b[0m Which open-source large language models are most widely used?\n",
            "\n",
            "\u001b[93mâŒ Semantic Cache MISS â€” running Agentic RAG pipeline...\u001b[0m\n",
            "\n",
            "\u001b[90mğŸ“ Route: INTERNET_QUERY\n",
            "ğŸ“ Reason: Question on general open-source LLMs.\u001b[0m\n",
            "Getting your response from the internet ğŸŒ ...\n",
            "\n",
            "\u001b[92mğŸ’¾ Answer cached for future similar queries.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[96mğŸ¤– Response:\u001b[0m\n",
            "[1] Top 10 open source LLMs for 2025\n",
            "    Unlike proprietary models developed by companies like OpenAI and Google, open source LLMs are licensed to be freely used, modified, and distributed by anyone.\n",
            "    Source: https://www.instaclustr.com/education/open-source-ai/top-10-open-source-llms-for-2025/\n",
            "\n",
            "[2] 9 Top Open-Source LLMs for 2026 and Their Uses\n",
            "    LLM are the foundation models of popular and widely-used chatbots, like ChatGPT and Google Gemini. ... most powerful open-source large language ...\n",
            "    Source: https://www.datacamp.com/blog/top-open-source-llms\n",
            "\n",
            "[3] The best open source large language model\n",
            "    The largest open-source models, DeepSeek-V3 and DeepSeek-R1, match GPT-4o and o1-pro, respectively. Newer open source LLMs like Nemotron Llama ...\n",
            "    Source: https://www.baseten.co/blog/the-best-open-source-large-language-model/\n",
            "\n",
            "[4] Which LLM's are the best and opensource for code ...\n",
            "    Phi-4 is surprisingly good, even better than Qwen Coder 32B for JS and web-based stuff. It's not as good as DeepSeek V3, but it's shockingly comparable for ...\n",
            "    Source: https://www.reddit.com/r/LocalLLaMA/comments/1jn1njb/which_llms_are_the_best_and_opensource_for_code/\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Test 7: Cache HIT â€” similar to Test 6\n",
        "result = agentic_rag_with_cache(\"Which open-source large language models are most widely used?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "inspect-md",
      "metadata": {
        "id": "inspect-md"
      },
      "source": [
        "## 7. Inspecting the Cache\n",
        "\n",
        "View the current state of the semantic cache â€” what questions have been cached and what answers are stored."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "inspect-cell",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inspect-cell",
        "outputId": "cab240df-7854-4e0f-eccf-4e276a36ed54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total cached entries: 4\n",
            "FAISS index size: 4\n",
            "\n",
            "[0] Q: What was Uber's revenue in 2021?\n",
            "    A: Uber's revenue in 2021 was $3,208,323,000 [1].\n",
            "\n",
            "[1] Q: How do I build an agent with the OpenAI Agents SDK?\n",
            "    A: To build an agent with the OpenAI Agents SDK, you need to focus on three core components: Model, Tools, and Instructions...\n",
            "\n",
            "[2] Q: What are the most popular open-source LLMs?\n",
            "    A: [1] Open LLM Leaderboard 2025\n",
            "    This LLM leaderboard displays the latest public benchmark performance for SOTA open-so...\n",
            "\n",
            "[3] Q: Which open-source large language models are most widely used?\n",
            "    A: [1] Top 10 open source LLMs for 2025\n",
            "    Unlike proprietary models developed by companies like OpenAI and Google, open s...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(f\"Total cached entries: {len(cache.cache['questions'])}\")\n",
        "print(f\"FAISS index size: {cache.index.ntotal}\\n\")\n",
        "\n",
        "for i, (q, a) in enumerate(zip(cache.cache['questions'], cache.cache['response_text'])):\n",
        "    print(f\"[{i}] Q: {q}\")\n",
        "    print(f\"    A: {a[:120]}...\\n\" if len(a) > 120 else f\"    A: {a}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "assignment-md",
      "metadata": {
        "id": "assignment-md"
      },
      "source": [
        "## Assignment: Extend the System\n",
        "\n",
        "Try one or more of these extensions:\n",
        "\n",
        "1. **Adjustable similarity threshold** â€” Experiment with `threshold=0.1` (stricter) vs `threshold=0.35` (looser). How does it affect hit rate and answer quality?\n",
        "\n",
        "2. **Cache TTL (Time-To-Live)** â€” Add an expiry timestamp to each cache entry. Stale entries (e.g., older than 7 days) should be evicted and re-fetched.\n",
        "\n",
        "3. **Sub-query division** â€” Before checking the cache, use a GPT call to split compound questions (e.g., *\"What was Uber and Lyft revenue in 2021?\"*) into sub-queries. Check and populate the cache per sub-query.\n",
        "\n",
        "4. **Cache analytics** â€” Track and display cache hit rate, average latency for hits vs misses, and the most-queried topics over a session."
      ]
    }
  ]
}