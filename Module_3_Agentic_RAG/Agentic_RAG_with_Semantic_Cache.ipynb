{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": []
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title-cell",
   "metadata": {
    "id": "title-cell"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/hamzafarooq/multi-agent-course/blob/main/Module_3_Agentic_RAG/Agentic_RAG_with_Semantic_Cache.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intro-cell",
   "metadata": {
    "id": "intro-cell"
   },
   "source": [
    "# Agentic RAG with Semantic Cache\n",
    "\n",
    "This notebook combines two powerful concepts:\n",
    "\n",
    "- **Semantic Cache** â€” A FAISS-backed cache that stores previous query embeddings and their answers. When a new query is semantically similar to a cached one, the stored answer is returned instantly â€” no LLM or API call needed.\n",
    "- **Agentic RAG** â€” An intelligent retrieval system that routes queries to the right knowledge source: OpenAI documentation (via Qdrant), 10-K financial filings (via Qdrant), or live internet search (via SerpApi).\n",
    "\n",
    "## Architecture\n",
    "\n",
    "```\n",
    "User Query\n",
    "    â”‚\n",
    "    â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Is query time-sensitive?   â”‚  â”€â”€YESâ”€â”€â–¶  Agentic RAG (no caching)\n",
    "â”‚  (current events, \"today\",  â”‚\n",
    "â”‚   live data, etc.)          â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "               â”‚ NO\n",
    "               â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   Semantic Cache Lookup     â”‚  â”€â”€HITâ”€â”€â–¶  Return cached answer âš¡\n",
    "â”‚   (FAISS similarity search) â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "               â”‚ MISS\n",
    "               â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚      Agentic RAG Router     â”‚\n",
    "â”‚   (GPT-4o classifies query) â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "       â”‚          â”‚           â”‚\n",
    "  OPENAI      10K_DOC    INTERNET\n",
    "  QUERY       QUERY       QUERY\n",
    "    â”‚            â”‚            â”‚\n",
    "  Qdrant      Qdrant      SerpApi\n",
    "  (RAG)       (RAG)      (live web)\n",
    "       â”‚          â”‚           â”‚\n",
    "       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "               â”‚\n",
    "               â–¼\n",
    "    Store answer in cache ğŸ’¾\n",
    "               â”‚\n",
    "               â–¼\n",
    "          Return answer\n",
    "```\n",
    "\n",
    "## Why combine them?\n",
    "\n",
    "- **Speed**: Cached answers return in milliseconds vs. 2â€“5 seconds for full RAG.\n",
    "- **Cost**: Fewer LLM and API calls for repeated or similar questions.\n",
    "- **Correctness**: Time-sensitive queries (e.g., *\"What happened today?\"*) always bypass the cache to ensure fresh answers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-md",
   "metadata": {
    "id": "setup-md"
   },
   "source": "## 1. Setup\n\nInstall dependencies, clone the course repository (which contains `rag_helpers.py` and the pre-built Qdrant vector database), and import the helper module."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-cell",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "install-cell",
    "outputId": "b6acb5d5-2e9b-46a3-c8a4-30e73fa1e2e3"
   },
   "outputs": [],
   "source": "!pip install -U faiss-cpu sentence_transformers transformers openai qdrant_client python-dotenv nest_asyncio -q\n\nimport os, sys, nest_asyncio\n\n# â”€â”€ Colab: clone the course repo if not already present â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ntry:\n    import google.colab\n    _REPO = \"/content/multi-agent-course\"\n    if not os.path.exists(_REPO):\n        os.system(f\"git clone https://github.com/hamzafarooq/multi-agent-course.git {_REPO}\")\n        print(\"Repository cloned âœ…\")\n    else:\n        print(\"Repository already present âœ…\")\n    _MODULE_DIR = f\"{_REPO}/Module_3_Agentic_RAG\"\nexcept ImportError:\n    # Running locally â€” rag_helpers.py lives in Module_3_Agentic_RAG/\n    _MODULE_DIR = os.path.dirname(os.path.abspath(\"__file__\"))\n    print(f\"Running locally â€” helpers path: {_MODULE_DIR}\")\n\nsys.path.insert(0, _MODULE_DIR)\nnest_asyncio.apply()  # Required for asyncio.run() inside Jupyter/Colab\n\nfrom rag_helpers import init_rag, SemanticCaching, agentic_rag_with_cache\nprint(\"âœ… Helpers imported from rag_helpers.py\")"
  },
  {
   "cell_type": "markdown",
   "id": "api-keys-md",
   "metadata": {
    "id": "api-keys-md"
   },
   "source": [
    "## 2. API Keys\n",
    "\n",
    "**On Google Colab** â€” store keys in the Secrets panel (`ğŸ”‘` icon, left sidebar):\n",
    "| Secret name | Where to get it |\n",
    "|---|---|\n",
    "| `SERP_API_KEY` | [serpapi.com](https://serpapi.com) |\n",
    "| `OPENAI_API_KEY` | [platform.openai.com](https://platform.openai.com) |\n",
    "\n",
    "**Running locally** â€” add keys to `Module_3_Agentic_RAG/.env`:\n",
    "```\n",
    "serp_api_key=<your_key>\n",
    "openai_api_key=<your_key>\n",
    "```\n",
    "The cell below detects the environment automatically and loads from the right source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "api-keys-cell",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "api-keys-cell",
    "outputId": "d7cb30f9-7b08-4bfe-d263-421c47f7c0e0"
   },
   "outputs": [],
   "source": "# â”€â”€ Load API keys â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ntry:\n    from google.colab import userdata\n    serp_api_key   = userdata.get('SERP_API_KEY')\n    openai_api_key = userdata.get('OPENAI_API_KEY')\n    QDRANT_PATH    = f\"{_REPO}/Module_3_Agentic_RAG/Agentic_RAG/qdrant_data\"\n    print(\"Colab: credentials loaded from Secrets.\")\nexcept ImportError:\n    from dotenv import load_dotenv, find_dotenv\n    load_dotenv(find_dotenv())\n    serp_api_key   = os.getenv(\"serp_api_key\")   or os.getenv(\"SERP_API_KEY\")\n    openai_api_key = os.getenv(\"openai_api_key\") or os.getenv(\"OPENAI_API_KEY\")\n    QDRANT_PATH    = os.path.join(_MODULE_DIR, \"Agentic_RAG\", \"qdrant_data\")\n    print(\"Local: credentials loaded from .env.\")\n\nprint(f\"SerpApi key:    {'âœ…' if serp_api_key else 'âŒ MISSING'}\")\nprint(f\"OpenAI API key: {'âœ…' if openai_api_key else 'âŒ MISSING'}\")\n\n# â”€â”€ Initialise the RAG pipeline (loads models + connects to Qdrant) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ninit_rag(openai_api_key=openai_api_key, serp_api_key=serp_api_key, qdrant_path=QDRANT_PATH)"
  },
  {
   "cell_type": "markdown",
   "id": "cache-md",
   "metadata": {
    "id": "cache-md"
   },
   "source": "## 3. Create the Semantic Cache\n\n`SemanticCaching` is defined in `rag_helpers.py`. It provides:\n\n| Method | Purpose |\n|---|---|\n| `is_time_sensitive(q)` | Returns `True` for questions with temporal keywords â€” these always bypass the cache |\n| `check_cache(q)` | Embeds the query and runs a FAISS nearest-neighbour search; returns hit/miss + pre-computed embedding |\n| `add_to_cache(q, answer, embedding)` | Persists a new entry to FAISS + JSON after a RAG call |\n\n**Similarity threshold** (`threshold=0.2`): distance â‰¤ 0.2 (Euclidean) counts as a hit. Lower = stricter matching. Try `0.1` for exact-ish matches or `0.35` for a looser hit rate."
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "init-cache-cell",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "init-cache-cell",
    "outputId": "f61f4142-c3d3-4ab0-aa00-aacaa9aa05bc"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading Nomic embedding model...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:transformers_modules.nomic_hyphen_ai.nomic_hyphen_bert_hyphen_2048.7710840340a098cfb869c4f65e87cf2b1b70caca.modeling_hf_nomic_bert:<All keys matched successfully>\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Embedding model ready.\n",
      "Semantic cache cleared.\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the semantic cache\n",
    "# Set clear_on_init=True to wipe any previously stored entries\n",
    "cache = SemanticCaching(json_file='rag_cache.json', threshold=0.2, clear_on_init=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rag-components-md",
   "metadata": {
    "id": "rag-components-md"
   },
   "source": "## 4. Agentic RAG Pipeline (from `rag_helpers.py`)\n\nThe pipeline components are all defined in `rag_helpers.py` â€” see the file for full implementations.\n\n| Function | What it does |\n|---|---|\n| `get_internet_content(query)` | Live Google search via SerpApi |\n| `route_query(query)` | GPT-4o classifies into `OPENAI_QUERY`, `10K_DOCUMENT_QUERY`, or `INTERNET_QUERY` |\n| `_retrieve_and_respond(query, action)` | Embeds query â†’ searches the right Qdrant collection â†’ generates a cited RAG answer |\n| `_run_rag_pipeline(query)` | Orchestrates routing + handler dispatch; returns the answer string |\n| **`agentic_rag_with_cache(query, cache)`** | **Main entry point** â€” applies the cache layer on top of the full pipeline |\n\n**Qdrant collections loaded by `init_rag()`:**\n- `opnai_data` â€” OpenAI Agents documentation  \n- `10k_data`   â€” Uber 2021 & Lyft 2024 10-K filings"
  },
  {
   "cell_type": "markdown",
   "id": "combine-md",
   "metadata": {
    "id": "combine-md"
   },
   "source": "## 5. Demo â€” Semantic Cache + Agentic RAG in Action\n\n`agentic_rag_with_cache(query, cache)` is the single function to call. It handles all routing, retrieval, caching, and display automatically."
  },
  {
   "cell_type": "markdown",
   "id": "tests-md",
   "metadata": {
    "id": "tests-md"
   },
   "source": "### Test queries â€” three cache paths\n\n| Query | Expected path |\n|---|---|\n| *\"What was Uber's revenue in 2021?\"* | Cache MISS â†’ 10K RAG â†’ stored |\n| *\"How much did Uber earn in 2021?\"* | Cache HIT (semantically similar) |\n| *\"How do I build an agent with the OpenAI Agents SDK?\"* | Cache MISS â†’ OpenAI RAG â†’ stored |\n| *\"What are the best AI tools this week?\"* | Time-sensitive â†’ bypass cache â†’ SerpApi |\n| *\"What is the current stock price of Apple?\"* | Time-sensitive â†’ bypass cache â†’ SerpApi |\n| *\"What are the most popular open-source LLMs?\"* | Cache MISS â†’ INTERNET â†’ SerpApi â†’ stored |"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "test1-cell",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "test1-cell",
    "outputId": "48d5cde1-0879-4446-a812-c0209e5c486a"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[1m\u001b[96mğŸ‘¤ User Query:\u001b[0m What was Uber's revenue in 2021?\n",
      "\n",
      "\u001b[93mâŒ Semantic Cache MISS â€” running Agentic RAG pipeline...\u001b[0m\n",
      "\n",
      "\u001b[90mğŸ“ Route: 10K_DOCUMENT_QUERY\n",
      "ğŸ“ Reason: The question relates to Uber's financials.\u001b[0m\n",
      "\n",
      "\u001b[92mğŸ’¾ Answer cached for future similar queries.\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[96mğŸ¤– Response:\u001b[0m\n",
      "Uber's revenue in 2021 was $3,208,323,000 [1].\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test 1: Cache MISS â€” should route to 10K_DOCUMENT_QUERY and store result\n",
    "result = agentic_rag_with_cache(\"What was Uber's revenue in 2021?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "test2-cell",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "test2-cell",
    "outputId": "0875d8f3-b114-43e9-bc16-20633b42fd13"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[1m\u001b[96mğŸ‘¤ User Query:\u001b[0m How much did Uber earn in fiscal year 2021?\n",
      "\n",
      "\u001b[92mâœ… Semantic Cache HIT\u001b[0m (row 0, similarity: 0.838, time: 0.123s)\n",
      "\n",
      "\u001b[1m\u001b[96mğŸ¤– Response (cached):\u001b[0m\n",
      "Uber's revenue in 2021 was $3,208,323,000 [1].\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test 2: Cache HIT â€” semantically similar to Test 1, returns instantly from cache\n",
    "result = agentic_rag_with_cache(\"How much did Uber earn in fiscal year 2021?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "test3-cell",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "test3-cell",
    "outputId": "d3af2ba2-c046-49fb-cb82-8a0dc80a2835"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[1m\u001b[96mğŸ‘¤ User Query:\u001b[0m How do I build an agent with the OpenAI Agents SDK?\n",
      "\n",
      "\u001b[93mâŒ Semantic Cache MISS â€” running Agentic RAG pipeline...\u001b[0m\n",
      "\n",
      "\u001b[90mğŸ“ Route: OPENAI_QUERY\n",
      "ğŸ“ Reason: Question about OpenAI's documentation on agents.\u001b[0m\n",
      "\n",
      "\u001b[92mğŸ’¾ Answer cached for future similar queries.\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[96mğŸ¤– Response:\u001b[0m\n",
      "To build an agent with the OpenAI Agents SDK, you need to focus on three core components: Model, Tools, and Instructions. Here's a step-by-step guide using the SDK:\n",
      "\n",
      "1. **Model Selection**: Choose an appropriate LLM for your agent by evaluating task complexity, latency, and cost. Start with the most capable model to establish a performance baseline, then experiment with smaller models to optimize for cost and speed [1][2].\n",
      "\n",
      "2. **Define Tools**: Tools are external functions or APIs the agent can utilize to perform actions. Define these tools clearly for the agent to use, such as a `get_weather` function in the example provided [1].\n",
      "\n",
      "3. **Provide Instructions**: Write explicit guidelines and guardrails that define agent behavior. This could include instructions like \"You are a helpful agent who can talk to users about the weather\" [1].\n",
      "\n",
      "Here's what the agent definition could look like in Python:\n",
      "\n",
      "```python\n",
      "weather_agent = Agent(\n",
      "    name=\"Weather agent\",\n",
      "    instructions=\"You are a helpful agent who can talk to users about the weather.\",\n",
      "    tools=[get_weather],\n",
      ")\n",
      "```\n",
      "This code snippet creates an agent named \"Weather agent\" with specific instructions and a tool to get weather information [1].\n",
      "\n",
      "By following these principles and the foundational framework for agent design, you can start building efficient and effective agents using OpenAI's SDK [1].\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test 3: Cache MISS â€” routes to OPENAI_QUERY and stores result\n",
    "result = agentic_rag_with_cache(\"How do I build an agent with the OpenAI Agents SDK?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "test4-cell",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "test4-cell",
    "outputId": "f449dc4f-5cb1-4c22-bdd9-1af909926cce"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[1m\u001b[96mğŸ‘¤ User Query:\u001b[0m What are the best AI tools this week?\n",
      "\n",
      "\u001b[93mâ° Time-sensitive query detected â€” bypassing semantic cache to ensure a fresh answer.\u001b[0m\n",
      "\n",
      "\u001b[90mğŸ“ Route: INTERNET_QUERY\n",
      "ğŸ“ Reason: General inquiry about AI tools.\u001b[0m\n",
      "Getting your response from the internet ğŸŒ ...\n",
      "\n",
      "\u001b[1m\u001b[96mğŸ¤– Response (live):\u001b[0m\n",
      "[1] The Best AI Tools for 2026\n",
      "    Without a doubt, ChatGPT, Gemini, and Claude are the best AI tools to date. They can provide answers to your everyday questions, do web searches ...\n",
      "    Source: https://medium.com/artificial-corner/the-best-ai-tools-for-2026-933535a44f8b\n",
      "\n",
      "[2] I tried 70+ best AI tools in 2026\n",
      "    I went deep into each tool, from image generation to email automation, chatbot building to scheduling assistants.\n",
      "    Source: https://www.techradar.com/best/best-ai-tools\n",
      "\n",
      "[3] I Tested 47 AI Tools in 30 Days â€” Here Are the Only 7 Worth ...\n",
      "    I Tested 47 AI Tools in 30 Days â€” Here Are the Only 7 Worth Paying For How I cut my workday from 10 hours to 6 without sacrificing quality ...\n",
      "    Source: https://levelup.gitconnected.com/i-tested-47-ai-tools-in-30-days-here-are-the-only-7-worth-paying-for-ad31b1a9c0ab\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test 4: Time-sensitive query â€” BYPASSES cache, calls Ares API for live answer\n",
    "result = agentic_rag_with_cache(\"What are the best AI tools this week?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "test5-cell",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "test5-cell",
    "outputId": "4737d561-5ad9-4d19-fb53-641df6cd77b0"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[1m\u001b[96mğŸ‘¤ User Query:\u001b[0m What is the current stock price of Apple?\n",
      "\n",
      "\u001b[93mâ° Time-sensitive query detected â€” bypassing semantic cache to ensure a fresh answer.\u001b[0m\n",
      "\n",
      "\u001b[90mğŸ“ Route: INTERNET_QUERY\n",
      "ğŸ“ Reason: Question on real-time stock price\u001b[0m\n",
      "Getting your response from the internet ğŸŒ ...\n",
      "\n",
      "\u001b[1m\u001b[96mğŸ¤– Response (live):\u001b[0m\n",
      "[1] Stock Price - Apple - Investor Relations\n",
      "    Stock Quote: NASDAQ: AAPL ; Day's Open262.60 ; Closing Price260.58 ; Volume30.8 ; Intraday High264.48 ; Intraday Low260.05.\n",
      "    Source: https://investor.apple.com/stock-price/default.aspx\n",
      "\n",
      "[2] AAPL: Apple Inc - Stock Price, Quote and News\n",
      "    Apple Inc AAPL:NASDAQ ; Open258.97 ; Day High264.75 ; Day Low258.16 ; Prev Close260.58 ; 52 Week High288.62 ...\n",
      "    Source: https://www.cnbc.com/quotes/AAPL\n",
      "\n",
      "[3] Buy or Sell Apple Stock - AAPL Stock Price Quote & News\n",
      "    Shares are currently priced at $261.38, which is +1.2% above the low and -1.3% below the high. Apple(AAPL) shares are trading with a volume of 42.05M, against a ...\n",
      "    Source: https://robinhood.com/us/en/stocks/AAPL/\n",
      "\n",
      "[4] Apple Inc. Stock Quote (U.S.: Nasdaq) - AAPL\n",
      "    264.57 ; Volume: 40.88M Â· 65 Day Avg: 48.26M ; 258.16 Day Range 264.75 ; 169.21 52 Week Range 288.62 ...\n",
      "    Source: https://www.marketwatch.com/investing/stock/aapl?gaa_at=eafs&gaa_n=AWEtsqcOslNE7J82ldssL4ASeXAF2bBFfW7dg6bdjyEmWnLCy8D24jYy3wEk&gaa_ts=6998f9ca&gaa_sig=4x3yowkl9IvIt9CyUmS-Xmnz-aGsUpX-8nHTKIpELs2VpgDZtGsNOi3miHD8PPZTYDD6VbVK868ogZs_EmKyaw%3D%3D\n",
      "\n",
      "[5] Apple Inc. (AAPL) Stock Price, News, Quote & History\n",
      "    Find the latest Apple Inc. (AAPL) stock quote, history, news and other vital information to help you with your stock trading and investing.\n",
      "    Source: https://finance.yahoo.com/quote/AAPL/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test 5: Time-sensitive query â€” stock price, always fetched live\n",
    "result = agentic_rag_with_cache(\"What is the current stock price of Apple?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "test6-cell",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "test6-cell",
    "outputId": "ed68ebb5-e8f9-46da-d837-48fb76d5d0a6"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[1m\u001b[96mğŸ‘¤ User Query:\u001b[0m What are the most popular open-source LLMs?\n",
      "\n",
      "\u001b[93mâŒ Semantic Cache MISS â€” running Agentic RAG pipeline...\u001b[0m\n",
      "\n",
      "\u001b[90mğŸ“ Route: INTERNET_QUERY\n",
      "ğŸ“ Reason: General inquiry about open-source LLMs.\u001b[0m\n",
      "Getting your response from the internet ğŸŒ ...\n",
      "\n",
      "\u001b[92mğŸ’¾ Answer cached for future similar queries.\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[96mğŸ¤– Response:\u001b[0m\n",
      "[1] Open LLM Leaderboard 2025\n",
      "    This LLM leaderboard displays the latest public benchmark performance for SOTA open-sourced model versions released after April 2024.\n",
      "    Source: https://www.vellum.ai/open-llm-leaderboard\n",
      "\n",
      "[2] Top 10 open source LLMs for 2025\n",
      "    Top open source LLMs in 2024 Â· 1. LLaMA 3 Â· 2. Google Gemma 2 Â· 3. Command R+ Â· 4. Mistral-8x22b Â· 5. Falcon 2 Â· 6. Grok 1.5 Â· 7. Qwen1.5 Â· 8. BLOOM.\n",
      "    Source: https://www.instaclustr.com/education/open-source-ai/top-10-open-source-llms-for-2025/\n",
      "\n",
      "[3] I locally benchmarked 41 open-source LLMs across 19 ...\n",
      "    Hello everyone! I benchmarked 41 open-source LLMs using lm-evaluation-harness. Here are the 19 tasks covered:.\n",
      "    Source: https://www.reddit.com/r/LocalLLaMA/comments/1n57hb8/i_locally_benchmarked_41_opensource_llms_across/\n",
      "\n",
      "[4] The best open source large language model\n",
      "    Best overall open source LLM: Llama 3.3 70B Instruct Â· Llama 3.3 70B only supports eight languages, while many models support 2-3x as many.\n",
      "    Source: https://www.baseten.co/blog/the-best-open-source-large-language-model/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test 6: Cache MISS â€” INTERNET_QUERY, stored after Ares API call\n",
    "result = agentic_rag_with_cache(\"What are the most popular open-source LLMs?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "test7-cell",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "test7-cell",
    "outputId": "2289624e-3de6-4f4a-8835-f3397d5801ef"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[1m\u001b[96mğŸ‘¤ User Query:\u001b[0m Which open-source large language models are most widely used?\n",
      "\n",
      "\u001b[93mâŒ Semantic Cache MISS â€” running Agentic RAG pipeline...\u001b[0m\n",
      "\n",
      "\u001b[90mğŸ“ Route: INTERNET_QUERY\n",
      "ğŸ“ Reason: Question on general open-source LLMs.\u001b[0m\n",
      "Getting your response from the internet ğŸŒ ...\n",
      "\n",
      "\u001b[92mğŸ’¾ Answer cached for future similar queries.\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[96mğŸ¤– Response:\u001b[0m\n",
      "[1] Top 10 open source LLMs for 2025\n",
      "    Unlike proprietary models developed by companies like OpenAI and Google, open source LLMs are licensed to be freely used, modified, and distributed by anyone.\n",
      "    Source: https://www.instaclustr.com/education/open-source-ai/top-10-open-source-llms-for-2025/\n",
      "\n",
      "[2] 9 Top Open-Source LLMs for 2026 and Their Uses\n",
      "    LLM are the foundation models of popular and widely-used chatbots, like ChatGPT and Google Gemini. ... most powerful open-source large language ...\n",
      "    Source: https://www.datacamp.com/blog/top-open-source-llms\n",
      "\n",
      "[3] The best open source large language model\n",
      "    The largest open-source models, DeepSeek-V3 and DeepSeek-R1, match GPT-4o and o1-pro, respectively. Newer open source LLMs like Nemotron Llama ...\n",
      "    Source: https://www.baseten.co/blog/the-best-open-source-large-language-model/\n",
      "\n",
      "[4] Which LLM's are the best and opensource for code ...\n",
      "    Phi-4 is surprisingly good, even better than Qwen Coder 32B for JS and web-based stuff. It's not as good as DeepSeek V3, but it's shockingly comparable for ...\n",
      "    Source: https://www.reddit.com/r/LocalLLaMA/comments/1jn1njb/which_llms_are_the_best_and_opensource_for_code/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test 7: Cache HIT â€” similar to Test 6\n",
    "result = agentic_rag_with_cache(\"Which open-source large language models are most widely used?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inspect-md",
   "metadata": {
    "id": "inspect-md"
   },
   "source": "## 6. Inspect the Cache\n\nView all entries currently stored in the semantic cache."
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "inspect-cell",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "inspect-cell",
    "outputId": "cab240df-7854-4e0f-eccf-4e276a36ed54"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total cached entries: 4\n",
      "FAISS index size: 4\n",
      "\n",
      "[0] Q: What was Uber's revenue in 2021?\n",
      "    A: Uber's revenue in 2021 was $3,208,323,000 [1].\n",
      "\n",
      "[1] Q: How do I build an agent with the OpenAI Agents SDK?\n",
      "    A: To build an agent with the OpenAI Agents SDK, you need to focus on three core components: Model, Tools, and Instructions...\n",
      "\n",
      "[2] Q: What are the most popular open-source LLMs?\n",
      "    A: [1] Open LLM Leaderboard 2025\n",
      "    This LLM leaderboard displays the latest public benchmark performance for SOTA open-so...\n",
      "\n",
      "[3] Q: Which open-source large language models are most widely used?\n",
      "    A: [1] Top 10 open source LLMs for 2025\n",
      "    Unlike proprietary models developed by companies like OpenAI and Google, open s...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total cached entries: {len(cache.cache['questions'])}\")\n",
    "print(f\"FAISS index size: {cache.index.ntotal}\\n\")\n",
    "\n",
    "for i, (q, a) in enumerate(zip(cache.cache['questions'], cache.cache['response_text'])):\n",
    "    print(f\"[{i}] Q: {q}\")\n",
    "    print(f\"    A: {a[:120]}...\\n\" if len(a) > 120 else f\"    A: {a}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assignment-md",
   "metadata": {
    "id": "assignment-md"
   },
   "source": [
    "## Assignment: Extend the System\n",
    "\n",
    "Try one or more of these extensions:\n",
    "\n",
    "1. **Adjustable similarity threshold** â€” Experiment with `threshold=0.1` (stricter) vs `threshold=0.35` (looser). How does it affect hit rate and answer quality?\n",
    "\n",
    "2. **Cache TTL (Time-To-Live)** â€” Add an expiry timestamp to each cache entry. Stale entries (e.g., older than 7 days) should be evicted and re-fetched.\n",
    "\n",
    "3. **Sub-query division** â€” Before checking the cache, use a GPT call to split compound questions (e.g., *\"What was Uber and Lyft revenue in 2021?\"*) into sub-queries. Check and populate the cache per sub-query.\n",
    "\n",
    "4. **Cache analytics** â€” Track and display cache hit rate, average latency for hits vs misses, and the most-queried topics over a session."
   ]
  }
 ]
}