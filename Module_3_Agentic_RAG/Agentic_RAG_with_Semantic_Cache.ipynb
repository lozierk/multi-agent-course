{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": []
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title-cell",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/hamzafarooq/multi-agent-course/blob/main/Module_3_Agentic_RAG/Agentic_RAG_with_Semantic_Cache.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intro-cell",
   "metadata": {},
   "source": "# Agentic RAG with Semantic Cache\n\nThis notebook combines two powerful concepts:\n\n- **Semantic Cache** â€” A FAISS-backed cache that stores previous query embeddings and their answers. When a new query is semantically similar to a cached one, the stored answer is returned instantly â€” no LLM or API call needed.\n- **Agentic RAG** â€” An intelligent retrieval system that routes queries to the right knowledge source: OpenAI documentation (via Qdrant), 10-K financial filings (via Qdrant), or live internet search (via SerpApi).\n\n## Architecture\n\n```\nUser Query\n    â”‚\n    â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  Is query time-sensitive?   â”‚  â”€â”€YESâ”€â”€â–¶  Agentic RAG (no caching)\nâ”‚  (current events, \"today\",  â”‚\nâ”‚   live data, etc.)          â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n               â”‚ NO\n               â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚   Semantic Cache Lookup     â”‚  â”€â”€HITâ”€â”€â–¶  Return cached answer âš¡\nâ”‚   (FAISS similarity search) â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n               â”‚ MISS\n               â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚      Agentic RAG Router     â”‚\nâ”‚   (GPT-4o classifies query) â”‚\nâ””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n       â”‚          â”‚           â”‚\n  OPENAI      10K_DOC    INTERNET\n  QUERY       QUERY       QUERY\n    â”‚            â”‚            â”‚\n  Qdrant      Qdrant      SerpApi\n  (RAG)       (RAG)      (live web)\n       â”‚          â”‚           â”‚\n       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n               â”‚\n               â–¼\n    Store answer in cache ğŸ’¾\n               â”‚\n               â–¼\n          Return answer\n```\n\n## Why combine them?\n\n- **Speed**: Cached answers return in milliseconds vs. 2â€“5 seconds for full RAG.\n- **Cost**: Fewer LLM and API calls for repeated or similar questions.\n- **Correctness**: Time-sensitive queries (e.g., *\"What happened today?\"*) always bypass the cache to ensure fresh answers."
  },
  {
   "cell_type": "markdown",
   "id": "setup-md",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-cell",
   "metadata": {},
   "outputs": [],
   "source": "!pip install -U faiss-cpu sentence_transformers transformers openai qdrant_client python-dotenv"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports-cell",
   "metadata": {},
   "outputs": [],
   "source": "import faiss                          # Efficient vector similarity search\nimport json                           # JSON persistence for cache\nimport numpy as np                    # Numerical operations on embeddings\nimport requests                       # HTTP calls to SerpApi and Qdrant\nimport re                             # Regex for parsing LLM JSON responses\nimport time                           # Latency measurement\nimport asyncio                        # Async support for Qdrant retrieval\nimport nest_asyncio                   # Allow nested event loops in Colab/Jupyter\nimport os\n\nfrom sentence_transformers import SentenceTransformer      # Nomic embed model wrapper\nfrom transformers import AutoTokenizer, AutoModel          # Tokenizer + model for RAG embeddings\nfrom openai import OpenAI, OpenAIError                     # OpenAI client for routing + generation\nimport qdrant_client                                       # Qdrant vector database client\nfrom qdrant_client import models\n\nnest_asyncio.apply()  # Allow asyncio.run() inside Jupyter/Colab"
  },
  {
   "cell_type": "markdown",
   "id": "api-keys-md",
   "metadata": {},
   "source": "## 2. API Keys\n\n**On Google Colab** â€” store keys in the Secrets panel (`ğŸ”‘` icon, left sidebar):\n| Secret name | Where to get it |\n|---|---|\n| `SERP_API_KEY` | [serpapi.com](https://serpapi.com) |\n| `OPENAI_API_KEY` | [platform.openai.com](https://platform.openai.com) |\n\n**Running locally** â€” add keys to `Module_3_Agentic_RAG/.env`:\n```\nserp_api_key=<your_key>\nopenai_api_key=<your_key>\n```\nThe cell below detects the environment automatically and loads from the right source."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "api-keys-cell",
   "metadata": {},
   "outputs": [],
   "source": "# â”€â”€ Credential loading â€” works on Colab and locally â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ntry:\n    from google.colab import userdata\n    serp_api_key   = userdata.get('SERP_API_KEY')\n    openai_api_key = userdata.get('OPENAI_API_KEY')\n    print(\"Running on Colab â€” credentials loaded from Secrets.\")\nexcept ImportError:\n    from dotenv import load_dotenv, find_dotenv\n    load_dotenv(find_dotenv())  # walks up the directory tree to find .env\n    serp_api_key   = os.getenv(\"serp_api_key\")   or os.getenv(\"SERP_API_KEY\")\n    openai_api_key = os.getenv(\"openai_api_key\") or os.getenv(\"OPENAI_API_KEY\")\n    print(\"Running locally â€” credentials loaded from .env file.\")\n\nprint(f\"SerpApi key loaded:    {'âœ…' if serp_api_key else 'âŒ MISSING'}\")\nprint(f\"OpenAI API key loaded: {'âœ…' if openai_api_key else 'âŒ MISSING'}\")\n\n# Initialize OpenAI client â€” used for query routing and RAG response generation\nopenaiclient = OpenAI(api_key=openai_api_key)"
  },
  {
   "cell_type": "markdown",
   "id": "cache-md",
   "metadata": {},
   "source": [
    "## 3. Semantic Cache with Time-Sensitivity Filter\n",
    "\n",
    "The `SemanticCaching` class provides:\n",
    "\n",
    "- **`is_time_sensitive(question)`** â€” Detects questions that should never be cached because their answers change over time (e.g., *\"What is the weather today?\"*, *\"Latest AI news this week\"*). These go directly to the Agentic RAG pipeline for a fresh answer.\n",
    "- **`check_cache(question)`** â€” Embeds the query and checks FAISS for a near-neighbor within the similarity threshold. Returns hit/miss, the cached answer if available, and the computed embedding (reused on cache miss to avoid re-encoding).\n",
    "- **`add_to_cache(question, answer, embedding)`** â€” Stores a new entry after a successful RAG response.\n",
    "\n",
    "### What counts as time-sensitive?\n",
    "\n",
    "A question is flagged as time-sensitive if it contains temporal indicators such as:\n",
    "> `today`, `now`, `current`, `latest`, `this week`, `yesterday`, `upcoming`, `live`, `breaking`, `stock price`, `weather`, `forecast`, `real-time`, `last month`, `this quarter` â€¦\n",
    "\n",
    "These questions are intentionally excluded from caching because a cached answer from last week would be incorrect today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cache-class-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticCaching:\n",
    "    \"\"\"\n",
    "    A semantic cache backed by FAISS that:\n",
    "    - Detects and bypasses time-sensitive queries\n",
    "    - Returns cached answers for semantically similar past queries\n",
    "    - Persists cache entries to disk as JSON\n",
    "    \"\"\"\n",
    "\n",
    "    # Keywords that indicate a question is time-sensitive and should NOT be cached\n",
    "    TIME_SENSITIVE_KEYWORDS = [\n",
    "        \"today\", \"tonight\", \"now\", \"currently\", \"current\",\n",
    "        \"latest\", \"recent\", \"recently\", \"right now\", \"at the moment\",\n",
    "        \"at present\", \"as of now\", \"this week\", \"this month\", \"this year\",\n",
    "        \"this quarter\", \"this season\", \"this morning\", \"this afternoon\",\n",
    "        \"this evening\", \"this weekend\", \"yesterday\", \"tomorrow\",\n",
    "        \"last week\", \"last month\", \"last year\", \"upcoming\", \"live\",\n",
    "        \"breaking\", \"just happened\", \"what time\", \"what day\", \"what date\",\n",
    "        \"happening now\", \"events today\", \"news today\", \"news this week\",\n",
    "        \"stock price\", \"share price\", \"weather\", \"forecast\", \"temperature\",\n",
    "        \"real-time\", \"realtime\", \"right now\", \"schedule today\",\n",
    "    ]\n",
    "\n",
    "    def __init__(self, json_file='rag_cache.json', threshold=0.2, clear_on_init=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            json_file (str): Path to the JSON file for cache persistence.\n",
    "            threshold (float): Max Euclidean distance to count as a cache hit (lower = stricter).\n",
    "            clear_on_init (bool): If True, wipe existing cache on startup.\n",
    "        \"\"\"\n",
    "        self.embedding_dim = 768\n",
    "        self.index = faiss.IndexFlatL2(self.embedding_dim)\n",
    "        self.euclidean_threshold = threshold\n",
    "        self.json_file = json_file\n",
    "\n",
    "        # Load the Nomic embedding model for query encoding\n",
    "        print(\"Loading Nomic embedding model...\")\n",
    "        self.encoder = SentenceTransformer(\n",
    "            'nomic-ai/nomic-embed-text-v1.5',\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        print(\"Embedding model ready.\")\n",
    "\n",
    "        if clear_on_init:\n",
    "            self.clear_cache()\n",
    "        else:\n",
    "            self.load_cache()\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Time-sensitivity detection\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    def is_time_sensitive(self, question: str) -> bool:\n",
    "        \"\"\"\n",
    "        Returns True if the question references time-dependent information\n",
    "        that should NOT be cached (answers change frequently).\n",
    "\n",
    "        Examples that return True:\n",
    "            'What is the weather today?'\n",
    "            'Who won the game last night?'\n",
    "            'What are the latest AI news this week?'\n",
    "            'What is the current stock price of Apple?'\n",
    "\n",
    "        Examples that return False (safe to cache):\n",
    "            'What is the capital of France?'\n",
    "            'Who founded Apple?'\n",
    "            'What is machine learning?'\n",
    "            'What was Uber revenue in 2021?'\n",
    "        \"\"\"\n",
    "        question_lower = question.lower()\n",
    "        return any(keyword in question_lower for keyword in self.TIME_SENSITIVE_KEYWORDS)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Cache persistence\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    def clear_cache(self):\n",
    "        \"\"\"Reset all in-memory state and overwrite the JSON file.\"\"\"\n",
    "        self.cache = {'questions': [], 'embeddings': [], 'response_text': []}\n",
    "        self.index = faiss.IndexFlatL2(self.embedding_dim)\n",
    "        self.save_cache()\n",
    "        print(\"Semantic cache cleared.\")\n",
    "\n",
    "    def load_cache(self):\n",
    "        \"\"\"Load cache from JSON and rebuild the FAISS index from stored embeddings.\"\"\"\n",
    "        try:\n",
    "            with open(self.json_file, 'r') as f:\n",
    "                self.cache = json.load(f)\n",
    "            # Rebuild FAISS index so lookups work after a cold start\n",
    "            if self.cache['embeddings']:\n",
    "                embeddings = np.array(self.cache['embeddings'], dtype=np.float32)\n",
    "                self.index.add(embeddings)\n",
    "            print(f\"Cache loaded: {len(self.cache['questions'])} entries.\")\n",
    "        except FileNotFoundError:\n",
    "            self.cache = {'questions': [], 'embeddings': [], 'response_text': []}\n",
    "            print(\"No existing cache found â€” starting fresh.\")\n",
    "\n",
    "    def save_cache(self):\n",
    "        \"\"\"Persist the cache to disk.\"\"\"\n",
    "        with open(self.json_file, 'w') as f:\n",
    "            json.dump(self.cache, f)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Cache lookup and storage\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    def check_cache(self, question: str):\n",
    "        \"\"\"\n",
    "        Encode the question and search the FAISS index for a near-enough neighbor.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (\n",
    "                hit (bool),           -- True if a cached answer was found\n",
    "                answer (str|None),    -- Cached answer text, or None on miss\n",
    "                embedding (np.array), -- The computed embedding (reuse on cache miss)\n",
    "                similarity (float|None), -- Similarity score [0,1], or None on miss\n",
    "                row_id (int|None),    -- Index of the matched row, or None on miss\n",
    "            )\n",
    "        \"\"\"\n",
    "        embedding = self.encoder.encode([question], normalize_embeddings=True)\n",
    "\n",
    "        # Nothing in the index yet â€” guaranteed miss\n",
    "        if self.index.ntotal == 0:\n",
    "            return False, None, embedding, None, None\n",
    "\n",
    "        D, I = self.index.search(embedding, 1)\n",
    "\n",
    "        if I[0][0] != -1 and D[0][0] <= self.euclidean_threshold:\n",
    "            row_id = int(I[0][0])\n",
    "            similarity = float(1.0 - D[0][0])  # Convert distance â†’ similarity score\n",
    "            return True, self.cache['response_text'][row_id], embedding, similarity, row_id\n",
    "\n",
    "        return False, None, embedding, None, None\n",
    "\n",
    "    def add_to_cache(self, question: str, answer: str, embedding: np.ndarray):\n",
    "        \"\"\"\n",
    "        Store a new question-answer pair.\n",
    "\n",
    "        Args:\n",
    "            question (str): The original user query.\n",
    "            answer (str): The answer from the Agentic RAG pipeline.\n",
    "            embedding (np.ndarray): Pre-computed query embedding (shape: [1, 768]).\n",
    "        \"\"\"\n",
    "        self.cache['questions'].append(question)\n",
    "        self.cache['embeddings'].append(embedding[0].tolist())\n",
    "        self.cache['response_text'].append(answer)\n",
    "        self.index.add(embedding)\n",
    "        self.save_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "init-cache-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the semantic cache\n",
    "# Set clear_on_init=True to wipe any previously stored entries\n",
    "cache = SemanticCaching(json_file='rag_cache.json', threshold=0.2, clear_on_init=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rag-components-md",
   "metadata": {},
   "source": "## 4. Agentic RAG Components\n\nThese are the building blocks of the Agentic RAG pipeline, adapted from the `Agentic_RAG_Notebook.ipynb`.\n\n### Components\n| Component | Purpose |\n|---|---|\n| `get_internet_content()` | Fetches real-time answers via SerpApi (Google search) |\n| `route_query()` | Uses GPT-4o to classify the query into `OPENAI_QUERY`, `10K_DOCUMENT_QUERY`, or `INTERNET_QUERY` |\n| `get_text_embeddings()` | Converts text to dense vectors using Nomic embed model |\n| `rag_formatted_response()` | Generates a grounded answer from retrieved Qdrant chunks |\n| `retrieve_and_response()` | Async: embeds query â†’ searches Qdrant â†’ calls RAG generator |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "internet-tool-cell",
   "metadata": {},
   "outputs": [],
   "source": "def get_internet_content(user_query: str, action: str = \"INTERNET_QUERY\") -> str:\n    \"\"\"\n    Search Google in real time using SerpApi and return a formatted answer.\n\n    Used for INTERNET_QUERY routes â€” queries that need live web information\n    beyond what is available in the Qdrant document collections.\n\n    Args:\n        user_query (str): The user's question.\n        action (str): Route label (always \"INTERNET_QUERY\" here).\n\n    Returns:\n        str: Formatted string combining Google's answer box and top organic\n             result snippets, or an error message.\n    \"\"\"\n    print(\"Getting your response from the internet ğŸŒ ...\")\n\n    if not serp_api_key:\n        return \"Error: Missing SERP_API_KEY â€” add it to Colab Secrets or .env\"\n\n    params = {\n        \"q\": user_query,\n        \"api_key\": serp_api_key,\n        \"engine\": \"google\",\n        \"num\": 5,\n    }\n\n    try:\n        response = requests.get(\"https://serpapi.com/search.json\", params=params)\n        response.raise_for_status()\n        data = response.json()\n\n        parts = []\n\n        # Answer box â€” Google's highlighted direct answer (most relevant)\n        answer_box = data.get(\"answer_box\", {})\n        if answer_box.get(\"answer\"):\n            parts.append(f\"[Direct Answer] {answer_box['answer']}\")\n        elif answer_box.get(\"snippet\"):\n            parts.append(f\"[Direct Answer] {answer_box['snippet']}\")\n\n        # Top organic results â€” titles + snippets\n        for i, result in enumerate(data.get(\"organic_results\", [])[:5], start=1):\n            title   = result.get(\"title\", \"\")\n            snippet = result.get(\"snippet\", \"\")\n            link    = result.get(\"link\", \"\")\n            if snippet:\n                parts.append(f\"[{i}] {title}\\n    {snippet}\\n    Source: {link}\")\n\n        return \"\\n\\n\".join(parts) if parts else \"No results found.\"\n\n    except requests.exceptions.RequestException as e:\n        return f\"SerpApi request error: {e}\"\n    except Exception as e:\n        return f\"Unexpected error: {e}\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "router-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_query(user_query: str) -> dict:\n",
    "    \"\"\"\n",
    "    Classify the user query into one of three categories using GPT-4o.\n",
    "\n",
    "    Returns:\n",
    "        dict with keys:\n",
    "            'action': 'OPENAI_QUERY' | '10K_DOCUMENT_QUERY' | 'INTERNET_QUERY'\n",
    "            'reason': brief justification\n",
    "            'answer': short answer if applicable, else empty string\n",
    "    \"\"\"\n",
    "    router_system_prompt = f\"\"\"\n",
    "    As a professional query router, classify user input into one of three categories:\n",
    "    1. \"OPENAI_QUERY\": Questions about OpenAI's documentation â€” agents, tools, APIs, models, embeddings, guardrails.\n",
    "    2. \"10K_DOCUMENT_QUERY\": Questions about company financials, 10-K annual reports, Uber or Lyft filings.\n",
    "    3. \"INTERNET_QUERY\": Everything else â€” general knowledge, comparisons, trends, or anything not in internal docs.\n",
    "\n",
    "    Always respond in this valid JSON format:\n",
    "    {{\n",
    "        \"action\": \"OPENAI_QUERY\" or \"10K_DOCUMENT_QUERY\" or \"INTERNET_QUERY\",\n",
    "        \"reason\": \"brief justification\",\n",
    "        \"answer\": \"AT MAX 5 words. Leave empty if INTERNET_QUERY\"\n",
    "    }}\n",
    "\n",
    "    User: {user_query}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = openaiclient.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\"role\": \"system\", \"content\": router_system_prompt}]\n",
    "        )\n",
    "        task_response = response.choices[0].message.content\n",
    "        json_match = re.search(r\"\\{.*\\}\", task_response, re.DOTALL)\n",
    "        return json.loads(json_match.group())\n",
    "    except (OpenAIError, json.JSONDecodeError, AttributeError) as err:\n",
    "        return {\"action\": \"INTERNET_QUERY\", \"reason\": f\"Routing error: {err}\", \"answer\": \"\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qdrant-md",
   "metadata": {},
   "source": [
    "### Qdrant Vector Database Setup\n",
    "\n",
    "We load the pre-built Qdrant collections from the course repository. Two collections are available:\n",
    "- `opnai_data` â€” OpenAI Agents documentation\n",
    "- `10k_data` â€” Uber 2021 and Lyft 2024 10-K filings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qdrant-setup-cell",
   "metadata": {},
   "outputs": [],
   "source": "import os\n\n# Resolve the path to the prebuilt Qdrant vector database.\n# On Colab: clone the repo if needed, then point to /content/multi-agent-course/...\n# Locally:  the qdrant_data folder is already present in the cloned repo next to this notebook.\n\ntry:\n    from google.colab import userdata  # will only succeed on Colab\n    QDRANT_PATH = \"/content/multi-agent-course/Module_3_Agentic_RAG/Agentic_RAG/qdrant_data\"\n    if not os.path.exists(\"/content/multi-agent-course\"):\n        import subprocess\n        subprocess.run([\"git\", \"clone\", \"https://github.com/hamzafarooq/multi-agent-course.git\",\n                        \"/content/multi-agent-course\"], check=True)\n        print(\"Repository cloned.\")\n    else:\n        print(\"Repository already present.\")\nexcept ImportError:\n    # Running locally â€” qdrant_data is in the Agentic_RAG sub-folder\n    NOTEBOOK_DIR = os.path.dirname(os.path.abspath(\"__file__\"))\n    QDRANT_PATH = os.path.join(NOTEBOOK_DIR, \"Agentic_RAG\", \"qdrant_data\")\n    print(f\"Running locally â€” Qdrant path: {QDRANT_PATH}\")\n\nprint(f\"Qdrant data path: {QDRANT_PATH}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qdrant-client-cell",
   "metadata": {},
   "outputs": [],
   "source": "# Initialize async Qdrant client using the resolved path from the cell above\nqdrant = qdrant_client.AsyncQdrantClient(path=QDRANT_PATH)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embeddings-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and model for query embedding during Qdrant retrieval\n",
    "# Note: The Nomic model is already loaded inside SemanticCaching above.\n",
    "# We use AutoTokenizer/AutoModel here to match the Agentic RAG notebook's approach\n",
    "# (mean-pooling over token embeddings, without normalization â€” different from SentenceTransformer).\n",
    "text_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"nomic-ai/nomic-embed-text-v1.5\", trust_remote_code=True\n",
    ")\n",
    "text_model = AutoModel.from_pretrained(\n",
    "    \"nomic-ai/nomic-embed-text-v1.5\", trust_remote_code=True\n",
    ")\n",
    "\n",
    "def get_text_embeddings(text: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Embed a text string using mean pooling over token embeddings.\n",
    "    Used for Qdrant retrieval queries.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text to embed.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: 1-D embedding vector of shape (768,).\n",
    "    \"\"\"\n",
    "    inputs = text_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    outputs = text_model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "    return embeddings[0].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rag-response-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_formatted_response(user_query: str, context: list) -> str:\n",
    "    \"\"\"\n",
    "    Generate a grounded answer from retrieved document chunks.\n",
    "    Citations are formatted as [1][2]... matching chunk order.\n",
    "\n",
    "    Args:\n",
    "        user_query (str): The user's original question.\n",
    "        context (list): List of text chunks retrieved from Qdrant.\n",
    "\n",
    "    Returns:\n",
    "        str: Model-generated answer with numbered citations.\n",
    "    \"\"\"\n",
    "    rag_prompt = f\"\"\"\n",
    "    Based on the given context, answer the user query: {user_query}\\nContext:\\n{context}\n",
    "    Employ references to the ID of articles provided [ID], ensuring their relevance to the query.\n",
    "    The referencing should always be in the format of [1][2]... etc.\n",
    "    \"\"\"\n",
    "    response = openaiclient.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[{\"role\": \"system\", \"content\": rag_prompt}]\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retrieve-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def retrieve_and_response(user_query: str, action: str) -> str:\n",
    "    \"\"\"\n",
    "    Retrieve relevant chunks from the appropriate Qdrant collection\n",
    "    and generate a RAG response.\n",
    "\n",
    "    Args:\n",
    "        user_query (str): The user's question.\n",
    "        action (str): 'OPENAI_QUERY' or '10K_DOCUMENT_QUERY'.\n",
    "\n",
    "    Returns:\n",
    "        str: Generated answer grounded in retrieved document chunks.\n",
    "    \"\"\"\n",
    "    collections = {\n",
    "        \"OPENAI_QUERY\": \"opnai_data\",\n",
    "        \"10K_DOCUMENT_QUERY\": \"10k_data\",\n",
    "    }\n",
    "    if action not in collections:\n",
    "        return \"Invalid action type for retrieval.\"\n",
    "\n",
    "    try:\n",
    "        query_embedding = get_text_embeddings(user_query)\n",
    "    except Exception as e:\n",
    "        return f\"Embedding error: {e}\"\n",
    "\n",
    "    try:\n",
    "        text_hits = await qdrant.query_points(\n",
    "            collection_name=collections[action],\n",
    "            query=query_embedding,\n",
    "            limit=3\n",
    "        )\n",
    "    except Exception as e:\n",
    "        return f\"Qdrant query error: {e}\"\n",
    "\n",
    "    contents = [point.payload['content'] for point in text_hits.points]\n",
    "    if not contents:\n",
    "        return \"No relevant content found in the database.\"\n",
    "\n",
    "    try:\n",
    "        return rag_formatted_response(user_query, contents)\n",
    "    except Exception as e:\n",
    "        return f\"RAG generation error: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "combine-md",
   "metadata": {},
   "source": [
    "## 5. Combining Agentic RAG with Semantic Cache\n",
    "\n",
    "The `agentic_rag_with_cache()` function is the main entry point. It:\n",
    "\n",
    "1. **Checks time-sensitivity** â€” time-dependent questions always get a fresh answer.\n",
    "2. **Checks the semantic cache** â€” similar past queries return instantly.\n",
    "3. **Runs full Agentic RAG** on cache misses via `_get_rag_result()`, which:\n",
    "   - Routes the query (GPT-4o â†’ `OPENAI_QUERY` / `10K_DOCUMENT_QUERY` / `INTERNET_QUERY`)\n",
    "   - Calls the appropriate retrieval or search function\n",
    "   - Returns the answer as a string\n",
    "4. **Stores the result** in the cache for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "routes-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maps routing labels to their handler functions\n",
    "routes = {\n",
    "    \"OPENAI_QUERY\": retrieve_and_response,\n",
    "    \"10K_DOCUMENT_QUERY\": retrieve_and_response,\n",
    "    \"INTERNET_QUERY\": get_internet_content,\n",
    "}\n",
    "\n",
    "\n",
    "def _get_rag_result(user_query: str) -> str:\n",
    "    \"\"\"\n",
    "    Run the full Agentic RAG pipeline and return the answer string.\n",
    "\n",
    "    This is a helper used by agentic_rag_with_cache() so the result can be\n",
    "    captured, displayed, and stored in the semantic cache.\n",
    "\n",
    "    Args:\n",
    "        user_query (str): The user's question.\n",
    "\n",
    "    Returns:\n",
    "        str: The final answer from the chosen knowledge source.\n",
    "    \"\"\"\n",
    "    GREY = \"\\033[90m\"\n",
    "    RESET = \"\\033[0m\"\n",
    "\n",
    "    # Step 1: Route the query\n",
    "    route_response = route_query(user_query)\n",
    "    action = route_response.get(\"action\", \"INTERNET_QUERY\")\n",
    "    reason = route_response.get(\"reason\", \"\")\n",
    "\n",
    "    print(f\"{GREY}ğŸ“ Route: {action}\")\n",
    "    print(f\"ğŸ“ Reason: {reason}{RESET}\")\n",
    "\n",
    "    # Step 2: Call the appropriate handler\n",
    "    route_fn = routes.get(action)\n",
    "    if not route_fn:\n",
    "        return f\"Unsupported action: {action}\"\n",
    "\n",
    "    try:\n",
    "        if action in (\"OPENAI_QUERY\", \"10K_DOCUMENT_QUERY\"):\n",
    "            result = asyncio.run(route_fn(user_query, action))\n",
    "        else:\n",
    "            result = route_fn(user_query, action)\n",
    "    except Exception as e:\n",
    "        result = f\"Execution error: {e}\"\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "main-fn-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agentic_rag_with_cache(user_query: str) -> str:\n",
    "    \"\"\"\n",
    "    Main entry point: Agentic RAG with semantic cache layer.\n",
    "\n",
    "    Query flow:\n",
    "        1. If time-sensitive â†’ skip cache, run Agentic RAG directly\n",
    "        2. Check semantic cache â†’ if hit, return cached answer instantly\n",
    "        3. Cache miss â†’ run Agentic RAG, store answer, return answer\n",
    "\n",
    "    Args:\n",
    "        user_query (str): The user's question.\n",
    "\n",
    "    Returns:\n",
    "        str: The answer text.\n",
    "    \"\"\"\n",
    "    CYAN  = \"\\033[96m\"\n",
    "    GREEN = \"\\033[92m\"\n",
    "    YELLOW = \"\\033[93m\"\n",
    "    BOLD  = \"\\033[1m\"\n",
    "    RESET = \"\\033[0m\"\n",
    "\n",
    "    print(f\"{BOLD}{CYAN}ğŸ‘¤ User Query:{RESET} {user_query}\\n\")\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    # Step 1: Time-sensitivity check                                       #\n",
    "    # ------------------------------------------------------------------ #\n",
    "    if cache.is_time_sensitive(user_query):\n",
    "        print(\n",
    "            f\"{YELLOW}â° Time-sensitive query detected â€” bypassing semantic cache \"\n",
    "            f\"to ensure a fresh answer.{RESET}\\n\"\n",
    "        )\n",
    "        result = _get_rag_result(user_query)\n",
    "        print(f\"\\n{BOLD}{CYAN}ğŸ¤– Response (live):{RESET}\\n{result}\\n\")\n",
    "        return result\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    # Step 2: Semantic cache lookup                                        #\n",
    "    # ------------------------------------------------------------------ #\n",
    "    start_time = time.time()\n",
    "    hit, cached_answer, embedding, similarity, row_id = cache.check_cache(user_query)\n",
    "\n",
    "    if hit:\n",
    "        elapsed = time.time() - start_time\n",
    "        print(\n",
    "            f\"{GREEN}âœ… Semantic Cache HIT{RESET} \"\n",
    "            f\"(row {row_id}, similarity: {similarity:.3f}, time: {elapsed:.3f}s)\\n\"\n",
    "        )\n",
    "        print(f\"{BOLD}{CYAN}ğŸ¤– Response (cached):{RESET}\\n{cached_answer}\\n\")\n",
    "        return cached_answer\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    # Step 3: Cache miss â†’ run Agentic RAG                                 #\n",
    "    # ------------------------------------------------------------------ #\n",
    "    print(f\"{YELLOW}âŒ Semantic Cache MISS â€” running Agentic RAG pipeline...{RESET}\\n\")\n",
    "    result = _get_rag_result(user_query)\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    # Step 4: Store the answer for future cache hits                       #\n",
    "    # ------------------------------------------------------------------ #\n",
    "    cache.add_to_cache(user_query, result, embedding)\n",
    "    print(f\"\\n{GREEN}ğŸ’¾ Answer cached for future similar queries.{RESET}\")\n",
    "\n",
    "    print(f\"\\n{BOLD}{CYAN}ğŸ¤– Response:{RESET}\\n{result}\\n\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tests-md",
   "metadata": {},
   "source": "## 6. Testing the Combined System\n\nWe run a series of queries to demonstrate all three pathways:\n\n| Query | Expected path |\n|---|---|\n| *\"What was Uber's revenue in 2021?\"* | Cache MISS â†’ 10K RAG â†’ stored |\n| *\"How much did Uber earn in 2021?\"* | Cache HIT (semantically similar to above) |\n| *\"How to use the OpenAI Agents SDK?\"* | Cache MISS â†’ OpenAI RAG â†’ stored |\n| *\"What are the best AI tools this week?\"* | Time-sensitive â†’ bypass cache â†’ SerpApi |\n| *\"What is the current stock price of Apple?\"* | Time-sensitive â†’ bypass cache â†’ SerpApi |\n| *\"List new LLMs in 2025\"* | Cache MISS â†’ INTERNET â†’ SerpApi â†’ stored |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test1-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Cache MISS â€” should route to 10K_DOCUMENT_QUERY and store result\n",
    "result = agentic_rag_with_cache(\"What was Uber's revenue in 2021?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test2-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Cache HIT â€” semantically similar to Test 1, returns instantly from cache\n",
    "result = agentic_rag_with_cache(\"How much did Uber earn in fiscal year 2021?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test3-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: Cache MISS â€” routes to OPENAI_QUERY and stores result\n",
    "result = agentic_rag_with_cache(\"How do I build an agent with the OpenAI Agents SDK?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test4-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 4: Time-sensitive query â€” BYPASSES cache, calls Ares API for live answer\n",
    "result = agentic_rag_with_cache(\"What are the best AI tools this week?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test5-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 5: Time-sensitive query â€” stock price, always fetched live\n",
    "result = agentic_rag_with_cache(\"What is the current stock price of Apple?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test6-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 6: Cache MISS â€” INTERNET_QUERY, stored after Ares API call\n",
    "result = agentic_rag_with_cache(\"What are the most popular open-source LLMs?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test7-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 7: Cache HIT â€” similar to Test 6\n",
    "result = agentic_rag_with_cache(\"Which open-source large language models are most widely used?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inspect-md",
   "metadata": {},
   "source": [
    "## 7. Inspecting the Cache\n",
    "\n",
    "View the current state of the semantic cache â€” what questions have been cached and what answers are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inspect-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total cached entries: {len(cache.cache['questions'])}\")\n",
    "print(f\"FAISS index size: {cache.index.ntotal}\\n\")\n",
    "\n",
    "for i, (q, a) in enumerate(zip(cache.cache['questions'], cache.cache['response_text'])):\n",
    "    print(f\"[{i}] Q: {q}\")\n",
    "    print(f\"    A: {a[:120]}...\\n\" if len(a) > 120 else f\"    A: {a}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assignment-md",
   "metadata": {},
   "source": [
    "## Assignment: Extend the System\n",
    "\n",
    "Try one or more of these extensions:\n",
    "\n",
    "1. **Adjustable similarity threshold** â€” Experiment with `threshold=0.1` (stricter) vs `threshold=0.35` (looser). How does it affect hit rate and answer quality?\n",
    "\n",
    "2. **Cache TTL (Time-To-Live)** â€” Add an expiry timestamp to each cache entry. Stale entries (e.g., older than 7 days) should be evicted and re-fetched.\n",
    "\n",
    "3. **Sub-query division** â€” Before checking the cache, use a GPT call to split compound questions (e.g., *\"What was Uber and Lyft revenue in 2021?\"*) into sub-queries. Check and populate the cache per sub-query.\n",
    "\n",
    "4. **Cache analytics** â€” Track and display cache hit rate, average latency for hits vs misses, and the most-queried topics over a session."
   ]
  }
 ]
}