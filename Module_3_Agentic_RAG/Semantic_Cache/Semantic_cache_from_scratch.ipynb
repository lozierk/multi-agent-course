{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "p2KzXaJYks8S",
   "metadata": {
    "id": "p2KzXaJYks8S"
   },
   "source": [
    "**If you use our code, please cite:**\n",
    "\n",
    "@misc{2024<br>\n",
    "  title = {Semantic Cache from Scratch},<br>\n",
    "  author = {Hamza Farooq, Darshil Modi, Kanwal Mehreen, Nazila Shafiei},<br>\n",
    "  keywords = {Semantic Cache},<br>\n",
    "  year = {2024},<br>\n",
    "  copyright = {APACHE 2.0 license}<br>\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gxQxLQJymkCl",
   "metadata": {
    "id": "gxQxLQJymkCl"
   },
   "source": [
    "## Semantic Cache\n",
    "\n",
    "Semantic caching accelerates retrieval-augmented workflows by storing and reusing previous embedding-based lookups instead of issuing fresh queries every time. In this notebook, we'll build a lightweight semantic cache from scratch using:\n",
    "\n",
    "- **Nomic text embeddings** (`nomic-ai/nomic-embed-text-v1.5`) to convert documents and queries into dense vectors  \n",
    "- **FAISS** (Facebook AI Similarity Search) to index and quickly search those vectors  \n",
    "- **Traversaal Pro API** to perform RAG over the AWS documentation corpus when a cache miss occurs  \n",
    "\n",
    "Rather than re-computing embeddings and retrieval for every query, our cache lets us:\n",
    "\n",
    "1. **Embed** each new query and check if it's already \"covered\" by a cached result  \n",
    "2. **Fall back** to a full RAG retrieval (and store the new result) only when necessary  \n",
    "3. **Skip the cache entirely** for time-sensitive questions that need a fresh answer  \n",
    "4. **Invoke** the Traversaal Pro API for document-grounded answers on cache misses  \n",
    "\n",
    "This approach reduces redundant compute, lowers end-to-end latency, and makes RAG pipelines more efficient‚Äîespecially when query patterns exhibit repetition or high similarity. We'll walk through:\n",
    "\n",
    "1. Loading the Nomic embed model with `trust_remote_code=True`  \n",
    "2. Building a FAISS index for fast L2 nearest-neighbor lookup  \n",
    "3. Implementing the core cache hit/miss logic with a time-sensitivity filter  \n",
    "4. Falling back to Traversaal Pro RAG API for live document retrieval on cache misses  \n",
    "5. Measuring performance gains against a \"no-cache\" baseline  \n",
    "\n",
    "By the end, you'll have a reusable semantic cache scaffold that you can plug into any RAG or search-over-embeddings pipeline. Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Hm2NYbYQmykl",
   "metadata": {
    "id": "Hm2NYbYQmykl"
   },
   "source": [
    "## Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025_hZMnZUIE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "025_hZMnZUIE",
    "outputId": "0b61c55b-c058-4a2c-cc1d-095320fe263b"
   },
   "outputs": [],
   "source": [
    "# Install the necessary libraries\n",
    "!pip install -U faiss-cpu sentence_transformers transformers python-dotenv einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52273bc0-575b-4007-b63d-bfe53d4abde6",
   "metadata": {
    "id": "52273bc0-575b-4007-b63d-bfe53d4abde6"
   },
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "\n",
    "# FAISS for efficient similarity search over vector embeddings\n",
    "import faiss  # Builds and queries approximate nearest neighbor indices\n",
    "\n",
    "# Lightweight SQL database for caching metadata, query logs, or evaluation results\n",
    "import sqlite3  # Persistence layer for storing cache entries or metrics\n",
    "\n",
    "# SentenceTransformers wrapper around transformer models for text embeddings\n",
    "from sentence_transformers import SentenceTransformer  # Loads Nomic/embed or other SBERT-style models\n",
    "\n",
    "# PyTorch backend required by SentenceTransformer and optional model fine-tuning\n",
    "import torch  # Tensor operations, GPU acceleration, and model inference support\n",
    "\n",
    "# Transformers library components for causal LLM-based answer generation\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "#   - AutoModelForCausalLM: Load pretrained language models (e.g., GPT variants)\n",
    "#   - AutoTokenizer: Tokenize text input/output for the LLM\n",
    "\n",
    "# Core numerical library for array and matrix operations on embeddings\n",
    "import numpy as np  # Handles vector math, concatenation, and statistical computations\n",
    "\n",
    "# Pretty-printing complex Python objects during development/debugging\n",
    "from pprint import pprint  # Nicely formats nested dicts or lists when exploring outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4JHRRiuKlM3t",
   "metadata": {
    "id": "4JHRRiuKlM3t"
   },
   "source": [
    "# Define the Retrieval Functions\n",
    "\n",
    "This notebook uses **two different APIs** depending on whether a question is stable or time-sensitive:\n",
    "\n",
    "| Question type | Backend | Cached? |\n",
    "|---|---|---|\n",
    "| Stable / document-grounded | **Traversaal Pro** (RAG over AWS guidebook) | ‚úÖ Yes |\n",
    "| Time-sensitive / live data | **SerpApi** (Google search results) | ‚ùå Never |\n",
    "\n",
    "---\n",
    "\n",
    "## Traversaal Pro ‚Äî RAG as a Service\n",
    "\n",
    "[Traversaal Pro](https://pro.traversaal.ai) is a hosted RAG platform. You upload documents into a project; the API handles chunking, embedding, retrieval, and generation. In this notebook the corpus is the **AWS Guidebook**.\n",
    "\n",
    "**API details:**\n",
    "\n",
    "| Property | Value |\n",
    "|---|---|\n",
    "| Endpoint | `POST https://pro-documents.traversaal-api.com/documents/search` |\n",
    "| Auth | `Authorization: Bearer <your_token>` |\n",
    "| Request | `{\"query\": \"...\", \"generation\": true}` |\n",
    "| Response | `{\"response\": \"...\", \"references\": [{score, chunk_text, ...}]}` |\n",
    "\n",
    "Sign up at [pro.traversaal.ai](https://pro.traversaal.ai) to get your Bearer token.\n",
    "\n",
    "---\n",
    "\n",
    "## SerpApi ‚Äî Live Internet Search\n",
    "\n",
    "[SerpApi](https://serpapi.com) provides structured Google search results via a REST API. We use it for time-sensitive questions that require up-to-date information from the web (current events, live pricing, outages, etc.) ‚Äî answers that must never be served from cache.\n",
    "\n",
    "**API details:**\n",
    "\n",
    "| Property | Value |\n",
    "|---|---|\n",
    "| Endpoint | `GET https://serpapi.com/search.json` |\n",
    "| Auth | `?api_key=<your_key>` query param |\n",
    "| Key params | `q=<query>`, `engine=google`, `num=5` |\n",
    "| Response | `organic_results[].snippet`, `answer_box` |\n",
    "\n",
    "Sign up at [serpapi.com](https://serpapi.com) for a free API key (100 searches/month on the free tier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38331891-adb4-4d16-b26f-d74d7c9ce728",
   "metadata": {
    "id": "38331891-adb4-4d16-b26f-d74d7c9ce728"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests  # HTTP client for REST API calls \n",
    "\n",
    "# ‚îÄ‚îÄ Credential loading ‚Äî works on Colab and locally ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# On Colab:  store keys in the Secrets panel (üîë left sidebar)\n",
    "#              TRAVERSAAL_PRO_API_KEY\n",
    "#              SERP_API_KEY\n",
    "# Locally:   keys are read from Module_3_Agentic_RAG/.env\n",
    "#              traversaal_pro_api_key=<token>\n",
    "#              serp_api_key=<key>\n",
    "\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    traversaal_pro_api_key = userdata.get(\"TRAVERSAAL_PRO_API_KEY\")\n",
    "    serp_api_key = userdata.get(\"SERP_API_KEY\")\n",
    "    print(\"Running on Colab ‚Äî credentials loaded from Secrets.\")\n",
    "except ImportError:\n",
    "    from dotenv import load_dotenv, find_dotenv\n",
    "    load_dotenv(find_dotenv())   # walks up the directory tree to find .env\n",
    "    # .env uses lowercase key names; fall back to uppercase too\n",
    "    traversaal_pro_api_key = os.getenv(\"traversaal_pro_api_key\") or os.getenv(\"TRAVERSAAL_PRO_API_KEY\")\n",
    "    serp_api_key = os.getenv(\"serp_api_key\") or os.getenv(\"SERP_API_KEY\")\n",
    "    print(\"Running locally ‚Äî credentials loaded from .env file.\")\n",
    "\n",
    "print(f\"Traversaal Pro key loaded: {'‚úÖ' if traversaal_pro_api_key else '‚ùå MISSING'}\")\n",
    "print(f\"SerpApi key loaded:        {'‚úÖ' if serp_api_key else '‚ùå MISSING'}\")\n",
    "\n",
    "\n",
    "# ‚îÄ‚îÄ Traversaal Pro: RAG over AWS Guidebook ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "def make_prediction(query: str) -> dict:\n",
    "    \"\"\"\n",
    "    Query the Traversaal Pro RAG API with a natural language question.\n",
    "\n",
    "    The API performs retrieval over the configured document corpus (AWS Guidebook)\n",
    "    and returns a generated answer together with source chunk references.\n",
    "\n",
    "    Request:\n",
    "        POST https://pro-documents.traversaal-api.com/documents/search\n",
    "        {\"query\": \"...\", \"generation\": true}\n",
    "\n",
    "    Response:\n",
    "        {\n",
    "          \"response\": \"<generated answer string>\",\n",
    "          \"references\": [\n",
    "            {\n",
    "              \"score\": 0.81,\n",
    "              \"file_id\": \"...\",\n",
    "              \"chunk_index\": 1,\n",
    "              \"chunk_text\": \"...\",\n",
    "              \"original_file_name\": \"aws-guide.pdf\"\n",
    "            },\n",
    "            ...\n",
    "          ]\n",
    "        }\n",
    "\n",
    "    Args:\n",
    "        query (str): Natural language question answerable from the AWS Guidebook.\n",
    "\n",
    "    Returns:\n",
    "        dict: Full API response with 'response' and 'references' keys.\n",
    "    \"\"\"\n",
    "    if not traversaal_pro_api_key:\n",
    "        raise RuntimeError(\"Missing TRAVERSAAL_PRO_API_KEY ‚Äî add it to Colab Secrets or .env\")\n",
    "\n",
    "    url = \"https://pro-documents.traversaal-api.com/documents/search\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {traversaal_pro_api_key}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "    payload = {\"query\": query, \"generation\": True}\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, json=payload, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            print(\"Traversaal Pro: request successful.\")\n",
    "            try:\n",
    "                return response.json()\n",
    "            except ValueError:\n",
    "                print(\"Response was not valid JSON.\")\n",
    "                return None\n",
    "        else:\n",
    "            print(f\"Traversaal Pro: request failed ({response.status_code}): {response.text}\")\n",
    "            return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Traversaal Pro: request error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# ‚îÄ‚îÄ SerpApi: Live Google Search ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "def search_live(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Search Google in real time using SerpApi and return a formatted answer.\n",
    "\n",
    "    Used exclusively for time-sensitive questions (current events, live pricing,\n",
    "    outages, etc.) where a cached answer would quickly become stale.\n",
    "    Results are intentionally NOT stored in the semantic cache.\n",
    "\n",
    "    Args:\n",
    "        query (str): The time-sensitive question to search for.\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted string combining the answer box (if present) and\n",
    "             top organic result snippets.\n",
    "    \"\"\"\n",
    "    if not serp_api_key:\n",
    "        raise RuntimeError(\"Missing SERP_API_KEY ‚Äî add it to Colab Secrets or .env\")\n",
    "\n",
    "    print(\"SerpApi: fetching live search results üåê ...\")\n",
    "    params = {\n",
    "        \"q\": query,\n",
    "        \"api_key\": serp_api_key,\n",
    "        \"engine\": \"google\",\n",
    "        \"num\": 5,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(\"https://serpapi.com/search.json\", params=params)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "\n",
    "        parts = []\n",
    "\n",
    "        # Answer box ‚Äî Google's highlighted direct answer (most relevant)\n",
    "        answer_box = data.get(\"answer_box\", {})\n",
    "        if answer_box.get(\"answer\"):\n",
    "            parts.append(f\"[Direct Answer] {answer_box['answer']}\")\n",
    "        elif answer_box.get(\"snippet\"):\n",
    "            parts.append(f\"[Direct Answer] {answer_box['snippet']}\")\n",
    "\n",
    "        # Top organic results ‚Äî titles + snippets\n",
    "        for i, result in enumerate(data.get(\"organic_results\", [])[:5], start=1):\n",
    "            title = result.get(\"title\", \"\")\n",
    "            snippet = result.get(\"snippet\", \"\")\n",
    "            link = result.get(\"link\", \"\")\n",
    "            if snippet:\n",
    "                parts.append(f\"[{i}] {title}\\n    {snippet}\\n    Source: {link}\")\n",
    "\n",
    "        if not parts:\n",
    "            return \"No results found.\"\n",
    "\n",
    "        return \"\\n\\n\".join(parts)\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"SerpApi request error: {e}\"\n",
    "    except Exception as e:\n",
    "        return f\"Unexpected error: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QC027Sholey1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QC027Sholey1",
    "outputId": "8b479bc0-3861-4dbb-b389-b964df66a8bd"
   },
   "outputs": [],
   "source": [
    "# Test Traversaal Pro ‚Äî stable AWS question (answer comes from the AWS Guidebook)\n",
    "result = make_prediction(\"What is an S3 bucket in AWS?\")\n",
    "print(\"Generated answer:\")\n",
    "print(result[\"response\"])\n",
    "print(\"\\nTop source reference:\")\n",
    "if result.get(\"references\"):\n",
    "    top_ref = result[\"references\"][0]\n",
    "    print(f\"  Score: {top_ref['score']:.3f}\")\n",
    "    print(f\"  File:  {top_ref['original_file_name']}\")\n",
    "    print(f\"  Chunk: {top_ref['chunk_text'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vr6dbehodif",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test SerpApi ‚Äî time-sensitive question (live internet search, NOT from documents)\n",
    "live_answer = search_live(\"Are there any AWS outages right now?\")\n",
    "print(live_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db-RrmtuqFhC",
   "metadata": {
    "id": "db-RrmtuqFhC"
   },
   "source": [
    "### Define SemanticCaching Class\n",
    "\n",
    "In this cell we define `SemanticCaching`‚Äîa lightweight cache with dual-backend routing:\n",
    "\n",
    "1. **Time-sensitive guard** ‚Äî detects temporal keywords and routes to **SerpApi** (live Google search), bypassing the cache entirely.  \n",
    "2. **FAISS lookup** ‚Äî for stable questions, checks if a semantically similar question was already answered. If yes, returns the cached answer instantly.  \n",
    "3. **Traversaal Pro fallback** ‚Äî on a cache miss, queries the **AWS Guidebook RAG** to get a document-grounded answer, then stores it for future hits.  \n",
    "4. **JSON persistence** ‚Äî cache entries (questions, embeddings, answers) are saved to disk so the index survives notebook restarts.  \n",
    "5. **Latency logging** ‚Äî every call reports whether it was a hit, miss, or live search, and how long it took.\n",
    "\n",
    "---\n",
    "\n",
    "### What Should (and Should NOT) Be Semantically Cached?\n",
    "\n",
    "The cache is backed by the **AWS Guidebook** via Traversaal Pro. Since documentation is stable, most AWS concept questions are excellent cache candidates. The exceptions are anything that requires live, up-to-the-minute data.\n",
    "\n",
    "#### ‚úÖ Good to cache ‚Äî stable AWS documentation answers:\n",
    "| Question | Why it's safe to cache |\n",
    "|---|---|\n",
    "| *\"What is an S3 bucket in AWS?\"* | Core concept, always the same |\n",
    "| *\"How does AWS Lambda work?\"* | Stable service behaviour |\n",
    "| *\"What is AWS IAM?\"* | Conceptual definition from docs |\n",
    "| *\"What is the difference between EC2 and ECS?\"* | Architectural comparison |\n",
    "| *\"How does Amazon CloudFront work?\"* | Service explanation |\n",
    "| *\"What is an AWS VPC?\"* | Networking concept |\n",
    "\n",
    "#### ‚ùå Do NOT cache ‚Äî time-sensitive, answers change even for AWS:\n",
    "| Question | Why it must NOT be cached | Backend |\n",
    "|---|---|---|\n",
    "| *\"Are there any AWS outages right now?\"* | Status changes minute to minute | SerpApi |\n",
    "| *\"What are the latest AWS features this week?\"* | New releases announced daily | SerpApi |\n",
    "| *\"What is the current EC2 pricing today?\"* | AWS updates pricing periodically | SerpApi |\n",
    "| *\"Is AWS S3 down right now?\"* | Real-time health check | SerpApi |\n",
    "| *\"What new services did AWS announce this month?\"* | New info every month | SerpApi |\n",
    "\n",
    "The `is_time_sensitive()` method catches these using a keyword list and routes them to SerpApi ‚Äî they never touch the FAISS index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yDHhY-OBSEIw",
   "metadata": {
    "id": "yDHhY-OBSEIw"
   },
   "outputs": [],
   "source": [
    "import faiss            # Efficient similarity search over vector embeddings\n",
    "import json             # Read/write cache from a JSON file\n",
    "import numpy as np      # Numerical operations on embeddings\n",
    "from sentence_transformers import SentenceTransformer  # Load Nomic embed model\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM  # (Optional) LLM for answer gen\n",
    "import time             # Measure latency\n",
    "\n",
    "class SemanticCaching:\n",
    "    \"\"\"\n",
    "    A semantic cache that routes queries to the right backend:\n",
    "\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "    ‚îÇ  Query                                                   ‚îÇ\n",
    "    ‚îÇ    ‚îÇ                                                     ‚îÇ\n",
    "    ‚îÇ    ‚îú‚îÄ Time-sensitive? ‚îÄ‚îÄYES‚îÄ‚îÄ‚ñ∂ SerpApi (live search)     ‚îÇ\n",
    "    ‚îÇ    ‚îÇ                           NOT cached                ‚îÇ\n",
    "    ‚îÇ    ‚îÇ                                                     ‚îÇ\n",
    "    ‚îÇ    ‚îî‚îÄ Stable? ‚îÄ‚îÄ‚ñ∂ FAISS lookup                          ‚îÇ\n",
    "    ‚îÇ                     ‚îÇ                                    ‚îÇ\n",
    "    ‚îÇ                     ‚îú‚îÄ HIT  ‚îÄ‚îÄ‚ñ∂ return cached answer ‚ö°  ‚îÇ\n",
    "    ‚îÇ                     ‚îÇ                                    ‚îÇ\n",
    "    ‚îÇ                     ‚îî‚îÄ MISS ‚îÄ‚îÄ‚ñ∂ Traversaal Pro (RAG)     ‚îÇ\n",
    "    ‚îÇ                                 store ‚Üí return           ‚îÇ\n",
    "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "    \"\"\"\n",
    "\n",
    "    # Keywords that signal the question is time-sensitive and must NOT be cached.\n",
    "    # Answers to these questions change over time ‚Äî caching would return stale results.\n",
    "    TIME_SENSITIVE_KEYWORDS = [\n",
    "        \"today\", \"tonight\", \"now\", \"currently\", \"current\",\n",
    "        \"latest\", \"recent\", \"recently\", \"right now\", \"at the moment\",\n",
    "        \"at present\", \"as of now\", \"this week\", \"this month\", \"this year\",\n",
    "        \"this quarter\", \"this season\", \"this morning\", \"this afternoon\",\n",
    "        \"this evening\", \"this weekend\", \"yesterday\", \"tomorrow\",\n",
    "        \"last week\", \"last month\", \"last year\", \"upcoming\", \"live\",\n",
    "        \"breaking\", \"just happened\", \"what time\", \"what day\", \"what date\",\n",
    "        \"happening now\", \"events today\", \"news today\", \"news this week\",\n",
    "        \"stock price\", \"share price\", \"weather\", \"forecast\", \"temperature\",\n",
    "        \"real-time\", \"realtime\", \"schedule today\", \"outage\", \"down right now\",\n",
    "        \"is aws down\", \"aws status\",\n",
    "    ]\n",
    "\n",
    "    def __init__(self, json_file='cache.json', clear_on_init=False):\n",
    "        # Initialize Faiss index with Euclidean distance\n",
    "        self.index = faiss.IndexFlatL2(768)\n",
    "        if self.index.is_trained:\n",
    "            print('Index trained')\n",
    "\n",
    "        # Initialize Sentence Transformer model\n",
    "        self.encoder = SentenceTransformer('nomic-ai/nomic-embed-text-v1.5', trust_remote_code=True)\n",
    "\n",
    "        # Euclidean distance threshold for cache hits (lower = stricter)\n",
    "        self.euclidean_threshold = 0.2\n",
    "\n",
    "        # JSON file to persist cache entries\n",
    "        self.json_file = json_file\n",
    "\n",
    "        # Load cache or clear already loaded cache\n",
    "        if clear_on_init:\n",
    "            self.clear_cache()\n",
    "        else:\n",
    "            self.load_cache()\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Time-sensitivity detection\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    def is_time_sensitive(self, question: str) -> bool:\n",
    "        \"\"\"\n",
    "        Returns True if the question is time-sensitive and should NOT be cached.\n",
    "\n",
    "        Time-sensitive questions reference current events, live data, or time-bound\n",
    "        information whose answers change frequently. These are routed to SerpApi\n",
    "        for a real-time Google search answer instead of the document RAG system.\n",
    "\n",
    "        Examples that return True (‚Üí SerpApi, never cached):\n",
    "            'Are there any AWS outages right now?'\n",
    "            'What are the latest AWS features released this week?'\n",
    "            'What is the current EC2 pricing today?'\n",
    "            'Is AWS S3 down right now?'\n",
    "\n",
    "        Examples that return False (‚Üí check cache, then Traversaal Pro if miss):\n",
    "            'What is an S3 bucket in AWS?'\n",
    "            'How does AWS Lambda work?'\n",
    "            'What is AWS IAM?'\n",
    "            'What is the difference between EC2 and ECS?'\n",
    "        \"\"\"\n",
    "        question_lower = question.lower()\n",
    "        return any(keyword in question_lower for keyword in self.TIME_SENSITIVE_KEYWORDS)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Cache persistence\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    def clear_cache(self):\n",
    "        \"\"\"Clears in-memory cache, resets FAISS index, and overwrites the JSON file.\"\"\"\n",
    "        self.cache = {\n",
    "            'questions': [],\n",
    "            'embeddings': [],\n",
    "            'answers': [],\n",
    "            'response_text': []\n",
    "        }\n",
    "        self.index = faiss.IndexFlatL2(768)\n",
    "        self.save_cache()\n",
    "        print(\"Semantic cache cleared.\")\n",
    "\n",
    "    def load_cache(self):\n",
    "        \"\"\"Load existing cache or initialize empty structure.\"\"\"\n",
    "        try:\n",
    "            with open(self.json_file, 'r') as file:\n",
    "                self.cache = json.load(file)\n",
    "        except FileNotFoundError:\n",
    "            self.cache = {'questions': [], 'embeddings': [], 'answers': [], 'response_text': []}\n",
    "\n",
    "    def save_cache(self):\n",
    "        \"\"\"Persist cache back to disk.\"\"\"\n",
    "        with open(self.json_file, 'w') as file:\n",
    "            json.dump(self.cache, file)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Main query method\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    def ask(self, question: str) -> str:\n",
    "        \"\"\"\n",
    "        Route the question to the correct backend and return an answer.\n",
    "\n",
    "        Routing logic:\n",
    "          1. Time-sensitive  ‚Üí SerpApi (live Google search) ‚Äî answer NOT cached\n",
    "          2. Cache HIT       ‚Üí return stored answer instantly\n",
    "          3. Cache MISS      ‚Üí Traversaal Pro (RAG over AWS docs) ‚Äî answer stored\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        # ‚îÄ‚îÄ 1. Time-sensitivity guard ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "        # Live search via SerpApi ‚Äî result intentionally not stored\n",
    "        if self.is_time_sensitive(question):\n",
    "            print(\"‚è∞ Time-sensitive question ‚Äî routing to SerpApi (live search, not cached).\")\n",
    "            response_text = search_live(question)\n",
    "            print(f\"Time taken: {time.time() - start_time:.3f}s\")\n",
    "            return response_text\n",
    "\n",
    "        try:\n",
    "            # ‚îÄ‚îÄ 2. Cache lookup ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "            embedding = self.encoder.encode([question], normalize_embeddings=True)\n",
    "            D, I = self.index.search(embedding, 1)\n",
    "\n",
    "            if D[0] >= 0:\n",
    "                if I[0][0] != -1 and D[0][0] <= self.euclidean_threshold:\n",
    "                    row_id = int(I[0][0])\n",
    "                    print(f'‚úÖ Cache hit at row: {row_id} | similarity: {1 - D[0][0]:.4f}')\n",
    "                    print(f\"Time taken: {time.time() - start_time:.3f}s\")\n",
    "                    return self.cache['response_text'][row_id]\n",
    "\n",
    "            # ‚îÄ‚îÄ 3. Cache miss ‚Üí Traversaal Pro RAG ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "            answer, response_text = self.generate_answer(question)\n",
    "\n",
    "            self.cache['questions'].append(question)\n",
    "            self.cache['embeddings'].append(embedding[0].tolist())\n",
    "            self.cache['answers'].append(answer)\n",
    "            self.cache['response_text'].append(response_text)\n",
    "            self.index.add(embedding)\n",
    "            self.save_cache()\n",
    "            print(f\"Time taken: {time.time() - start_time:.3f}s\")\n",
    "\n",
    "            return response_text\n",
    "\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error during 'ask' method: {e}\")\n",
    "\n",
    "    def generate_answer(self, question: str):\n",
    "        \"\"\"\n",
    "        Call Traversaal Pro to answer a stable document-grounded question.\n",
    "\n",
    "        Uses the AWS Guidebook corpus loaded into your Traversaal Pro project.\n",
    "        Extracts the 'response' field from the API reply as the answer text.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (full API result dict, answer string)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            result = make_prediction(question)\n",
    "            # Traversaal Pro returns {\"response\": \"...\", \"references\": [...]}\n",
    "            response_text = result.get('response', str(result))\n",
    "            return result, response_text\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error during 'generate_answer' method: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc661dab-f7cc-4d74-9575-1c756b4cdef0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dc661dab-f7cc-4d74-9575-1c756b4cdef0",
    "outputId": "c55b9d0c-69f3-42ac-81f9-5b014a3b7415"
   },
   "outputs": [],
   "source": [
    "# Instantiate the semantic cache: builds/loads FAISS index, encoder, and JSON cache\n",
    "cache = SemanticCaching()\n",
    "\n",
    "# Uncomment and use to re-instantiate the semantic cache and clear exisitng cache entries\n",
    "# cache = SemanticCaching(clear_on_init=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noc3G8Abxy2t",
   "metadata": {
    "id": "noc3G8Abxy2t"
   },
   "source": [
    "### Testing the Semantic Cache\n",
    "\n",
    "We validate the `SemanticCaching` class using AWS Guidebook questions. These are stable, document-grounded questions ‚Äî ideal for caching because the answers don't change over time.\n",
    "\n",
    "Watch the routing in action:\n",
    "- **First ask** of a question ‚Üí cache miss ‚Üí Traversaal Pro RAG ‚Üí answer stored  \n",
    "- **Rephrased version** of the same question ‚Üí cache hit ‚Üí instant return  \n",
    "- **Time-sensitive question** ‚Üí SerpApi live search ‚Üí never stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f64fe4d-fe89-44a7-bf3f-3ad721985f3e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2f64fe4d-fe89-44a7-bf3f-3ad721985f3e",
    "outputId": "90ecbeb7-fd6d-4222-86ab-e472e0dbadf7"
   },
   "outputs": [],
   "source": [
    "# Q1: Cache miss ‚Äî Traversaal Pro answers from AWS docs, stores result\n",
    "question1 = \"What is an S3 bucket in AWS?\"\n",
    "answer1 = cache.ask(question1)\n",
    "print(answer1)\n",
    "\n",
    "# Q2: Cache miss ‚Äî different AWS service\n",
    "question2 = \"How does AWS Lambda work?\"\n",
    "answer2 = cache.ask(question2)\n",
    "print(answer2)\n",
    "\n",
    "# Q3: Cache miss ‚Äî another AWS service\n",
    "question3 = \"What is AWS IAM used for?\"\n",
    "answer3 = cache.ask(question3)\n",
    "print(answer3)\n",
    "\n",
    "# Q4: Cache miss ‚Äî networking concept\n",
    "question4 = \"What is an Amazon VPC?\"\n",
    "answer4 = cache.ask(question4)\n",
    "print(answer4)\n",
    "\n",
    "# Note:\n",
    "# All four are distinct enough to each get a separate Traversaal Pro call.\n",
    "# Next, we'll test cache hits with rephrased versions of these questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eade92a-a4f7-406f-85d3-ae24146d9c00",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5eade92a-a4f7-406f-85d3-ae24146d9c00",
    "outputId": "f57cfdc7-0288-499c-d9f0-27ed2916acc3"
   },
   "outputs": [],
   "source": [
    "# Cache HIT ‚Äî rephrased version of Q1 (\"What is an S3 bucket?\")\n",
    "# The FAISS index finds the stored embedding is similar enough ‚Üí returns instantly\n",
    "print(cache.ask(\"Can you explain what Amazon S3 buckets are?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067af075-1df3-4fa7-90bf-52b14d819406",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "067af075-1df3-4fa7-90bf-52b14d819406",
    "outputId": "4efe08d2-b888-43d4-fe2e-fbcd139e92cb"
   },
   "outputs": [],
   "source": [
    "# Cache HIT ‚Äî exact same question as Q3\n",
    "print(cache.ask(\"What is AWS IAM used for?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9a6a23-83d7-4688-b037-fc015f295e83",
   "metadata": {
    "collapsed": true,
    "id": "7b9a6a23-83d7-4688-b037-fc015f295e83"
   },
   "outputs": [],
   "source": [
    "# Cache MISS ‚Äî new AWS concept not yet in cache ‚Üí Traversaal Pro call\n",
    "print(cache.ask(\"What is Amazon CloudFront and how does it work?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2P3Tso8TTElH",
   "metadata": {
    "collapsed": true,
    "id": "2P3Tso8TTElH"
   },
   "outputs": [],
   "source": [
    "# Cache HIT ‚Äî semantically similar to the CloudFront question above\n",
    "print(cache.ask(\"How does AWS CloudFront serve content to users?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015dd13f-9de9-409b-9273-6730fe173585",
   "metadata": {
    "collapsed": true,
    "id": "015dd13f-9de9-409b-9273-6730fe173585"
   },
   "outputs": [],
   "source": [
    "# Cache MISS ‚Äî new question about DynamoDB ‚Üí Traversaal Pro call + stored\n",
    "print(cache.ask(\"What is Amazon DynamoDB and when should I use it?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf696d0-2660-4cae-99b1-583807e7e5f1",
   "metadata": {
    "collapsed": true,
    "id": "2cf696d0-2660-4cae-99b1-583807e7e5f1"
   },
   "outputs": [],
   "source": [
    "# Cache HIT ‚Äî rephrased DynamoDB question ‚Üí returned from cache instantly\n",
    "print(cache.ask(\"When would I choose DynamoDB over other databases on AWS?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e94626-1fd1-4493-8b8f-9550a1460e7a",
   "metadata": {
    "collapsed": true,
    "id": "b4e94626-1fd1-4493-8b8f-9550a1460e7a"
   },
   "outputs": [],
   "source": [
    "# Cache MISS ‚Äî different enough to not match DynamoDB ‚Üí Traversaal Pro call\n",
    "print(cache.ask(\"How does Amazon RDS differ from DynamoDB?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ys1zxjtkc4",
   "metadata": {},
   "source": [
    "### Testing the Time-Sensitivity Filter + Dual-Backend Routing\n",
    "\n",
    "Here we demonstrate the full routing logic:\n",
    "\n",
    "| Question type | Detected by | Backend | Cached? |\n",
    "|---|---|---|---|\n",
    "| Contains temporal keyword | `is_time_sensitive()` ‚Üí `True` | **SerpApi** (live Google search) | ‚ùå Never |\n",
    "| Stable AWS concept | `is_time_sensitive()` ‚Üí `False` + cache miss | **Traversaal Pro** (AWS docs RAG) | ‚úÖ Stored |\n",
    "| Previously seen question | `is_time_sensitive()` ‚Üí `False` + cache hit | **FAISS cache** | ‚úÖ Already stored |\n",
    "\n",
    "**AWS-specific time-sensitive examples** ‚Äî even though they're about AWS, these need live answers:\n",
    "- *\"Are there any AWS outages right now?\"* ‚Üí changes minute to minute  \n",
    "- *\"What are the latest AWS features released this week?\"* ‚Üí new announcements daily  \n",
    "- *\"What is the current EC2 pricing today?\"* ‚Üí pricing can be updated by AWS anytime  \n",
    "\n",
    "**AWS stable examples** ‚Äî these come from documentation and don't change:\n",
    "- *\"What is an S3 bucket?\"* ‚Äî always the same concept\n",
    "- *\"How does Lambda work?\"* ‚Äî core service behaviour is stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dlfbvrouhim",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification check ‚Äî see which questions are flagged before running any queries\n",
    "time_sensitive_aws = [\n",
    "    \"Are there any AWS outages right now?\",\n",
    "    \"What are the latest AWS features released this week?\",\n",
    "    \"What is the current EC2 pricing today?\",\n",
    "    \"Is AWS S3 down right now?\",\n",
    "    \"What new services did AWS announce this month?\",\n",
    "    \"What is the current AWS free tier limit as of now?\",\n",
    "]\n",
    "\n",
    "stable_aws = [\n",
    "    \"What is an S3 bucket in AWS?\",\n",
    "    \"How does AWS Lambda work?\",\n",
    "    \"What is AWS IAM?\",\n",
    "    \"What is the difference between EC2 and ECS?\",\n",
    "    \"How does Amazon CloudFront work?\",\n",
    "    \"What is an AWS VPC?\",\n",
    "]\n",
    "\n",
    "print(\"=== Time-Sensitive AWS Questions (‚Üí SerpApi, never cached) ===\")\n",
    "for q in time_sensitive_aws:\n",
    "    flag = cache.is_time_sensitive(q)\n",
    "    label = \"‚è∞ SerpApi (live)\" if flag else \"‚úÖ Traversaal Pro (cached)\"\n",
    "    print(f\"  [{label}] {q}\")\n",
    "\n",
    "print(\"\\n=== Stable AWS Questions (‚Üí Traversaal Pro on miss, cached) ===\")\n",
    "for q in stable_aws:\n",
    "    flag = cache.is_time_sensitive(q)\n",
    "    label = \"‚è∞ SerpApi (live)\" if flag else \"‚úÖ Traversaal Pro (cached)\"\n",
    "    print(f\"  [{label}] {q}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xm8an2uz38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-sensitive AWS questions ‚Üí routed to SerpApi, NEVER stored in cache\n",
    "# Ask the same question twice ‚Äî both calls go live, nothing accumulates in FAISS\n",
    "\n",
    "print(\"--- Query A (time-sensitive: outage check) ---\")\n",
    "print(cache.ask(\"Are there any AWS outages right now?\"))\n",
    "\n",
    "print(\"\\n--- Query A again (still time-sensitive ‚Üí SerpApi again, not cached) ---\")\n",
    "print(cache.ask(\"Are there any AWS outages right now?\"))\n",
    "\n",
    "print(\"\\n--- Query B (time-sensitive: pricing) ---\")\n",
    "print(cache.ask(\"What is the current EC2 pricing today?\"))\n",
    "\n",
    "# Verify the cache count has not grown due to these time-sensitive calls\n",
    "print(f\"\\nCache entries (should be same as before): {len(cache.cache['questions'])}\")\n",
    "print(\"Cached questions:\", cache.cache['questions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "w7nq7un0l6j",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stable AWS question ‚Üí cache miss on first call (Traversaal Pro), cache hit on second\n",
    "print(\"--- Query C (stable AWS, first call ‚Üí Traversaal Pro) ---\")\n",
    "print(cache.ask(\"How do you configure S3 bucket policies?\"))\n",
    "\n",
    "print(\"\\n--- Query D (semantically similar to C ‚Üí cache hit) ---\")\n",
    "print(cache.ask(\"What is the way to set up an S3 bucket access policy?\"))\n",
    "\n",
    "print(f\"\\nTotal cached entries now: {len(cache.cache['questions'])}\")\n",
    "print(\"Cached questions:\", cache.cache['questions'])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "olive-python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
