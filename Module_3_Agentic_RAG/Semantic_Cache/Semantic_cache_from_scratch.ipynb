{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
<<<<<<< Updated upstream
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/hamzafarooq/multi-agent-course/blob/main/Module_3_Agentic_RAG/Semantic_Cache/Semantic_cache_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
=======
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/hamzafarooq/multi-agent-course/blob/main/Module_3/Semantic_Cache/Semantic_cache_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< Updated upstream
   "id": "p2KzXaJYks8S",
   "metadata": {
    "id": "p2KzXaJYks8S"
   },
=======
>>>>>>> Stashed changes
   "source": [
    "**If you use our code, please cite:**\n",
    "\n",
    "@misc{2024<br>\n",
    "  title = {Semantic Cache from Scratch},<br>\n",
    "  author = {Hamza Farooq, Darshil Modi, Kanwal Mehreen, Nazila Shafiei},<br>\n",
    "  keywords = {Semantic Cache},<br>\n",
    "  year = {2024},<br>\n",
    "  copyright = {APACHE 2.0 license}<br>\n",
    "}"
<<<<<<< Updated upstream
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gxQxLQJymkCl",
   "metadata": {
    "id": "gxQxLQJymkCl"
   },
   "source": [
    "## Semantic Cache\n",
    "\n",
    "Semantic caching accelerates retrieval-augmented workflows by storing and reusing previous embedding-based lookups instead of issuing fresh queries every time. In this notebook, we'll build a lightweight semantic cache from scratch using:\n",
    "\n",
    "- **Nomic text embeddings** (`nomic-ai/nomic-embed-text-v1.5`) to convert documents and queries into dense vectors  \n",
    "- **FAISS** (Facebook AI Similarity Search) to index and quickly search those vectors  \n",
    "- **Traversaal Pro API** to perform RAG over the AWS documentation corpus when a cache miss occurs  \n",
    "\n",
    "Rather than re-computing embeddings and retrieval for every query, our cache lets us:\n",
    "\n",
    "1. **Embed** each new query and check if it's already \"covered\" by a cached result  \n",
    "2. **Fall back** to a full RAG retrieval (and store the new result) only when necessary  \n",
    "3. **Skip the cache entirely** for time-sensitive questions that need a fresh answer  \n",
    "4. **Invoke** the Traversaal Pro API for document-grounded answers on cache misses  \n",
    "\n",
    "This approach reduces redundant compute, lowers end-to-end latency, and makes RAG pipelines more efficient‚Äîespecially when query patterns exhibit repetition or high similarity. We'll walk through:\n",
    "\n",
    "1. Loading the Nomic embed model with `trust_remote_code=True`  \n",
    "2. Building a FAISS index for fast L2 nearest-neighbor lookup  \n",
    "3. Implementing the core cache hit/miss logic with a time-sensitivity filter  \n",
    "4. Falling back to Traversaal Pro RAG API for live document retrieval on cache misses  \n",
    "5. Measuring performance gains against a \"no-cache\" baseline  \n",
    "\n",
    "By the end, you'll have a reusable semantic cache scaffold that you can plug into any RAG or search-over-embeddings pipeline. Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Hm2NYbYQmykl",
   "metadata": {
    "id": "Hm2NYbYQmykl"
   },
   "source": [
    "## Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "025_hZMnZUIE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "025_hZMnZUIE",
    "outputId": "0b61c55b-c058-4a2c-cc1d-095320fe263b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Using cached faiss_cpu-1.13.2-cp310-abi3-macosx_14_0_arm64.whl.metadata (7.6 kB)\n",
      "Collecting sentence_transformers\n",
      "  Using cached sentence_transformers-5.2.3-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting transformers\n",
      "  Using cached transformers-5.2.0-py3-none-any.whl.metadata (32 kB)\n",
      "Requirement already satisfied: python-dotenv in /Users/traversaal-001-hf/anaconda3/envs/olive-python/lib/python3.14/site-packages (1.2.1)\n",
      "Collecting einops\n",
      "  Using cached einops-0.8.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /Users/traversaal-001-hf/anaconda3/envs/olive-python/lib/python3.14/site-packages (from faiss-cpu) (2.4.1)\n",
      "Requirement already satisfied: packaging in /Users/traversaal-001-hf/anaconda3/envs/olive-python/lib/python3.14/site-packages (from faiss-cpu) (26.0)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence_transformers)\n",
      "  Using cached huggingface_hub-1.4.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting torch>=1.11.0 (from sentence_transformers)\n",
      "  Downloading torch-2.10.0-cp314-cp314-macosx_14_0_arm64.whl.metadata (31 kB)\n",
      "Collecting scikit-learn (from sentence_transformers)\n",
      "  Downloading scikit_learn-1.8.0-cp314-cp314-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Collecting scipy (from sentence_transformers)\n",
      "  Downloading scipy-1.17.0-cp314-cp314-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /Users/traversaal-001-hf/anaconda3/envs/olive-python/lib/python3.14/site-packages (from sentence_transformers) (4.15.0)\n",
      "Requirement already satisfied: tqdm in /Users/traversaal-001-hf/anaconda3/envs/olive-python/lib/python3.14/site-packages (from sentence_transformers) (4.67.2)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Downloading pyyaml-6.0.3-cp314-cp314-macosx_11_0_arm64.whl.metadata (2.4 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2026.2.19-cp314-cp314-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Using cached tokenizers-0.22.2-cp39-abi3-macosx_11_0_arm64.whl.metadata (7.3 kB)\n",
      "Collecting typer-slim (from transformers)\n",
      "  Using cached typer_slim-0.24.0-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Using cached safetensors-0.7.0-cp38-abi3-macosx_11_0_arm64.whl.metadata (4.1 kB)\n",
      "Collecting filelock (from huggingface-hub>=0.20.0->sentence_transformers)\n",
      "  Downloading filelock-3.24.3-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub>=0.20.0->sentence_transformers)\n",
      "  Downloading fsspec-2026.2.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.2.0 (from huggingface-hub>=0.20.0->sentence_transformers)\n",
      "  Using cached hf_xet-1.2.0-cp37-abi3-macosx_11_0_arm64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/traversaal-001-hf/anaconda3/envs/olive-python/lib/python3.14/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (0.28.1)\n",
      "Collecting shellingham (from huggingface-hub>=0.20.0->sentence_transformers)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: anyio in /Users/traversaal-001-hf/anaconda3/envs/olive-python/lib/python3.14/site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence_transformers) (4.12.1)\n",
      "Requirement already satisfied: certifi in /Users/traversaal-001-hf/anaconda3/envs/olive-python/lib/python3.14/site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence_transformers) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/traversaal-001-hf/anaconda3/envs/olive-python/lib/python3.14/site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence_transformers) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/traversaal-001-hf/anaconda3/envs/olive-python/lib/python3.14/site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence_transformers) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/traversaal-001-hf/anaconda3/envs/olive-python/lib/python3.14/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence_transformers) (0.16.0)\n",
      "Collecting setuptools (from torch>=1.11.0->sentence_transformers)\n",
      "  Downloading setuptools-82.0.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting sympy>=1.13.3 (from torch>=1.11.0->sentence_transformers)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx>=2.5.1 (from torch>=1.11.0->sentence_transformers)\n",
      "  Using cached networkx-3.6.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting jinja2 (from torch>=1.11.0->sentence_transformers)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch>=1.11.0->sentence_transformers)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch>=1.11.0->sentence_transformers)\n",
      "  Downloading markupsafe-3.0.3-cp314-cp314-macosx_11_0_arm64.whl.metadata (2.7 kB)\n",
      "Collecting joblib>=1.3.0 (from scikit-learn->sentence_transformers)\n",
      "  Downloading joblib-1.5.3-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting threadpoolctl>=3.2.0 (from scikit-learn->sentence_transformers)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting typer>=0.24.0 (from typer-slim->transformers)\n",
      "  Using cached typer-0.24.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting click>=8.2.1 (from typer>=0.24.0->typer-slim->transformers)\n",
      "  Using cached click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting rich>=12.3.0 (from typer>=0.24.0->typer-slim->transformers)\n",
      "  Downloading rich-14.3.3-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting annotated-doc>=0.0.2 (from typer>=0.24.0->typer-slim->transformers)\n",
      "  Using cached annotated_doc-0.0.4-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=12.3.0->typer>=0.24.0->typer-slim->transformers)\n",
      "  Using cached markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/traversaal-001-hf/anaconda3/envs/olive-python/lib/python3.14/site-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim->transformers) (2.19.2)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=12.3.0->typer>=0.24.0->typer-slim->transformers)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Using cached faiss_cpu-1.13.2-cp310-abi3-macosx_14_0_arm64.whl (3.5 MB)\n",
      "Using cached sentence_transformers-5.2.3-py3-none-any.whl (494 kB)\n",
      "Using cached transformers-5.2.0-py3-none-any.whl (10.4 MB)\n",
      "Using cached huggingface_hub-1.4.1-py3-none-any.whl (553 kB)\n",
      "Using cached hf_xet-1.2.0-cp37-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
      "Using cached tokenizers-0.22.2-cp39-abi3-macosx_11_0_arm64.whl (3.0 MB)\n",
      "Using cached einops-0.8.2-py3-none-any.whl (65 kB)\n",
      "Downloading fsspec-2026.2.0-py3-none-any.whl (202 kB)\n",
      "Downloading pyyaml-6.0.3-cp314-cp314-macosx_11_0_arm64.whl (173 kB)\n",
      "Downloading regex-2026.2.19-cp314-cp314-macosx_11_0_arm64.whl (289 kB)\n",
      "Using cached safetensors-0.7.0-cp38-abi3-macosx_11_0_arm64.whl (447 kB)\n",
      "Downloading torch-2.10.0-cp314-cp314-macosx_14_0_arm64.whl (79.5 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m79.5/79.5 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached networkx-3.6.1-py3-none-any.whl (2.1 MB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Downloading filelock-3.24.3-py3-none-any.whl (24 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Downloading markupsafe-3.0.3-cp314-cp314-macosx_11_0_arm64.whl (12 kB)\n",
      "Downloading scikit_learn-1.8.0-cp314-cp314-macosx_12_0_arm64.whl (8.1 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.3-py3-none-any.whl (309 kB)\n",
      "Downloading scipy-1.17.0-cp314-cp314-macosx_14_0_arm64.whl (20.1 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m20.1/20.1 MB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Downloading setuptools-82.0.0-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Using cached typer_slim-0.24.0-py3-none-any.whl (3.4 kB)\n",
      "Using cached typer-0.24.0-py3-none-any.whl (56 kB)\n",
      "Using cached annotated_doc-0.0.4-py3-none-any.whl (5.3 kB)\n",
      "Using cached click-8.3.1-py3-none-any.whl (108 kB)\n",
      "Downloading rich-14.3.3-py3-none-any.whl (310 kB)\n",
      "Using cached markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: mpmath, threadpoolctl, sympy, shellingham, setuptools, scipy, safetensors, regex, pyyaml, networkx, mdurl, MarkupSafe, joblib, hf-xet, fsspec, filelock, faiss-cpu, einops, click, annotated-doc, scikit-learn, markdown-it-py, jinja2, torch, rich, typer, typer-slim, huggingface-hub, tokenizers, transformers, sentence_transformers\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m31/31\u001b[0m [sentence_transformers]ence_transformers]\n",
      "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.3 annotated-doc-0.0.4 click-8.3.1 einops-0.8.2 faiss-cpu-1.13.2 filelock-3.24.3 fsspec-2026.2.0 hf-xet-1.2.0 huggingface-hub-1.4.1 jinja2-3.1.6 joblib-1.5.3 markdown-it-py-4.0.0 mdurl-0.1.2 mpmath-1.3.0 networkx-3.6.1 pyyaml-6.0.3 regex-2026.2.19 rich-14.3.3 safetensors-0.7.0 scikit-learn-1.8.0 scipy-1.17.0 sentence_transformers-5.2.3 setuptools-82.0.0 shellingham-1.5.4 sympy-1.14.0 threadpoolctl-3.6.0 tokenizers-0.22.2 torch-2.10.0 transformers-5.2.0 typer-0.24.0 typer-slim-0.24.0\n"
     ]
    }
   ],
   "source": [
    "# Install the necessary libraries\n",
    "!pip install -U faiss-cpu sentence_transformers transformers python-dotenv einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
=======
   ],
   "metadata": {
    "id": "p2KzXaJYks8S"
   },
   "id": "p2KzXaJYks8S"
  },
  {
   "cell_type": "markdown",
   "source": "## Semantic Cache\n\nSemantic caching accelerates retrieval-augmented workflows by storing and reusing previous embedding-based lookups instead of issuing fresh queries every time. In this notebook, we'll build a lightweight semantic cache from scratch using:\n\n- **Nomic text embeddings** (`nomic-ai/nomic-embed-text-v1.5`) to convert documents and queries into dense vectors  \n- **FAISS** (Facebook AI Similarity Search) to index and quickly search those vectors  \n- **Traversaal Pro API** to perform RAG over the AWS documentation corpus when a cache miss occurs  \n\nRather than re-computing embeddings and retrieval for every query, our cache lets us:\n\n1. **Embed** each new query and check if it's already \"covered\" by a cached result  \n2. **Fall back** to a full RAG retrieval (and store the new result) only when necessary  \n3. **Skip the cache entirely** for time-sensitive questions that need a fresh answer  \n4. **Invoke** the Traversaal Pro API for document-grounded answers on cache misses  \n\nThis approach reduces redundant compute, lowers end-to-end latency, and makes RAG pipelines more efficient‚Äîespecially when query patterns exhibit repetition or high similarity. We'll walk through:\n\n1. Loading the Nomic embed model with `trust_remote_code=True`  \n2. Building a FAISS index for fast L2 nearest-neighbor lookup  \n3. Implementing the core cache hit/miss logic with a time-sensitivity filter  \n4. Falling back to Traversaal Pro RAG API for live document retrieval on cache misses  \n5. Measuring performance gains against a \"no-cache\" baseline  \n\nBy the end, you'll have a reusable semantic cache scaffold that you can plug into any RAG or search-over-embeddings pipeline. Let's get started!",
   "metadata": {
    "id": "gxQxLQJymkCl"
   },
   "id": "gxQxLQJymkCl"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup and Dependencies"
   ],
   "metadata": {
    "id": "Hm2NYbYQmykl"
   },
   "id": "Hm2NYbYQmykl"
  },
  {
   "cell_type": "code",
   "source": "# Install the necessary libraries\n!pip install -U faiss-cpu sentence_transformers transformers python-dotenv",
   "metadata": {
    "id": "025_hZMnZUIE",
    "outputId": "0b61c55b-c058-4a2c-cc1d-095320fe263b",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "025_hZMnZUIE",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
>>>>>>> Stashed changes
   "id": "52273bc0-575b-4007-b63d-bfe53d4abde6",
   "metadata": {
    "id": "52273bc0-575b-4007-b63d-bfe53d4abde6"
   },
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "\n",
    "# FAISS for efficient similarity search over vector embeddings\n",
    "import faiss  # Builds and queries approximate nearest neighbor indices\n",
    "\n",
    "# Lightweight SQL database for caching metadata, query logs, or evaluation results\n",
    "import sqlite3  # Persistence layer for storing cache entries or metrics\n",
    "\n",
    "# SentenceTransformers wrapper around transformer models for text embeddings\n",
    "from sentence_transformers import SentenceTransformer  # Loads Nomic/embed or other SBERT-style models\n",
    "\n",
    "# PyTorch backend required by SentenceTransformer and optional model fine-tuning\n",
    "import torch  # Tensor operations, GPU acceleration, and model inference support\n",
    "\n",
    "# Transformers library components for causal LLM-based answer generation\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "#   - AutoModelForCausalLM: Load pretrained language models (e.g., GPT variants)\n",
    "#   - AutoTokenizer: Tokenize text input/output for the LLM\n",
    "\n",
    "# Core numerical library for array and matrix operations on embeddings\n",
    "import numpy as np  # Handles vector math, concatenation, and statistical computations\n",
    "\n",
    "# Pretty-printing complex Python objects during development/debugging\n",
    "from pprint import pprint  # Nicely formats nested dicts or lists when exploring outputs"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< Updated upstream
   "id": "4JHRRiuKlM3t",
   "metadata": {
    "id": "4JHRRiuKlM3t"
   },
   "source": [
    "# Define the Retrieval Functions\n",
    "\n",
    "This notebook uses **two different APIs** depending on whether a question is stable or time-sensitive:\n",
    "\n",
    "| Question type | Backend | Cached? |\n",
    "|---|---|---|\n",
    "| Stable / document-grounded | **Traversaal Pro** (RAG over AWS guidebook) | ‚úÖ Yes |\n",
    "| Time-sensitive / live data | **SerpApi** (Google search results) | ‚ùå Never |\n",
    "\n",
    "---\n",
    "\n",
    "## Traversaal Pro ‚Äî RAG as a Service\n",
    "\n",
    "[Traversaal Pro](https://pro.traversaal.ai) is a hosted RAG platform. You upload documents into a project; the API handles chunking, embedding, retrieval, and generation. In this notebook the corpus is the **AWS Guidebook**.\n",
    "\n",
    "**API details:**\n",
    "\n",
    "| Property | Value |\n",
    "|---|---|\n",
    "| Endpoint | `POST https://pro-documents.traversaal-api.com/documents/search` |\n",
    "| Auth | `Authorization: Bearer <your_token>` |\n",
    "| Request | `{\"query\": \"...\", \"generation\": true}` |\n",
    "| Response | `{\"response\": \"...\", \"references\": [{score, chunk_text, ...}]}` |\n",
    "\n",
    "Sign up at [pro.traversaal.ai](https://pro.traversaal.ai) to get your Bearer token.\n",
    "\n",
    "---\n",
    "\n",
    "## SerpApi ‚Äî Live Internet Search\n",
    "\n",
    "[SerpApi](https://serpapi.com) provides structured Google search results via a REST API. We use it for time-sensitive questions that require up-to-date information from the web (current events, live pricing, outages, etc.) ‚Äî answers that must never be served from cache.\n",
    "\n",
    "**API details:**\n",
    "\n",
    "| Property | Value |\n",
    "|---|---|\n",
    "| Endpoint | `GET https://serpapi.com/search.json` |\n",
    "| Auth | `?api_key=<your_key>` query param |\n",
    "| Key params | `q=<query>`, `engine=google`, `num=5` |\n",
    "| Response | `organic_results[].snippet`, `answer_box` |\n",
    "\n",
    "Sign up at [serpapi.com](https://serpapi.com) for a free API key (100 searches/month on the free tier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
=======
   "source": "# Define the Retrieval Functions\n\nThis notebook uses **two different APIs** depending on whether a question is stable or time-sensitive:\n\n| Question type | Backend | Cached? |\n|---|---|---|\n| Stable / document-grounded | **Traversaal Pro** (RAG over AWS guidebook) | ‚úÖ Yes |\n| Time-sensitive / live data | **SerpApi** (Google search results) | ‚ùå Never |\n\n---\n\n## Traversaal Pro ‚Äî RAG as a Service\n\n[Traversaal Pro](https://pro.traversaal.ai) is a hosted RAG platform. You upload documents into a project; the API handles chunking, embedding, retrieval, and generation. In this notebook the corpus is the **AWS Guidebook**.\n\n**API details:**\n\n| Property | Value |\n|---|---|\n| Endpoint | `POST https://pro-documents.traversaal-api.com/documents/search` |\n| Auth | `Authorization: Bearer <your_token>` |\n| Request | `{\"query\": \"...\", \"generation\": true}` |\n| Response | `{\"response\": \"...\", \"references\": [{score, chunk_text, ...}]}` |\n\nSign up at [pro.traversaal.ai](https://pro.traversaal.ai) to get your Bearer token.\n\n---\n\n## SerpApi ‚Äî Live Internet Search\n\n[SerpApi](https://serpapi.com) provides structured Google search results via a REST API. We use it for time-sensitive questions that require up-to-date information from the web (current events, live pricing, outages, etc.) ‚Äî answers that must never be served from cache.\n\n**API details:**\n\n| Property | Value |\n|---|---|\n| Endpoint | `GET https://serpapi.com/search.json` |\n| Auth | `?api_key=<your_key>` query param |\n| Key params | `q=<query>`, `engine=google`, `num=5` |\n| Response | `organic_results[].snippet`, `answer_box` |\n\nSign up at [serpapi.com](https://serpapi.com) for a free API key (100 searches/month on the free tier).",
   "metadata": {
    "id": "4JHRRiuKlM3t"
   },
   "id": "4JHRRiuKlM3t"
  },
  {
   "cell_type": "code",
   "execution_count": null,
>>>>>>> Stashed changes
   "id": "38331891-adb4-4d16-b26f-d74d7c9ce728",
   "metadata": {
    "id": "38331891-adb4-4d16-b26f-d74d7c9ce728"
   },
<<<<<<< Updated upstream
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running locally ‚Äî credentials loaded from .env file.\n",
      "Traversaal Pro key loaded: ‚úÖ\n",
      "SerpApi key loaded:        ‚úÖ\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests  # HTTP client for REST API calls\n",
    "\n",
    "# ‚îÄ‚îÄ Credential loading ‚Äî works on Colab and locally ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# On Colab:  store keys in the Secrets panel (üîë left sidebar)\n",
    "#              TRAVERSAAL_PRO_API_KEY\n",
    "#              SERP_API_KEY\n",
    "# Locally:   keys are read from Module_3_Agentic_RAG/.env\n",
    "#              traversaal_pro_api_key=<token>\n",
    "#              serp_api_key=<key>\n",
    "\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    traversaal_pro_api_key = userdata.get(\"TRAVERSAAL_PRO_API_KEY\")\n",
    "    serp_api_key = userdata.get(\"SERP_API_KEY\")\n",
    "    print(\"Running on Colab ‚Äî credentials loaded from Secrets.\")\n",
    "except ImportError:\n",
    "    from dotenv import load_dotenv, find_dotenv\n",
    "    load_dotenv(find_dotenv())   # walks up the directory tree to find .env\n",
    "    # .env uses lowercase key names; fall back to uppercase too\n",
    "    traversaal_pro_api_key = os.getenv(\"traversaal_pro_api_key\") or os.getenv(\"TRAVERSAAL_PRO_API_KEY\")\n",
    "    serp_api_key = os.getenv(\"serp_api_key\") or os.getenv(\"SERP_API_KEY\")\n",
    "    print(\"Running locally ‚Äî credentials loaded from .env file.\")\n",
    "\n",
    "print(f\"Traversaal Pro key loaded: {'‚úÖ' if traversaal_pro_api_key else '‚ùå MISSING'}\")\n",
    "print(f\"SerpApi key loaded:        {'‚úÖ' if serp_api_key else '‚ùå MISSING'}\")\n",
    "\n",
    "\n",
    "# ‚îÄ‚îÄ Traversaal Pro: RAG over AWS Guidebook ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "def make_prediction(query: str) -> dict:\n",
    "    \"\"\"\n",
    "    Query the Traversaal Pro RAG API with a natural language question.\n",
    "\n",
    "    The API performs retrieval over the configured document corpus (AWS Guidebook)\n",
    "    and returns a generated answer together with source chunk references.\n",
    "\n",
    "    Request:\n",
    "        POST https://pro-documents.traversaal-api.com/documents/search\n",
    "        {\"query\": \"...\", \"generation\": true}\n",
    "\n",
    "    Response:\n",
    "        {\n",
    "          \"response\": \"<generated answer string>\",\n",
    "          \"references\": [\n",
    "            {\n",
    "              \"score\": 0.81,\n",
    "              \"file_id\": \"...\",\n",
    "              \"chunk_index\": 1,\n",
    "              \"chunk_text\": \"...\",\n",
    "              \"original_file_name\": \"aws-guide.pdf\"\n",
    "            },\n",
    "            ...\n",
    "          ]\n",
    "        }\n",
    "\n",
    "    Args:\n",
    "        query (str): Natural language question answerable from the AWS Guidebook.\n",
    "\n",
    "    Returns:\n",
    "        dict: Full API response with 'response' and 'references' keys.\n",
    "    \"\"\"\n",
    "    if not traversaal_pro_api_key:\n",
    "        raise RuntimeError(\"Missing TRAVERSAAL_PRO_API_KEY ‚Äî add it to Colab Secrets or .env\")\n",
    "\n",
    "    url = \"https://pro-documents.traversaal-api.com/documents/search\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {traversaal_pro_api_key}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "    payload = {\"query\": query, \"generation\": True}\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, json=payload, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            print(\"Traversaal Pro: request successful.\")\n",
    "            try:\n",
    "                return response.json()\n",
    "            except ValueError:\n",
    "                print(\"Response was not valid JSON.\")\n",
    "                return None\n",
    "        else:\n",
    "            print(f\"Traversaal Pro: request failed ({response.status_code}): {response.text}\")\n",
    "            return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Traversaal Pro: request error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# ‚îÄ‚îÄ SerpApi: Live Google Search ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "def search_live(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Search Google in real time using SerpApi and return a formatted answer.\n",
    "\n",
    "    Used exclusively for time-sensitive questions (current events, live pricing,\n",
    "    outages, etc.) where a cached answer would quickly become stale.\n",
    "    Results are intentionally NOT stored in the semantic cache.\n",
    "\n",
    "    Args:\n",
    "        query (str): The time-sensitive question to search for.\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted string combining the answer box (if present) and\n",
    "             top organic result snippets.\n",
    "    \"\"\"\n",
    "    if not serp_api_key:\n",
    "        raise RuntimeError(\"Missing SERP_API_KEY ‚Äî add it to Colab Secrets or .env\")\n",
    "\n",
    "    print(\"SerpApi: fetching live search results üåê ...\")\n",
    "    params = {\n",
    "        \"q\": query,\n",
    "        \"api_key\": serp_api_key,\n",
    "        \"engine\": \"google\",\n",
    "        \"num\": 5,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(\"https://serpapi.com/search.json\", params=params)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "\n",
    "        parts = []\n",
    "\n",
    "        # Answer box ‚Äî Google's highlighted direct answer (most relevant)\n",
    "        answer_box = data.get(\"answer_box\", {})\n",
    "        if answer_box.get(\"answer\"):\n",
    "            parts.append(f\"[Direct Answer] {answer_box['answer']}\")\n",
    "        elif answer_box.get(\"snippet\"):\n",
    "            parts.append(f\"[Direct Answer] {answer_box['snippet']}\")\n",
    "\n",
    "        # Top organic results ‚Äî titles + snippets\n",
    "        for i, result in enumerate(data.get(\"organic_results\", [])[:5], start=1):\n",
    "            title = result.get(\"title\", \"\")\n",
    "            snippet = result.get(\"snippet\", \"\")\n",
    "            link = result.get(\"link\", \"\")\n",
    "            if snippet:\n",
    "                parts.append(f\"[{i}] {title}\\n    {snippet}\\n    Source: {link}\")\n",
    "\n",
    "        if not parts:\n",
    "            return \"No results found.\"\n",
    "\n",
    "        return \"\\n\\n\".join(parts)\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"SerpApi request error: {e}\"\n",
    "    except Exception as e:\n",
    "        return f\"Unexpected error: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "QC027Sholey1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QC027Sholey1",
    "outputId": "8b479bc0-3861-4dbb-b389-b964df66a8bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traversaal Pro: request successful.\n",
      "Generated answer:\n",
      "An S3 bucket in AWS is conceptually similar to a folder in a traditional file-storage system. It is a container where objects (data) are stored within the Amazon Simple Storage Service (S3). Each S3 bucket must be created before any data can be stored in it. The name of an S3 bucket must be globally unique across all AWS accounts, and access control can be implemented at the bucket level. For example, if there is an object with the key \"omgcat.png\" in the S3 bucket \"adorable-cat-photos,\" the addressable path of the object would be \"s3://adorable-cat-photos/omgcat.png\" [1].\n",
      "\n",
      "Top source reference:\n",
      "  Score: 0.820\n",
      "  File:  aws-guide (1).pdf\n",
      "  Chunk: Amazon Simple Storage Service (S3)\n",
      "What is it?\n",
      "Amazon Simple Storage Service, commonly known as S3, is a fast, scalable, and durable\n",
      "object-storage service. S3 can be used to store and retrieve any ty...\n"
     ]
    }
   ],
   "source": [
    "# Test Traversaal Pro ‚Äî stable AWS question (answer comes from the AWS Guidebook)\n",
    "result = make_prediction(\"What is an S3 bucket in AWS?\")\n",
    "print(\"Generated answer:\")\n",
    "print(result[\"response\"])\n",
    "print(\"\\nTop source reference:\")\n",
    "if result.get(\"references\"):\n",
    "    top_ref = result[\"references\"][0]\n",
    "    print(f\"  Score: {top_ref['score']:.3f}\")\n",
    "    print(f\"  File:  {top_ref['original_file_name']}\")\n",
    "    print(f\"  Chunk: {top_ref['chunk_text'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "vr6dbehodif",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SerpApi: fetching live search results üåê ...\n",
      "[1] AWS live status. Problems and outages for Amazon Web ...\n",
      "    Real-time AWS (Amazon Web Services) status. Is AWS down or suffering an outages? Here you see what is going on.\n",
      "    Source: https://downdetector.com/status/aws-amazon-web-services/\n",
      "\n",
      "[2] Service health - Feb 19, 2026 | AWS Health Dashboard | Global\n",
      "    The following table is a running log of AWS service interruptions for the past 12 months. Choose a status icon to see status updates for ...\n",
      "    Source: https://health.aws.amazon.com/\n",
      "\n",
      "[3] Service health - Feb 19, 2026 | AWS Health Dashboard\n",
      "    View the overall status and health of AWS services using the AWS Health Dashboard.\n",
      "    Source: https://health.aws.amazon.com/health/status?eventID=arn:aws:health:us-east-1::event/MULTIPLE_SERVICES/AWS_MULTIPLE_SERVICES_OPERATIONAL_ISSUE/AWS_MULTIPLE_SERVICES_OPERATIONAL_ISSUE_BA540_514A652BE1A\n",
      "\n",
      "[4] Aws.amazon.com - Is Amazon Web Services Down ...\n",
      "    Aws.amazon.com is UP and reachable by us. Please check and report on local outages below ... The above graph displays service status activity for Aws.amazon.com ...\n",
      "    Source: https://www.isitdownrightnow.com/aws.amazon.com.html\n"
     ]
    }
   ],
   "source": [
    "# Test SerpApi ‚Äî time-sensitive question (live internet search, NOT from documents)\n",
    "live_answer = search_live(\"Are there any AWS outages right now?\")\n",
    "print(live_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db-RrmtuqFhC",
   "metadata": {
    "id": "db-RrmtuqFhC"
   },
   "source": [
    "### Define SemanticCaching Class\n",
    "\n",
    "In this cell we define `SemanticCaching`‚Äîa lightweight cache with dual-backend routing:\n",
    "\n",
    "1. **Time-sensitive guard** ‚Äî detects temporal keywords and routes to **SerpApi** (live Google search), bypassing the cache entirely.  \n",
    "2. **FAISS lookup** ‚Äî for stable questions, checks if a semantically similar question was already answered. If yes, returns the cached answer instantly.  \n",
    "3. **Traversaal Pro fallback** ‚Äî on a cache miss, queries the **AWS Guidebook RAG** to get a document-grounded answer, then stores it for future hits.  \n",
    "4. **JSON persistence** ‚Äî cache entries (questions, embeddings, answers) are saved to disk so the index survives notebook restarts.  \n",
    "5. **Latency logging** ‚Äî every call reports whether it was a hit, miss, or live search, and how long it took.\n",
    "\n",
    "---\n",
    "\n",
    "### What Should (and Should NOT) Be Semantically Cached?\n",
    "\n",
    "The cache is backed by the **AWS Guidebook** via Traversaal Pro. Since documentation is stable, most AWS concept questions are excellent cache candidates. The exceptions are anything that requires live, up-to-the-minute data.\n",
    "\n",
    "#### ‚úÖ Good to cache ‚Äî stable AWS documentation answers:\n",
    "| Question | Why it's safe to cache |\n",
    "|---|---|\n",
    "| *\"What is an S3 bucket in AWS?\"* | Core concept, always the same |\n",
    "| *\"How does AWS Lambda work?\"* | Stable service behaviour |\n",
    "| *\"What is AWS IAM?\"* | Conceptual definition from docs |\n",
    "| *\"What is the difference between EC2 and ECS?\"* | Architectural comparison |\n",
    "| *\"How does Amazon CloudFront work?\"* | Service explanation |\n",
    "| *\"What is an AWS VPC?\"* | Networking concept |\n",
    "\n",
    "#### ‚ùå Do NOT cache ‚Äî time-sensitive, answers change even for AWS:\n",
    "| Question | Why it must NOT be cached | Backend |\n",
    "|---|---|---|\n",
    "| *\"Are there any AWS outages right now?\"* | Status changes minute to minute | SerpApi |\n",
    "| *\"What are the latest AWS features this week?\"* | New releases announced daily | SerpApi |\n",
    "| *\"What is the current EC2 pricing today?\"* | AWS updates pricing periodically | SerpApi |\n",
    "| *\"Is AWS S3 down right now?\"* | Real-time health check | SerpApi |\n",
    "| *\"What new services did AWS announce this month?\"* | New info every month | SerpApi |\n",
    "\n",
    "The `is_time_sensitive()` method catches these using a keyword list and routes them to SerpApi ‚Äî they never touch the FAISS index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "yDHhY-OBSEIw",
   "metadata": {
    "id": "yDHhY-OBSEIw"
   },
   "outputs": [],
   "source": [
    "import faiss            # Efficient similarity search over vector embeddings\n",
    "import json             # Read/write cache from a JSON file\n",
    "import numpy as np      # Numerical operations on embeddings\n",
    "from sentence_transformers import SentenceTransformer  # Load Nomic embed model\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM  # (Optional) LLM for answer gen\n",
    "import time             # Measure latency\n",
    "\n",
    "class SemanticCaching:\n",
    "    \"\"\"\n",
    "    A semantic cache that routes queries to the right backend:\n",
    "\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "    ‚îÇ  Query                                                   ‚îÇ\n",
    "    ‚îÇ    ‚îÇ                                                     ‚îÇ\n",
    "    ‚îÇ    ‚îú‚îÄ Time-sensitive? ‚îÄ‚îÄYES‚îÄ‚îÄ‚ñ∂ SerpApi (live search)     ‚îÇ\n",
    "    ‚îÇ    ‚îÇ                           NOT cached                ‚îÇ\n",
    "    ‚îÇ    ‚îÇ                                                     ‚îÇ\n",
    "    ‚îÇ    ‚îî‚îÄ Stable? ‚îÄ‚îÄ‚ñ∂ FAISS lookup                          ‚îÇ\n",
    "    ‚îÇ                     ‚îÇ                                    ‚îÇ\n",
    "    ‚îÇ                     ‚îú‚îÄ HIT  ‚îÄ‚îÄ‚ñ∂ return cached answer ‚ö°  ‚îÇ\n",
    "    ‚îÇ                     ‚îÇ                                    ‚îÇ\n",
    "    ‚îÇ                     ‚îî‚îÄ MISS ‚îÄ‚îÄ‚ñ∂ Traversaal Pro (RAG)     ‚îÇ\n",
    "    ‚îÇ                                 store ‚Üí return           ‚îÇ\n",
    "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "    \"\"\"\n",
    "\n",
    "    # Keywords that signal the question is time-sensitive and must NOT be cached.\n",
    "    # Answers to these questions change over time ‚Äî caching would return stale results.\n",
    "    TIME_SENSITIVE_KEYWORDS = [\n",
    "        \"today\", \"tonight\", \"now\", \"currently\", \"current\",\n",
    "        \"latest\", \"recent\", \"recently\", \"right now\", \"at the moment\",\n",
    "        \"at present\", \"as of now\", \"this week\", \"this month\", \"this year\",\n",
    "        \"this quarter\", \"this season\", \"this morning\", \"this afternoon\",\n",
    "        \"this evening\", \"this weekend\", \"yesterday\", \"tomorrow\",\n",
    "        \"last week\", \"last month\", \"last year\", \"upcoming\", \"live\",\n",
    "        \"breaking\", \"just happened\", \"what time\", \"what day\", \"what date\",\n",
    "        \"happening now\", \"events today\", \"news today\", \"news this week\",\n",
    "        \"stock price\", \"share price\", \"weather\", \"forecast\", \"temperature\",\n",
    "        \"real-time\", \"realtime\", \"schedule today\", \"outage\", \"down right now\",\n",
    "        \"is aws down\", \"aws status\",\n",
    "    ]\n",
    "\n",
    "    def __init__(self, json_file='cache.json', clear_on_init=False):\n",
    "        # Initialize Faiss index with Euclidean distance\n",
    "        self.index = faiss.IndexFlatL2(768)\n",
    "        if self.index.is_trained:\n",
    "            print('Index trained')\n",
    "\n",
    "        # Initialize Sentence Transformer model\n",
    "        self.encoder = SentenceTransformer('nomic-ai/nomic-embed-text-v1.5', trust_remote_code=True)\n",
    "\n",
    "        # Euclidean distance threshold for cache hits (lower = stricter)\n",
    "        self.euclidean_threshold = 0.2\n",
    "\n",
    "        # JSON file to persist cache entries\n",
    "        self.json_file = json_file\n",
    "\n",
    "        # Load cache or clear already loaded cache\n",
    "        if clear_on_init:\n",
    "            self.clear_cache()\n",
    "        else:\n",
    "            self.load_cache()\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Time-sensitivity detection\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    def is_time_sensitive(self, question: str) -> bool:\n",
    "        \"\"\"\n",
    "        Returns True if the question is time-sensitive and should NOT be cached.\n",
    "\n",
    "        Time-sensitive questions reference current events, live data, or time-bound\n",
    "        information whose answers change frequently. These are routed to SerpApi\n",
    "        for a real-time Google search answer instead of the document RAG system.\n",
    "\n",
    "        Examples that return True (‚Üí SerpApi, never cached):\n",
    "            'Are there any AWS outages right now?'\n",
    "            'What are the latest AWS features released this week?'\n",
    "            'What is the current EC2 pricing today?'\n",
    "            'Is AWS S3 down right now?'\n",
    "\n",
    "        Examples that return False (‚Üí check cache, then Traversaal Pro if miss):\n",
    "            'What is an S3 bucket in AWS?'\n",
    "            'How does AWS Lambda work?'\n",
    "            'What is AWS IAM?'\n",
    "            'What is the difference between EC2 and ECS?'\n",
    "        \"\"\"\n",
    "        question_lower = question.lower()\n",
    "        return any(keyword in question_lower for keyword in self.TIME_SENSITIVE_KEYWORDS)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Cache persistence\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    def clear_cache(self):\n",
    "        \"\"\"Clears in-memory cache, resets FAISS index, and overwrites the JSON file.\"\"\"\n",
    "        self.cache = {\n",
    "            'questions': [],\n",
    "            'embeddings': [],\n",
    "            'answers': [],\n",
    "            'response_text': []\n",
    "        }\n",
    "        self.index = faiss.IndexFlatL2(768)\n",
    "        self.save_cache()\n",
    "        print(\"Semantic cache cleared.\")\n",
    "\n",
    "    def load_cache(self):\n",
    "        \"\"\"Load existing cache or initialize empty structure.\"\"\"\n",
    "        try:\n",
    "            with open(self.json_file, 'r') as file:\n",
    "                self.cache = json.load(file)\n",
    "        except FileNotFoundError:\n",
    "            self.cache = {'questions': [], 'embeddings': [], 'answers': [], 'response_text': []}\n",
    "\n",
    "    def save_cache(self):\n",
    "        \"\"\"Persist cache back to disk.\"\"\"\n",
    "        with open(self.json_file, 'w') as file:\n",
    "            json.dump(self.cache, file)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Main query method\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    def ask(self, question: str) -> str:\n",
    "        \"\"\"\n",
    "        Route the question to the correct backend and return an answer.\n",
    "\n",
    "        Routing logic:\n",
    "          1. Time-sensitive  ‚Üí SerpApi (live Google search) ‚Äî answer NOT cached\n",
    "          2. Cache HIT       ‚Üí return stored answer instantly\n",
    "          3. Cache MISS      ‚Üí Traversaal Pro (RAG over AWS docs) ‚Äî answer stored\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        # ‚îÄ‚îÄ 1. Time-sensitivity guard ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "        # Live search via SerpApi ‚Äî result intentionally not stored\n",
    "        if self.is_time_sensitive(question):\n",
    "            print(\"‚è∞ Time-sensitive question ‚Äî routing to SerpApi (live search, not cached).\")\n",
    "            response_text = search_live(question)\n",
    "            print(f\"Time taken: {time.time() - start_time:.3f}s\")\n",
    "            return response_text\n",
    "\n",
    "        try:\n",
    "            # ‚îÄ‚îÄ 2. Cache lookup ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "            embedding = self.encoder.encode([question], normalize_embeddings=True)\n",
    "            D, I = self.index.search(embedding, 1)\n",
    "\n",
    "            if D[0] >= 0:\n",
    "                if I[0][0] != -1 and D[0][0] <= self.euclidean_threshold:\n",
    "                    row_id = int(I[0][0])\n",
    "                    print(f'‚úÖ Cache hit at row: {row_id} | similarity: {1 - D[0][0]:.4f}')\n",
    "                    print(f\"Time taken: {time.time() - start_time:.3f}s\")\n",
    "                    return self.cache['response_text'][row_id]\n",
    "\n",
    "            # ‚îÄ‚îÄ 3. Cache miss ‚Üí Traversaal Pro RAG ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "            answer, response_text = self.generate_answer(question)\n",
    "\n",
    "            self.cache['questions'].append(question)\n",
    "            self.cache['embeddings'].append(embedding[0].tolist())\n",
    "            self.cache['answers'].append(answer)\n",
    "            self.cache['response_text'].append(response_text)\n",
    "            self.index.add(embedding)\n",
    "            self.save_cache()\n",
    "            print(f\"Time taken: {time.time() - start_time:.3f}s\")\n",
    "\n",
    "            return response_text\n",
    "\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error during 'ask' method: {e}\")\n",
    "\n",
    "    def generate_answer(self, question: str):\n",
    "        \"\"\"\n",
    "        Call Traversaal Pro to answer a stable document-grounded question.\n",
    "\n",
    "        Uses the AWS Guidebook corpus loaded into your Traversaal Pro project.\n",
    "        Extracts the 'response' field from the API reply as the answer text.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (full API result dict, answer string)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            result = make_prediction(question)\n",
    "            # Traversaal Pro returns {\"response\": \"...\", \"references\": [...]}\n",
    "            response_text = result.get('response', str(result))\n",
    "            return result, response_text\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error during 'generate_answer' method: {e}\")"
   ]
=======
   "outputs": [],
   "source": "import os\nimport requests  # HTTP client for REST API calls\n\n# ‚îÄ‚îÄ Credential loading ‚Äî works on Colab and locally ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n# On Colab:  store keys in the Secrets panel (üîë left sidebar)\n#              TRAVERSAAL_PRO_API_KEY\n#              SERP_API_KEY\n# Locally:   keys are read from Module_3_Agentic_RAG/.env\n#              traversaal_pro_api_key=<token>\n#              serp_api_key=<key>\n\ntry:\n    from google.colab import userdata\n    traversaal_pro_api_key = userdata.get(\"TRAVERSAAL_PRO_API_KEY\")\n    serp_api_key = userdata.get(\"SERP_API_KEY\")\n    print(\"Running on Colab ‚Äî credentials loaded from Secrets.\")\nexcept ImportError:\n    from dotenv import load_dotenv, find_dotenv\n    load_dotenv(find_dotenv())   # walks up the directory tree to find .env\n    # .env uses lowercase key names; fall back to uppercase too\n    traversaal_pro_api_key = os.getenv(\"traversaal_pro_api_key\") or os.getenv(\"TRAVERSAAL_PRO_API_KEY\")\n    serp_api_key = os.getenv(\"serp_api_key\") or os.getenv(\"SERP_API_KEY\")\n    print(\"Running locally ‚Äî credentials loaded from .env file.\")\n\nprint(f\"Traversaal Pro key loaded: {'‚úÖ' if traversaal_pro_api_key else '‚ùå MISSING'}\")\nprint(f\"SerpApi key loaded:        {'‚úÖ' if serp_api_key else '‚ùå MISSING'}\")\n\n\n# ‚îÄ‚îÄ Traversaal Pro: RAG over AWS Guidebook ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\ndef make_prediction(query: str) -> dict:\n    \"\"\"\n    Query the Traversaal Pro RAG API with a natural language question.\n\n    The API performs retrieval over the configured document corpus (AWS Guidebook)\n    and returns a generated answer together with source chunk references.\n\n    Request:\n        POST https://pro-documents.traversaal-api.com/documents/search\n        {\"query\": \"...\", \"generation\": true}\n\n    Response:\n        {\n          \"response\": \"<generated answer string>\",\n          \"references\": [\n            {\n              \"score\": 0.81,\n              \"file_id\": \"...\",\n              \"chunk_index\": 1,\n              \"chunk_text\": \"...\",\n              \"original_file_name\": \"aws-guide.pdf\"\n            },\n            ...\n          ]\n        }\n\n    Args:\n        query (str): Natural language question answerable from the AWS Guidebook.\n\n    Returns:\n        dict: Full API response with 'response' and 'references' keys.\n    \"\"\"\n    if not traversaal_pro_api_key:\n        raise RuntimeError(\"Missing TRAVERSAAL_PRO_API_KEY ‚Äî add it to Colab Secrets or .env\")\n\n    url = \"https://pro-documents.traversaal-api.com/documents/search\"\n    headers = {\n        \"Authorization\": f\"Bearer {traversaal_pro_api_key}\",\n        \"Content-Type\": \"application/json\",\n    }\n    payload = {\"query\": query, \"generation\": True}\n\n    try:\n        response = requests.post(url, json=payload, headers=headers)\n        if response.status_code == 200:\n            print(\"Traversaal Pro: request successful.\")\n            try:\n                return response.json()\n            except ValueError:\n                print(\"Response was not valid JSON.\")\n                return None\n        else:\n            print(f\"Traversaal Pro: request failed ({response.status_code}): {response.text}\")\n            return None\n    except requests.exceptions.RequestException as e:\n        print(f\"Traversaal Pro: request error: {e}\")\n        return None\n\n\n# ‚îÄ‚îÄ SerpApi: Live Google Search ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\ndef search_live(query: str) -> str:\n    \"\"\"\n    Search Google in real time using SerpApi and return a formatted answer.\n\n    Used exclusively for time-sensitive questions (current events, live pricing,\n    outages, etc.) where a cached answer would quickly become stale.\n    Results are intentionally NOT stored in the semantic cache.\n\n    Args:\n        query (str): The time-sensitive question to search for.\n\n    Returns:\n        str: A formatted string combining the answer box (if present) and\n             top organic result snippets.\n    \"\"\"\n    if not serp_api_key:\n        raise RuntimeError(\"Missing SERP_API_KEY ‚Äî add it to Colab Secrets or .env\")\n\n    print(\"SerpApi: fetching live search results üåê ...\")\n    params = {\n        \"q\": query,\n        \"api_key\": serp_api_key,\n        \"engine\": \"google\",\n        \"num\": 5,\n    }\n\n    try:\n        response = requests.get(\"https://serpapi.com/search.json\", params=params)\n        response.raise_for_status()\n        data = response.json()\n\n        parts = []\n\n        # Answer box ‚Äî Google's highlighted direct answer (most relevant)\n        answer_box = data.get(\"answer_box\", {})\n        if answer_box.get(\"answer\"):\n            parts.append(f\"[Direct Answer] {answer_box['answer']}\")\n        elif answer_box.get(\"snippet\"):\n            parts.append(f\"[Direct Answer] {answer_box['snippet']}\")\n\n        # Top organic results ‚Äî titles + snippets\n        for i, result in enumerate(data.get(\"organic_results\", [])[:5], start=1):\n            title = result.get(\"title\", \"\")\n            snippet = result.get(\"snippet\", \"\")\n            link = result.get(\"link\", \"\")\n            if snippet:\n                parts.append(f\"[{i}] {title}\\n    {snippet}\\n    Source: {link}\")\n\n        if not parts:\n            return \"No results found.\"\n\n        return \"\\n\\n\".join(parts)\n\n    except requests.exceptions.RequestException as e:\n        return f\"SerpApi request error: {e}\"\n    except Exception as e:\n        return f\"Unexpected error: {e}\""
  },
  {
   "cell_type": "code",
   "source": "# Test Traversaal Pro ‚Äî stable AWS question (answer comes from the AWS Guidebook)\nresult = make_prediction(\"What is an S3 bucket in AWS?\")\nprint(\"Generated answer:\")\nprint(result[\"response\"])\nprint(\"\\nTop source reference:\")\nif result.get(\"references\"):\n    top_ref = result[\"references\"][0]\n    print(f\"  Score: {top_ref['score']:.3f}\")\n    print(f\"  File:  {top_ref['original_file_name']}\")\n    print(f\"  Chunk: {top_ref['chunk_text'][:200]}...\")",
   "metadata": {
    "id": "QC027Sholey1",
    "outputId": "8b479bc0-3861-4dbb-b389-b964df66a8bd",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "QC027Sholey1",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "vr6dbehodif",
   "source": "# Test SerpApi ‚Äî time-sensitive question (live internet search, NOT from documents)\nlive_answer = search_live(\"Are there any AWS outages right now?\")\nprint(live_answer)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Define SemanticCaching Class\n\nIn this cell we define `SemanticCaching`‚Äîa lightweight cache with dual-backend routing:\n\n1. **Time-sensitive guard** ‚Äî detects temporal keywords and routes to **SerpApi** (live Google search), bypassing the cache entirely.  \n2. **FAISS lookup** ‚Äî for stable questions, checks if a semantically similar question was already answered. If yes, returns the cached answer instantly.  \n3. **Traversaal Pro fallback** ‚Äî on a cache miss, queries the **AWS Guidebook RAG** to get a document-grounded answer, then stores it for future hits.  \n4. **JSON persistence** ‚Äî cache entries (questions, embeddings, answers) are saved to disk so the index survives notebook restarts.  \n5. **Latency logging** ‚Äî every call reports whether it was a hit, miss, or live search, and how long it took.\n\n---\n\n### What Should (and Should NOT) Be Semantically Cached?\n\nThe cache is backed by the **AWS Guidebook** via Traversaal Pro. Since documentation is stable, most AWS concept questions are excellent cache candidates. The exceptions are anything that requires live, up-to-the-minute data.\n\n#### ‚úÖ Good to cache ‚Äî stable AWS documentation answers:\n| Question | Why it's safe to cache |\n|---|---|\n| *\"What is an S3 bucket in AWS?\"* | Core concept, always the same |\n| *\"How does AWS Lambda work?\"* | Stable service behaviour |\n| *\"What is AWS IAM?\"* | Conceptual definition from docs |\n| *\"What is the difference between EC2 and ECS?\"* | Architectural comparison |\n| *\"How does Amazon CloudFront work?\"* | Service explanation |\n| *\"What is an AWS VPC?\"* | Networking concept |\n\n#### ‚ùå Do NOT cache ‚Äî time-sensitive, answers change even for AWS:\n| Question | Why it must NOT be cached | Backend |\n|---|---|---|\n| *\"Are there any AWS outages right now?\"* | Status changes minute to minute | SerpApi |\n| *\"What are the latest AWS features this week?\"* | New releases announced daily | SerpApi |\n| *\"What is the current EC2 pricing today?\"* | AWS updates pricing periodically | SerpApi |\n| *\"Is AWS S3 down right now?\"* | Real-time health check | SerpApi |\n| *\"What new services did AWS announce this month?\"* | New info every month | SerpApi |\n\nThe `is_time_sensitive()` method catches these using a keyword list and routes them to SerpApi ‚Äî they never touch the FAISS index.",
   "metadata": {
    "id": "db-RrmtuqFhC"
   },
   "id": "db-RrmtuqFhC"
  },
  {
   "cell_type": "code",
   "source": "import faiss            # Efficient similarity search over vector embeddings\nimport json             # Read/write cache from a JSON file\nimport numpy as np      # Numerical operations on embeddings\nfrom sentence_transformers import SentenceTransformer  # Load Nomic embed model\nfrom transformers import AutoTokenizer, AutoModelForCausalLM  # (Optional) LLM for answer gen\nimport time             # Measure latency\n\nclass SemanticCaching:\n    \"\"\"\n    A semantic cache that routes queries to the right backend:\n\n    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n    ‚îÇ  Query                                                   ‚îÇ\n    ‚îÇ    ‚îÇ                                                     ‚îÇ\n    ‚îÇ    ‚îú‚îÄ Time-sensitive? ‚îÄ‚îÄYES‚îÄ‚îÄ‚ñ∂ SerpApi (live search)     ‚îÇ\n    ‚îÇ    ‚îÇ                           NOT cached                ‚îÇ\n    ‚îÇ    ‚îÇ                                                     ‚îÇ\n    ‚îÇ    ‚îî‚îÄ Stable? ‚îÄ‚îÄ‚ñ∂ FAISS lookup                          ‚îÇ\n    ‚îÇ                     ‚îÇ                                    ‚îÇ\n    ‚îÇ                     ‚îú‚îÄ HIT  ‚îÄ‚îÄ‚ñ∂ return cached answer ‚ö°  ‚îÇ\n    ‚îÇ                     ‚îÇ                                    ‚îÇ\n    ‚îÇ                     ‚îî‚îÄ MISS ‚îÄ‚îÄ‚ñ∂ Traversaal Pro (RAG)     ‚îÇ\n    ‚îÇ                                 store ‚Üí return           ‚îÇ\n    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n    \"\"\"\n\n    # Keywords that signal the question is time-sensitive and must NOT be cached.\n    # Answers to these questions change over time ‚Äî caching would return stale results.\n    TIME_SENSITIVE_KEYWORDS = [\n        \"today\", \"tonight\", \"now\", \"currently\", \"current\",\n        \"latest\", \"recent\", \"recently\", \"right now\", \"at the moment\",\n        \"at present\", \"as of now\", \"this week\", \"this month\", \"this year\",\n        \"this quarter\", \"this season\", \"this morning\", \"this afternoon\",\n        \"this evening\", \"this weekend\", \"yesterday\", \"tomorrow\",\n        \"last week\", \"last month\", \"last year\", \"upcoming\", \"live\",\n        \"breaking\", \"just happened\", \"what time\", \"what day\", \"what date\",\n        \"happening now\", \"events today\", \"news today\", \"news this week\",\n        \"stock price\", \"share price\", \"weather\", \"forecast\", \"temperature\",\n        \"real-time\", \"realtime\", \"schedule today\", \"outage\", \"down right now\",\n        \"is aws down\", \"aws status\",\n    ]\n\n    def __init__(self, json_file='cache.json', clear_on_init=False):\n        # Initialize Faiss index with Euclidean distance\n        self.index = faiss.IndexFlatL2(768)\n        if self.index.is_trained:\n            print('Index trained')\n\n        # Initialize Sentence Transformer model\n        self.encoder = SentenceTransformer('nomic-ai/nomic-embed-text-v1.5', trust_remote_code=True)\n\n        # Euclidean distance threshold for cache hits (lower = stricter)\n        self.euclidean_threshold = 0.2\n\n        # JSON file to persist cache entries\n        self.json_file = json_file\n\n        # Load cache or clear already loaded cache\n        if clear_on_init:\n            self.clear_cache()\n        else:\n            self.load_cache()\n\n    # ------------------------------------------------------------------\n    # Time-sensitivity detection\n    # ------------------------------------------------------------------\n\n    def is_time_sensitive(self, question: str) -> bool:\n        \"\"\"\n        Returns True if the question is time-sensitive and should NOT be cached.\n\n        Time-sensitive questions reference current events, live data, or time-bound\n        information whose answers change frequently. These are routed to SerpApi\n        for a real-time Google search answer instead of the document RAG system.\n\n        Examples that return True (‚Üí SerpApi, never cached):\n            'Are there any AWS outages right now?'\n            'What are the latest AWS features released this week?'\n            'What is the current EC2 pricing today?'\n            'Is AWS S3 down right now?'\n\n        Examples that return False (‚Üí check cache, then Traversaal Pro if miss):\n            'What is an S3 bucket in AWS?'\n            'How does AWS Lambda work?'\n            'What is AWS IAM?'\n            'What is the difference between EC2 and ECS?'\n        \"\"\"\n        question_lower = question.lower()\n        return any(keyword in question_lower for keyword in self.TIME_SENSITIVE_KEYWORDS)\n\n    # ------------------------------------------------------------------\n    # Cache persistence\n    # ------------------------------------------------------------------\n\n    def clear_cache(self):\n        \"\"\"Clears in-memory cache, resets FAISS index, and overwrites the JSON file.\"\"\"\n        self.cache = {\n            'questions': [],\n            'embeddings': [],\n            'answers': [],\n            'response_text': []\n        }\n        self.index = faiss.IndexFlatL2(768)\n        self.save_cache()\n        print(\"Semantic cache cleared.\")\n\n    def load_cache(self):\n        \"\"\"Load existing cache or initialize empty structure.\"\"\"\n        try:\n            with open(self.json_file, 'r') as file:\n                self.cache = json.load(file)\n        except FileNotFoundError:\n            self.cache = {'questions': [], 'embeddings': [], 'answers': [], 'response_text': []}\n\n    def save_cache(self):\n        \"\"\"Persist cache back to disk.\"\"\"\n        with open(self.json_file, 'w') as file:\n            json.dump(self.cache, file)\n\n    # ------------------------------------------------------------------\n    # Main query method\n    # ------------------------------------------------------------------\n\n    def ask(self, question: str) -> str:\n        \"\"\"\n        Route the question to the correct backend and return an answer.\n\n        Routing logic:\n          1. Time-sensitive  ‚Üí SerpApi (live Google search) ‚Äî answer NOT cached\n          2. Cache HIT       ‚Üí return stored answer instantly\n          3. Cache MISS      ‚Üí Traversaal Pro (RAG over AWS docs) ‚Äî answer stored\n        \"\"\"\n        start_time = time.time()\n\n        # ‚îÄ‚îÄ 1. Time-sensitivity guard ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n        # Live search via SerpApi ‚Äî result intentionally not stored\n        if self.is_time_sensitive(question):\n            print(\"‚è∞ Time-sensitive question ‚Äî routing to SerpApi (live search, not cached).\")\n            response_text = search_live(question)\n            print(f\"Time taken: {time.time() - start_time:.3f}s\")\n            return response_text\n\n        try:\n            # ‚îÄ‚îÄ 2. Cache lookup ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n            embedding = self.encoder.encode([question], normalize_embeddings=True)\n            D, I = self.index.search(embedding, 1)\n\n            if D[0] >= 0:\n                if I[0][0] != -1 and D[0][0] <= self.euclidean_threshold:\n                    row_id = int(I[0][0])\n                    print(f'‚úÖ Cache hit at row: {row_id} | similarity: {1 - D[0][0]:.4f}')\n                    print(f\"Time taken: {time.time() - start_time:.3f}s\")\n                    return self.cache['response_text'][row_id]\n\n            # ‚îÄ‚îÄ 3. Cache miss ‚Üí Traversaal Pro RAG ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n            answer, response_text = self.generate_answer(question)\n\n            self.cache['questions'].append(question)\n            self.cache['embeddings'].append(embedding[0].tolist())\n            self.cache['answers'].append(answer)\n            self.cache['response_text'].append(response_text)\n            self.index.add(embedding)\n            self.save_cache()\n            print(f\"Time taken: {time.time() - start_time:.3f}s\")\n\n            return response_text\n\n        except Exception as e:\n            raise RuntimeError(f\"Error during 'ask' method: {e}\")\n\n    def generate_answer(self, question: str):\n        \"\"\"\n        Call Traversaal Pro to answer a stable document-grounded question.\n\n        Uses the AWS Guidebook corpus loaded into your Traversaal Pro project.\n        Extracts the 'response' field from the API reply as the answer text.\n\n        Returns:\n            tuple: (full API result dict, answer string)\n        \"\"\"\n        try:\n            result = make_prediction(question)\n            # Traversaal Pro returns {\"response\": \"...\", \"references\": [...]}\n            response_text = result.get('response', str(result))\n            return result, response_text\n        except Exception as e:\n            raise RuntimeError(f\"Error during 'generate_answer' method: {e}\")",
   "metadata": {
    "id": "yDHhY-OBSEIw"
   },
   "id": "yDHhY-OBSEIw",
   "execution_count": null,
   "outputs": []
>>>>>>> Stashed changes
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc661dab-f7cc-4d74-9575-1c756b4cdef0",
   "metadata": {
<<<<<<< Updated upstream
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dc661dab-f7cc-4d74-9575-1c756b4cdef0",
    "outputId": "c55b9d0c-69f3-42ac-81f9-5b014a3b7415"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
=======
    "id": "dc661dab-f7cc-4d74-9575-1c756b4cdef0",
    "outputId": "c55b9d0c-69f3-42ac-81f9-5b014a3b7415",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
>>>>>>> Stashed changes
     "text": [
      "Index trained\n"
     ]
    },
    {
<<<<<<< Updated upstream
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
=======
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:transformers_modules.nomic_hyphen_ai.nomic_hyphen_bert_hyphen_2048.7710840340a098cfb869c4f65e87cf2b1b70caca.modeling_hf_nomic_bert:<All keys matched successfully>\n"
>>>>>>> Stashed changes
     ]
    }
   ],
   "source": [
    "# Instantiate the semantic cache: builds/loads FAISS index, encoder, and JSON cache\n",
    "cache = SemanticCaching()\n",
    "\n",
    "# Uncomment and use to re-instantiate the semantic cache and clear exisitng cache entries\n",
    "# cache = SemanticCaching(clear_on_init=True)"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< Updated upstream
   "id": "noc3G8Abxy2t",
   "metadata": {
    "id": "noc3G8Abxy2t"
   },
   "source": [
    "### Testing the Semantic Cache\n",
    "\n",
    "We validate the `SemanticCaching` class using AWS Guidebook questions. These are stable, document-grounded questions ‚Äî ideal for caching because the answers don't change over time.\n",
    "\n",
    "Watch the routing in action:\n",
    "- **First ask** of a question ‚Üí cache miss ‚Üí Traversaal Pro RAG ‚Üí answer stored  \n",
    "- **Rephrased version** of the same question ‚Üí cache hit ‚Üí instant return  \n",
    "- **Time-sensitive question** ‚Üí SerpApi live search ‚Üí never stored"
   ]
=======
   "source": "### Testing the Semantic Cache\n\nWe validate the `SemanticCaching` class using AWS Guidebook questions. These are stable, document-grounded questions ‚Äî ideal for caching because the answers don't change over time.\n\nWatch the routing in action:\n- **First ask** of a question ‚Üí cache miss ‚Üí Traversaal Pro RAG ‚Üí answer stored  \n- **Rephrased version** of the same question ‚Üí cache hit ‚Üí instant return  \n- **Time-sensitive question** ‚Üí SerpApi live search ‚Üí never stored",
   "metadata": {
    "id": "noc3G8Abxy2t"
   },
   "id": "noc3G8Abxy2t"
>>>>>>> Stashed changes
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f64fe4d-fe89-44a7-bf3f-3ad721985f3e",
   "metadata": {
<<<<<<< Updated upstream
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2f64fe4d-fe89-44a7-bf3f-3ad721985f3e",
    "outputId": "90ecbeb7-fd6d-4222-86ab-e472e0dbadf7"
   },
   "outputs": [],
   "source": [
    "# Q1: Cache miss ‚Äî Traversaal Pro answers from AWS docs, stores result\n",
    "question1 = \"What is an S3 bucket in AWS?\"\n",
    "answer1 = cache.ask(question1)\n",
    "print(answer1)\n",
    "\n",
    "# Q2: Cache miss ‚Äî different AWS service\n",
    "question2 = \"How does AWS Lambda work?\"\n",
    "answer2 = cache.ask(question2)\n",
    "print(answer2)\n",
    "\n",
    "# Q3: Cache miss ‚Äî another AWS service\n",
    "question3 = \"What is AWS IAM used for?\"\n",
    "answer3 = cache.ask(question3)\n",
    "print(answer3)\n",
    "\n",
    "# Q4: Cache miss ‚Äî networking concept\n",
    "question4 = \"What is an Amazon VPC?\"\n",
    "answer4 = cache.ask(question4)\n",
    "print(answer4)\n",
    "\n",
    "# Note:\n",
    "# All four are distinct enough to each get a separate Traversaal Pro call.\n",
    "# Next, we'll test cache hits with rephrased versions of these questions."
   ]
=======
    "id": "2f64fe4d-fe89-44a7-bf3f-3ad721985f3e",
    "outputId": "90ecbeb7-fd6d-4222-86ab-e472e0dbadf7",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "source": "# Q1: Cache miss ‚Äî Traversaal Pro answers from AWS docs, stores result\nquestion1 = \"What is an S3 bucket in AWS?\"\nanswer1 = cache.ask(question1)\nprint(answer1)\n\n# Q2: Cache miss ‚Äî different AWS service\nquestion2 = \"How does AWS Lambda work?\"\nanswer2 = cache.ask(question2)\nprint(answer2)\n\n# Q3: Cache miss ‚Äî another AWS service\nquestion3 = \"What is AWS IAM used for?\"\nanswer3 = cache.ask(question3)\nprint(answer3)\n\n# Q4: Cache miss ‚Äî networking concept\nquestion4 = \"What is an Amazon VPC?\"\nanswer4 = cache.ask(question4)\nprint(answer4)\n\n# Note:\n# All four are distinct enough to each get a separate Traversaal Pro call.\n# Next, we'll test cache hits with rephrased versions of these questions."
>>>>>>> Stashed changes
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eade92a-a4f7-406f-85d3-ae24146d9c00",
   "metadata": {
<<<<<<< Updated upstream
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5eade92a-a4f7-406f-85d3-ae24146d9c00",
    "outputId": "f57cfdc7-0288-499c-d9f0-27ed2916acc3"
   },
   "outputs": [],
   "source": [
    "# Cache HIT ‚Äî rephrased version of Q1 (\"What is an S3 bucket?\")\n",
    "# The FAISS index finds the stored embedding is similar enough ‚Üí returns instantly\n",
    "print(cache.ask(\"Can you explain what Amazon S3 buckets are?\"))"
   ]
=======
    "id": "5eade92a-a4f7-406f-85d3-ae24146d9c00",
    "outputId": "f57cfdc7-0288-499c-d9f0-27ed2916acc3",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "source": "# Cache HIT ‚Äî rephrased version of Q1 (\"What is an S3 bucket?\")\n# The FAISS index finds the stored embedding is similar enough ‚Üí returns instantly\nprint(cache.ask(\"Can you explain what Amazon S3 buckets are?\"))"
>>>>>>> Stashed changes
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067af075-1df3-4fa7-90bf-52b14d819406",
   "metadata": {
<<<<<<< Updated upstream
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "067af075-1df3-4fa7-90bf-52b14d819406",
    "outputId": "4efe08d2-b888-43d4-fe2e-fbcd139e92cb"
   },
   "outputs": [],
   "source": [
    "# Cache HIT ‚Äî exact same question as Q3\n",
    "print(cache.ask(\"What is AWS IAM used for?\"))"
   ]
=======
    "id": "067af075-1df3-4fa7-90bf-52b14d819406",
    "outputId": "4efe08d2-b888-43d4-fe2e-fbcd139e92cb",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "source": "# Cache HIT ‚Äî exact same question as Q3\nprint(cache.ask(\"What is AWS IAM used for?\"))"
>>>>>>> Stashed changes
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9a6a23-83d7-4688-b037-fc015f295e83",
   "metadata": {
<<<<<<< Updated upstream
    "collapsed": true,
    "id": "7b9a6a23-83d7-4688-b037-fc015f295e83"
   },
   "outputs": [],
   "source": [
    "# Cache MISS ‚Äî new AWS concept not yet in cache ‚Üí Traversaal Pro call\n",
    "print(cache.ask(\"What is Amazon CloudFront and how does it work?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2P3Tso8TTElH",
   "metadata": {
    "collapsed": true,
    "id": "2P3Tso8TTElH"
   },
   "outputs": [],
   "source": [
    "# Cache HIT ‚Äî semantically similar to the CloudFront question above\n",
    "print(cache.ask(\"How does AWS CloudFront serve content to users?\"))"
   ]
=======
    "id": "7b9a6a23-83d7-4688-b037-fc015f295e83",
    "collapsed": true
   },
   "outputs": [],
   "source": "# Cache MISS ‚Äî new AWS concept not yet in cache ‚Üí Traversaal Pro call\nprint(cache.ask(\"What is Amazon CloudFront and how does it work?\"))"
  },
  {
   "cell_type": "code",
   "source": "# Cache HIT ‚Äî semantically similar to the CloudFront question above\nprint(cache.ask(\"How does AWS CloudFront serve content to users?\"))",
   "metadata": {
    "id": "2P3Tso8TTElH",
    "collapsed": true
   },
   "id": "2P3Tso8TTElH",
   "execution_count": null,
   "outputs": []
>>>>>>> Stashed changes
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015dd13f-9de9-409b-9273-6730fe173585",
   "metadata": {
<<<<<<< Updated upstream
    "collapsed": true,
    "id": "015dd13f-9de9-409b-9273-6730fe173585"
   },
   "outputs": [],
   "source": [
    "# Cache MISS ‚Äî new question about DynamoDB ‚Üí Traversaal Pro call + stored\n",
    "print(cache.ask(\"What is Amazon DynamoDB and when should I use it?\"))"
   ]
=======
    "id": "015dd13f-9de9-409b-9273-6730fe173585",
    "collapsed": true
   },
   "outputs": [],
   "source": "# Cache MISS ‚Äî new question about DynamoDB ‚Üí Traversaal Pro call + stored\nprint(cache.ask(\"What is Amazon DynamoDB and when should I use it?\"))"
>>>>>>> Stashed changes
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf696d0-2660-4cae-99b1-583807e7e5f1",
   "metadata": {
<<<<<<< Updated upstream
    "collapsed": true,
    "id": "2cf696d0-2660-4cae-99b1-583807e7e5f1"
   },
   "outputs": [],
   "source": [
    "# Cache HIT ‚Äî rephrased DynamoDB question ‚Üí returned from cache instantly\n",
    "print(cache.ask(\"When would I choose DynamoDB over other databases on AWS?\"))"
   ]
=======
    "id": "2cf696d0-2660-4cae-99b1-583807e7e5f1",
    "collapsed": true
   },
   "outputs": [],
   "source": "# Cache HIT ‚Äî rephrased DynamoDB question ‚Üí returned from cache instantly\nprint(cache.ask(\"When would I choose DynamoDB over other databases on AWS?\"))"
>>>>>>> Stashed changes
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e94626-1fd1-4493-8b8f-9550a1460e7a",
   "metadata": {
<<<<<<< Updated upstream
    "collapsed": true,
    "id": "b4e94626-1fd1-4493-8b8f-9550a1460e7a"
   },
   "outputs": [],
   "source": [
    "# Cache MISS ‚Äî different enough to not match DynamoDB ‚Üí Traversaal Pro call\n",
    "print(cache.ask(\"How does Amazon RDS differ from DynamoDB?\"))"
   ]
=======
    "id": "b4e94626-1fd1-4493-8b8f-9550a1460e7a",
    "collapsed": true
   },
   "outputs": [],
   "source": "# Cache MISS ‚Äî different enough to not match DynamoDB ‚Üí Traversaal Pro call\nprint(cache.ask(\"How does Amazon RDS differ from DynamoDB?\"))"
>>>>>>> Stashed changes
  },
  {
   "cell_type": "markdown",
   "id": "5ys1zxjtkc4",
<<<<<<< Updated upstream
   "metadata": {},
   "source": [
    "### Testing the Time-Sensitivity Filter + Dual-Backend Routing\n",
    "\n",
    "Here we demonstrate the full routing logic:\n",
    "\n",
    "| Question type | Detected by | Backend | Cached? |\n",
    "|---|---|---|---|\n",
    "| Contains temporal keyword | `is_time_sensitive()` ‚Üí `True` | **SerpApi** (live Google search) | ‚ùå Never |\n",
    "| Stable AWS concept | `is_time_sensitive()` ‚Üí `False` + cache miss | **Traversaal Pro** (AWS docs RAG) | ‚úÖ Stored |\n",
    "| Previously seen question | `is_time_sensitive()` ‚Üí `False` + cache hit | **FAISS cache** | ‚úÖ Already stored |\n",
    "\n",
    "**AWS-specific time-sensitive examples** ‚Äî even though they're about AWS, these need live answers:\n",
    "- *\"Are there any AWS outages right now?\"* ‚Üí changes minute to minute  \n",
    "- *\"What are the latest AWS features released this week?\"* ‚Üí new announcements daily  \n",
    "- *\"What is the current EC2 pricing today?\"* ‚Üí pricing can be updated by AWS anytime  \n",
    "\n",
    "**AWS stable examples** ‚Äî these come from documentation and don't change:\n",
    "- *\"What is an S3 bucket?\"* ‚Äî always the same concept\n",
    "- *\"How does Lambda work?\"* ‚Äî core service behaviour is stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dlfbvrouhim",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification check ‚Äî see which questions are flagged before running any queries\n",
    "time_sensitive_aws = [\n",
    "    \"Are there any AWS outages right now?\",\n",
    "    \"What are the latest AWS features released this week?\",\n",
    "    \"What is the current EC2 pricing today?\",\n",
    "    \"Is AWS S3 down right now?\",\n",
    "    \"What new services did AWS announce this month?\",\n",
    "    \"What is the current AWS free tier limit as of now?\",\n",
    "]\n",
    "\n",
    "stable_aws = [\n",
    "    \"What is an S3 bucket in AWS?\",\n",
    "    \"How does AWS Lambda work?\",\n",
    "    \"What is AWS IAM?\",\n",
    "    \"What is the difference between EC2 and ECS?\",\n",
    "    \"How does Amazon CloudFront work?\",\n",
    "    \"What is an AWS VPC?\",\n",
    "]\n",
    "\n",
    "print(\"=== Time-Sensitive AWS Questions (‚Üí SerpApi, never cached) ===\")\n",
    "for q in time_sensitive_aws:\n",
    "    flag = cache.is_time_sensitive(q)\n",
    "    label = \"‚è∞ SerpApi (live)\" if flag else \"‚úÖ Traversaal Pro (cached)\"\n",
    "    print(f\"  [{label}] {q}\")\n",
    "\n",
    "print(\"\\n=== Stable AWS Questions (‚Üí Traversaal Pro on miss, cached) ===\")\n",
    "for q in stable_aws:\n",
    "    flag = cache.is_time_sensitive(q)\n",
    "    label = \"‚è∞ SerpApi (live)\" if flag else \"‚úÖ Traversaal Pro (cached)\"\n",
    "    print(f\"  [{label}] {q}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xm8an2uz38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-sensitive AWS questions ‚Üí routed to SerpApi, NEVER stored in cache\n",
    "# Ask the same question twice ‚Äî both calls go live, nothing accumulates in FAISS\n",
    "\n",
    "print(\"--- Query A (time-sensitive: outage check) ---\")\n",
    "print(cache.ask(\"Are there any AWS outages right now?\"))\n",
    "\n",
    "print(\"\\n--- Query A again (still time-sensitive ‚Üí SerpApi again, not cached) ---\")\n",
    "print(cache.ask(\"Are there any AWS outages right now?\"))\n",
    "\n",
    "print(\"\\n--- Query B (time-sensitive: pricing) ---\")\n",
    "print(cache.ask(\"What is the current EC2 pricing today?\"))\n",
    "\n",
    "# Verify the cache count has not grown due to these time-sensitive calls\n",
    "print(f\"\\nCache entries (should be same as before): {len(cache.cache['questions'])}\")\n",
    "print(\"Cached questions:\", cache.cache['questions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "w7nq7un0l6j",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stable AWS question ‚Üí cache miss on first call (Traversaal Pro), cache hit on second\n",
    "print(\"--- Query C (stable AWS, first call ‚Üí Traversaal Pro) ---\")\n",
    "print(cache.ask(\"How do you configure S3 bucket policies?\"))\n",
    "\n",
    "print(\"\\n--- Query D (semantically similar to C ‚Üí cache hit) ---\")\n",
    "print(cache.ask(\"What is the way to set up an S3 bucket access policy?\"))\n",
    "\n",
    "print(f\"\\nTotal cached entries now: {len(cache.cache['questions'])}\")\n",
    "print(\"Cached questions:\", cache.cache['questions'])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "olive-python",
=======
   "source": "### Testing the Time-Sensitivity Filter + Dual-Backend Routing\n\nHere we demonstrate the full routing logic:\n\n| Question type | Detected by | Backend | Cached? |\n|---|---|---|---|\n| Contains temporal keyword | `is_time_sensitive()` ‚Üí `True` | **SerpApi** (live Google search) | ‚ùå Never |\n| Stable AWS concept | `is_time_sensitive()` ‚Üí `False` + cache miss | **Traversaal Pro** (AWS docs RAG) | ‚úÖ Stored |\n| Previously seen question | `is_time_sensitive()` ‚Üí `False` + cache hit | **FAISS cache** | ‚úÖ Already stored |\n\n**AWS-specific time-sensitive examples** ‚Äî even though they're about AWS, these need live answers:\n- *\"Are there any AWS outages right now?\"* ‚Üí changes minute to minute  \n- *\"What are the latest AWS features released this week?\"* ‚Üí new announcements daily  \n- *\"What is the current EC2 pricing today?\"* ‚Üí pricing can be updated by AWS anytime  \n\n**AWS stable examples** ‚Äî these come from documentation and don't change:\n- *\"What is an S3 bucket?\"* ‚Äî always the same concept\n- *\"How does Lambda work?\"* ‚Äî core service behaviour is stable",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "dlfbvrouhim",
   "source": "# Classification check ‚Äî see which questions are flagged before running any queries\ntime_sensitive_aws = [\n    \"Are there any AWS outages right now?\",\n    \"What are the latest AWS features released this week?\",\n    \"What is the current EC2 pricing today?\",\n    \"Is AWS S3 down right now?\",\n    \"What new services did AWS announce this month?\",\n    \"What is the current AWS free tier limit as of now?\",\n]\n\nstable_aws = [\n    \"What is an S3 bucket in AWS?\",\n    \"How does AWS Lambda work?\",\n    \"What is AWS IAM?\",\n    \"What is the difference between EC2 and ECS?\",\n    \"How does Amazon CloudFront work?\",\n    \"What is an AWS VPC?\",\n]\n\nprint(\"=== Time-Sensitive AWS Questions (‚Üí SerpApi, never cached) ===\")\nfor q in time_sensitive_aws:\n    flag = cache.is_time_sensitive(q)\n    label = \"‚è∞ SerpApi (live)\" if flag else \"‚úÖ Traversaal Pro (cached)\"\n    print(f\"  [{label}] {q}\")\n\nprint(\"\\n=== Stable AWS Questions (‚Üí Traversaal Pro on miss, cached) ===\")\nfor q in stable_aws:\n    flag = cache.is_time_sensitive(q)\n    label = \"‚è∞ SerpApi (live)\" if flag else \"‚úÖ Traversaal Pro (cached)\"\n    print(f\"  [{label}] {q}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "xm8an2uz38",
   "source": "# Time-sensitive AWS questions ‚Üí routed to SerpApi, NEVER stored in cache\n# Ask the same question twice ‚Äî both calls go live, nothing accumulates in FAISS\n\nprint(\"--- Query A (time-sensitive: outage check) ---\")\nprint(cache.ask(\"Are there any AWS outages right now?\"))\n\nprint(\"\\n--- Query A again (still time-sensitive ‚Üí SerpApi again, not cached) ---\")\nprint(cache.ask(\"Are there any AWS outages right now?\"))\n\nprint(\"\\n--- Query B (time-sensitive: pricing) ---\")\nprint(cache.ask(\"What is the current EC2 pricing today?\"))\n\n# Verify the cache count has not grown due to these time-sensitive calls\nprint(f\"\\nCache entries (should be same as before): {len(cache.cache['questions'])}\")\nprint(\"Cached questions:\", cache.cache['questions'])",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "w7nq7un0l6j",
   "source": "# Stable AWS question ‚Üí cache miss on first call (Traversaal Pro), cache hit on second\nprint(\"--- Query C (stable AWS, first call ‚Üí Traversaal Pro) ---\")\nprint(cache.ask(\"How do you configure S3 bucket policies?\"))\n\nprint(\"\\n--- Query D (semantically similar to C ‚Üí cache hit) ---\")\nprint(cache.ask(\"What is the way to set up an S3 bucket access policy?\"))\n\nprint(f\"\\nTotal cached entries now: {len(cache.cache['questions'])}\")\nprint(\"Cached questions:\", cache.cache['questions'])",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
>>>>>>> Stashed changes
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< Updated upstream
   "version": "3.14.2"
=======
   "version": "3.11.7"
  },
  "colab": {
   "provenance": [],
   "include_colab_link": true
>>>>>>> Stashed changes
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
<<<<<<< Updated upstream
}
=======
}
>>>>>>> Stashed changes
