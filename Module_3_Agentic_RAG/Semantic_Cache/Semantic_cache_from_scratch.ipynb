{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hamzafarooq/multi-agent-course/blob/main/Module_3_Agentic_RAG/Semantic_Cache/Semantic_cache_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "p2KzXaJYks8S",
      "metadata": {
        "id": "p2KzXaJYks8S"
      },
      "source": [
        "**If you use our code, please cite:**\n",
        "\n",
        "@misc{2024<br>\n",
        "  title = {Semantic Cache from Scratch},<br>\n",
        "  author = {Hamza Farooq, Darshil Modi, Kanwal Mehreen, Nazila Shafiei},<br>\n",
        "  keywords = {Semantic Cache},<br>\n",
        "  year = {2024},<br>\n",
        "  copyright = {APACHE 2.0 license}<br>\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gxQxLQJymkCl",
      "metadata": {
        "id": "gxQxLQJymkCl"
      },
      "source": [
        "## Semantic Cache\n",
        "\n",
        "Semantic caching accelerates retrieval-augmented workflows by storing and reusing previous embedding-based lookups instead of issuing fresh queries every time. In this notebook, we’ll build a lightweight semantic cache from scratch using:\n",
        "\n",
        "- **Nomic text embeddings** (`nomic-ai/nomic-embed-text-v1.5`) to convert documents and queries into dense vectors  \n",
        "- **FAISS** (Facebook AI Similarity Search) to index and quickly search those vectors  \n",
        "- A **ground-truth evaluation** dataset to measure cache hit/miss accuracy  \n",
        "- **Traversaal Ares API** to fetch live data when cache misses require real-time information  \n",
        "\n",
        "Rather than re-computing embeddings and retrieval for every query, our cache lets us:\n",
        "\n",
        "1. **Embed** a corpus once and index it for fast L2 nearest-neighbor lookup  \n",
        "2. **Embed** each new query and check if it’s already “covered” by a cached result  \n",
        "3. **Fall back** to a full retrieval (and store the new result) only when necessary  \n",
        "4. **Invoke** the Traversaal Ares API for live internet search when needed  \n",
        "\n",
        "This approach reduces redundant compute, lowers end-to-end latency, and makes RAG pipelines more efficient—especially when query patterns exhibit repetition, temporal locality, or high similarity. We’ll walk through:\n",
        "\n",
        "1. Loading the Nomic embed model with `trust_remote_code=True`  \n",
        "2. Encoding a document set and building a FAISS index  \n",
        "3. Loading a real-world ground-truth CSV for evaluation  \n",
        "4. Implementing the core cache hit/miss logic  \n",
        "5. Falling back to Traversaal Ares API for live data on cache misses  \n",
        "6. Measuring performance gains against a “no-cache” baseline  \n",
        "\n",
        "By the end, you’ll have a reusable semantic cache scaffold that you can plug into any RAG or search-over-embeddings pipeline. Let’s get started!  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Hm2NYbYQmykl",
      "metadata": {
        "id": "Hm2NYbYQmykl"
      },
      "source": [
        "## Setup and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "025_hZMnZUIE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "025_hZMnZUIE",
        "outputId": "0b61c55b-c058-4a2c-cc1d-095320fe263b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.13.0-cp39-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.12/dist-packages (5.1.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (2.9.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (1.16.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (0.36.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (3.5.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence_transformers) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence_transformers) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.3)\n",
            "Downloading faiss_cpu-1.13.0-cp39-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (23.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.6/23.6 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.13.0\n"
          ]
        }
      ],
      "source": [
        "# Install the necessary libraries\n",
        "!pip install -U faiss-cpu sentence_transformers transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "52273bc0-575b-4007-b63d-bfe53d4abde6",
      "metadata": {
        "id": "52273bc0-575b-4007-b63d-bfe53d4abde6"
      },
      "outputs": [],
      "source": [
        "# Import the necessary libraries\n",
        "\n",
        "# FAISS for efficient similarity search over vector embeddings\n",
        "import faiss  # Builds and queries approximate nearest neighbor indices\n",
        "\n",
        "# Lightweight SQL database for caching metadata, query logs, or evaluation results\n",
        "import sqlite3  # Persistence layer for storing cache entries or metrics\n",
        "\n",
        "# SentenceTransformers wrapper around transformer models for text embeddings\n",
        "from sentence_transformers import SentenceTransformer  # Loads Nomic/embed or other SBERT-style models\n",
        "\n",
        "# PyTorch backend required by SentenceTransformer and optional model fine-tuning\n",
        "import torch  # Tensor operations, GPU acceleration, and model inference support\n",
        "\n",
        "# Transformers library components for causal LLM-based answer generation\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "#   - AutoModelForCausalLM: Load pretrained language models (e.g., GPT variants)\n",
        "#   - AutoTokenizer: Tokenize text input/output for the LLM\n",
        "\n",
        "# Core numerical library for array and matrix operations on embeddings\n",
        "import numpy as np  # Handles vector math, concatenation, and statistical computations\n",
        "\n",
        "# Pretty-printing complex Python objects during development/debugging\n",
        "from pprint import pprint  # Nicely formats nested dicts or lists when exploring outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4JHRRiuKlM3t",
      "metadata": {
        "id": "4JHRRiuKlM3t"
      },
      "source": [
        "\n",
        "# Define prediction function (Using Traversaal Ares API)\n",
        "\n",
        "Instead of using an LLM endpoint, we will be using Ares API for retrieval and generation, however you can replace is with your own rag function in 'generate answer' function\n",
        "\n",
        "Traversaal Ares API is a cutting-edge solution designed to provide real-time search results generated from user queries. Leveraging advanced Large Language Models (LLMs), Ares connects to the internet to deliver accurate and factual information, including relevant URLs for reference. This API is tailored for speed and efficiency, providing lightning-fast search results within 3-4 seconds. Currently available for free during the beta phase, with priced solutions coming soon.\n",
        "\n",
        "## Key Features:\n",
        "- **Real-time Search Results:** Ares API offers unparalleled speed in generating search results.\n",
        "- **Internet Connectivity:** Connects to the internet to fetch the latest and most accurate information.\n",
        "- **Lightning-Fast Response:** Delivers search results with URLs in 3-4 seconds.\n",
        "- **Free Beta Access:** Available for free during for the first 100 calls\n",
        "- **Factual and Accurate:** Ensures the information provided is accurate and supported by relevant references. [Can make mistakes though]\n",
        "\n",
        "## Getting Started:\n",
        "To access the Ares API, sign up at [api.traversaal.ai](https://api.traversaal.ai) and refer to the usage documentation at [docs.traversaal.ai](https://docs.traversaal.ai/docs/intro).\n",
        "\n",
        "Experience the future of AI-driven search with Traversaal Ares API!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "38331891-adb4-4d16-b26f-d74d7c9ce728",
      "metadata": {
        "id": "38331891-adb4-4d16-b26f-d74d7c9ce728"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata # Colab utility for securely storing/retrieving credentials\n",
        "import requests # HTTP client for REST API calls\n",
        "\n",
        "def make_prediction(data):\n",
        "    \"\"\"\n",
        "    Send a text query to the Traversaal Ares live-predict endpoint and return parsed JSON.\n",
        "\n",
        "    Args:\n",
        "        data (str): The user query string to send for prediction.\n",
        "\n",
        "    Returns:\n",
        "        dict or None: Parsed JSON response from Ares API if successful, else None.\n",
        "\n",
        "    Raises:\n",
        "        RuntimeError: If ARES_API_KEY is missing.\n",
        "    \"\"\"\n",
        "    url = \"https://api-ares.traversaal.ai/live/predict\"\n",
        "    # Retrieve API key from Colab secure storage; fail fast if missing\n",
        "    api_key = userdata.get(\"ARES_API_KEY\")\n",
        "    if not api_key:\n",
        "        raise RuntimeError(\"Missing ARES_API_KEY in Colab userdata\")\n",
        "    headers = {\n",
        "        \"x-api-key\": api_key,\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    # The ARES API expects a JSON body of the form {\"query\": <text>}\n",
        "    payload = {\"query\": data}\n",
        "\n",
        "    try:\n",
        "        response = requests.post(url, json=payload, headers=headers)\n",
        "        # Check for HTTP 200 OK\n",
        "        if response.status_code == 200:\n",
        "            print(\"Request was successful.\")\n",
        "            try:\n",
        "                 # If the response contains JSON data, you can parse it using response.json()\n",
        "                return response.json()\n",
        "            except ValueError:\n",
        "                # Response text wasn’t valid JSON\n",
        "                print(\"No JSON data in the response.\")\n",
        "                return None\n",
        "        else:\n",
        "            # Non-200 status; surface the code for debugging\n",
        "            print(f\"Request failed with status code {response.status_code}.\")\n",
        "            return None\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error during request: {e}\")\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "QC027Sholey1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QC027Sholey1",
        "outputId": "8b479bc0-3861-4dbb-b389-b964df66a8bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Request was successful.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'data': {'response_text': \"Events happening in London this week:\\n\\n1. Bonfire Night Fireworks - November 5, 2025, at various locations in London.\\n2. Ice Skating in London - 1 - 30 November 2025, at various locations in London, including:\\n   - Somerset House Ice Rink, Strand, London WC2R 1LA\\n   - Natural History Museum Ice Rink, Cromwell Road, London SW7 5BD\\n   - Tower of London Ice Rink, Tower Hill, London EC3N 4AB\\n3. Regent Street Motor Show - November 1, 2025, on Regent Street, London W1B 5AT.\\n4. Winter at the Southbank Centre - November 2025, at the Southbank Centre, Belvedere Road, London SE1 8XX.\\n5. EFG London Jazz Festival - November 2025, at various locations in London, including:\\n   - Royal Festival Hall, Southbank Centre, Belvedere Road, London SE1 8XX\\n   - Barbican Centre, Silk Street, London EC2Y 8DS\\n6. Christmas at Kew - November 2025, at Kew Gardens, Richmond TW9 3AB.\\n7. Hogwarts in the Snow - November 2025, at the Warner Bros. Studio Tour London, Studio Tour Drive, Leavesden WD25 7LR.\\n8. Lord Mayor's Show - November 2025, on the River Thames, London EC4M 9BU.\\n9. Remembrance Day - November 2025, at the Cenotaph, Whitehall, London SW1A 2BJ.\\n10. Winter Wonderland - November 2025, at Hyde Park, London W2 2UH.\\n11. Christmas lights and festive markets - November 2025, at various locations in London, including:\\n    - Oxford Street, London W1C 1JH\\n    - Regent Street, London W1B 5AT\\n    - Covent Garden, London WC2E 8RD\",\n",
              "  'web_url': ['https://www.visitlondon.com/things-to-do/whats-on/special-events/london-events-calendar',\n",
              "   'https://www.londonperfect.com/plan-your-trip/events-in-london/november.php',\n",
              "   'https://www.timeout.com/london/things-to-do/london-events-in-november',\n",
              "   'https://londonist.com/london/weekend/things-to-do-in-london-this-weekend-22-23-november-2025',\n",
              "   'https://londoncheapo.com/events/november/',\n",
              "   'https://www.touristengland.com/events/events-in-london/things-to-do-in-london-november/',\n",
              "   'https://www.londontheatre.co.uk/whats-on/nov',\n",
              "   'https://www.london-tickets.co.uk/travel-guide/best-time-to-visit/london-in-november/',\n",
              "   'https://www.songkick.com/metro-areas/24426-uk-london',\n",
              "   'https://www.visitbritain.com/en/annual-events-britain']}}"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Test the make_prediction function\n",
        "response=make_prediction('Events happening in London this week. ')\n",
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db-RrmtuqFhC",
      "metadata": {
        "id": "db-RrmtuqFhC"
      },
      "source": [
        "### Define SemanticCaching Class\n",
        "\n",
        "In this cell we define `SemanticCaching`—a lightweight cache that:\n",
        "\n",
        "1. Uses **FAISS** to index and lookup question embeddings (Euclidean distance).  \n",
        "2. Leverages **Nomic Embed** (`nomic-ai/nomic-embed-text-v1.5`) to encode questions into vectors.  \n",
        "3. Persists cache entries (questions, embeddings, answers) in a JSON file.  \n",
        "4. Falls back to `make_prediction()` (via Traversaal Ares API) when no suitable cache hit is found.  \n",
        "5. Measures and logs query latency for both hits and misses.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "yDHhY-OBSEIw",
      "metadata": {
        "id": "yDHhY-OBSEIw"
      },
      "outputs": [],
      "source": [
        "import faiss            # Efficient similarity search over vector embeddings\n",
        "import json             # Read/write cache from a JSON file\n",
        "import numpy as np      # Numerical operations on embeddings\n",
        "from sentence_transformers import SentenceTransformer  # Load Nomic embed model\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM  # (Optional) LLM for answer gen\n",
        "import time             # Measure latency\n",
        "\n",
        "class SemanticCaching:\n",
        "\n",
        "    def __init__(self, json_file='cache.json', clear_on_init=False):\n",
        "        # Initialize Faiss index with Euclidean distance\n",
        "        self.index = faiss.IndexFlatL2(768)  # Use IndexFlatL2 with Euclidean distance\n",
        "        if self.index.is_trained:\n",
        "            print('Index trained')\n",
        "\n",
        "        # Initialize Sentence Transformer model\n",
        "        self.encoder = SentenceTransformer('nomic-ai/nomic-embed-text-v1.5', trust_remote_code=True)\n",
        "\n",
        "\n",
        "        # Uncomment the following lines to use DialoGPT for question generation\n",
        "        # self.tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-large\")\n",
        "        # self.model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-large\")\n",
        "\n",
        "        # Euclidean distance threshold for cache hits (lower = more similar)\n",
        "        self.euclidean_threshold = 0.2\n",
        "\n",
        "        # JSON file to persist cache entries\n",
        "        self.json_file = json_file\n",
        "\n",
        "        # Load cache or clear already loaded cache\n",
        "        if clear_on_init:\n",
        "          self.clear_cache()\n",
        "        else:\n",
        "          self.load_cache()\n",
        "\n",
        "    def clear_cache(self):\n",
        "        \"\"\"\n",
        "        Clears in-memory cache, resets FAISS index, and overwrites cache.json with an empty structure.\n",
        "        \"\"\"\n",
        "        self.cache = {\n",
        "            'questions': [],\n",
        "            'embeddings': [],\n",
        "            'answers': [],\n",
        "            'response_text': []\n",
        "        }\n",
        "        self.index = faiss.IndexFlatL2(768)  # Reinitialize FAISS index\n",
        "        self.save_cache()\n",
        "        print(\"Semantic cache cleared.\")\n",
        "\n",
        "    def load_cache(self):\n",
        "        \"\"\"Load existing cache or initialize empty structure.\"\"\"\n",
        "        try:\n",
        "            with open(self.json_file, 'r') as file:\n",
        "                self.cache = json.load(file)\n",
        "        except FileNotFoundError:\n",
        "          # Structure: lists of questions, embeddings, answers, and full response text\n",
        "            self.cache = {'questions': [], 'embeddings': [], 'answers': [], 'response_text': []}\n",
        "\n",
        "    def save_cache(self):\n",
        "        \"\"\"Persist cache back to disk.\"\"\"\n",
        "        with open(self.json_file, 'w') as file:\n",
        "            json.dump(self.cache, file)\n",
        "\n",
        "    def ask(self, question: str) -> str:\n",
        "        \"\"\"\n",
        "        Returns a cached answer if within threshold, otherwise generates,\n",
        "        caches, and returns a new answer.\n",
        "        \"\"\"\n",
        "        start_time = time.time()\n",
        "        try:\n",
        "            # Encode the incoming question\n",
        "            l = [question]\n",
        "            # embedding = self.encoder.encode(l)\n",
        "            embedding = self.encoder.encode(l, normalize_embeddings=True)\n",
        "\n",
        "            # Search for the nearest neighbor in the index\n",
        "            D, I = self.index.search(embedding, 1)\n",
        "\n",
        "            # 3) If a neighbor exists and is within threshold → cache hit\n",
        "            if D[0] >= 0:\n",
        "                if I[0][0] != -1 and D[0][0] <= self.euclidean_threshold:\n",
        "                    row_id = int(I[0][0])\n",
        "                    print(f'Cache hit at row: {row_id} with score {1 - D[0][0]}') #score inversed to show similarity\n",
        "                    print(f\"Time taken: {time.time() - start_time:.3f}s\")\n",
        "                    return self.cache['response_text'][row_id]\n",
        "\n",
        "            # Handle the case when there are not enough results or Euclidean distance is not met\n",
        "            answer, response_text = self.generate_answer(question)\n",
        "\n",
        "            # Append new entry to cache\n",
        "            self.cache['questions'].append(question)\n",
        "            self.cache['embeddings'].append(embedding[0].tolist())\n",
        "            self.cache['answers'].append(answer)\n",
        "            self.cache['response_text'].append(response_text)\n",
        "            self.index.add(embedding)\n",
        "            self.save_cache()\n",
        "            print(f\"Time taken: {time.time() - start_time:.3f}s\")\n",
        "\n",
        "            return response_text\n",
        "\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Error during 'ask' method: {e}\")\n",
        "\n",
        "    def generate_answer(self, question: str) -> str:\n",
        "        \"\"\"\n",
        "        Always use the Traversaal Ares API for new answers.\n",
        "        Returns (full API result dict, extracted response_text).\n",
        "        \"\"\"\n",
        "        try:\n",
        "            result = make_prediction(question)\n",
        "            response_text = result['data']['response_text']\n",
        "\n",
        "            return result, response_text\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Error during 'generate_answer' method: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "dc661dab-f7cc-4d74-9575-1c756b4cdef0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc661dab-f7cc-4d74-9575-1c756b4cdef0",
        "outputId": "c55b9d0c-69f3-42ac-81f9-5b014a3b7415"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index trained\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:transformers_modules.nomic_hyphen_ai.nomic_hyphen_bert_hyphen_2048.7710840340a098cfb869c4f65e87cf2b1b70caca.modeling_hf_nomic_bert:<All keys matched successfully>\n"
          ]
        }
      ],
      "source": [
        "# Instantiate the semantic cache: builds/loads FAISS index, encoder, and JSON cache\n",
        "cache = SemanticCaching()\n",
        "\n",
        "# Uncomment and use to re-instantiate the semantic cache and clear exisitng cache entries\n",
        "# cache = SemanticCaching(clear_on_init=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "noc3G8Abxy2t",
      "metadata": {
        "id": "noc3G8Abxy2t"
      },
      "source": [
        "### Testing the Semantic Cache\n",
        "\n",
        "In this section, we validate the behavior of our `SemanticCaching` class using a small set of example questions. We’ll loop through three queries:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "2f64fe4d-fe89-44a7-bf3f-3ad721985f3e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2f64fe4d-fe89-44a7-bf3f-3ad721985f3e",
        "outputId": "90ecbeb7-fd6d-4222-86ab-e472e0dbadf7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Request was successful.\n",
            "Time taken: 1.993s\n",
            "The capital of France is Paris.\n",
            "Request was successful.\n",
            "Time taken: 1.893s\n",
            "Who is the CEO of Apple?\n",
            "\n",
            "The current CEO of Apple is Tim Cook.\n",
            "Request was successful.\n",
            "Time taken: 2.025s\n",
            "The CEO of Facebook is Mark Zuckerberg.\n",
            "Request was successful.\n",
            "Time taken: 2.054s\n",
            "The capital of India is New Delhi.\n"
          ]
        }
      ],
      "source": [
        "# First test question\n",
        "question1 = \"What is the capital of France?\"\n",
        "answer1 = cache.ask(question1)  # Cache miss: generates new answer via API and stores it\n",
        "print(answer1)\n",
        "\n",
        "# Second test question\n",
        "question2 = \"Who is the CEO of Apple?\"\n",
        "answer2 = cache.ask(question2)  # Cache miss: generates answer, stores embedding + response\n",
        "print(answer2)\n",
        "\n",
        "# Third test question\n",
        "question3 = \"Who is the CEO of Facebook?\"\n",
        "answer3 = cache.ask(question3)  # Cache hit or miss depends on similarity threshold\n",
        "print(answer3)\n",
        "\n",
        "# Fourth test question\n",
        "question4 = \"What is the capital of India?\"\n",
        "answer4 = cache.ask(question4)\n",
        "print(answer4)\n",
        "\n",
        "# Note:\n",
        "# If question3 is found similar enough (within threshold) to question2, it returns cached answer2.\n",
        "# Otherwise, it generates a fresh answer and adds it to the cache."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "5eade92a-a4f7-406f-85d3-ae24146d9c00",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5eade92a-a4f7-406f-85d3-ae24146d9c00",
        "outputId": "f57cfdc7-0288-499c-d9f0-27ed2916acc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cache hit at row: 3 with score 0.93044114112854\n",
            "Time taken: 0.117s\n",
            "The capital of India is New Delhi.\n"
          ]
        }
      ],
      "source": [
        "print(cache.ask(\"Can you tell me what is the Capital of India\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "067af075-1df3-4fa7-90bf-52b14d819406",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "067af075-1df3-4fa7-90bf-52b14d819406",
        "outputId": "4efe08d2-b888-43d4-fe2e-fbcd139e92cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cache hit at row: 2 with score 1.0\n",
            "Time taken: 0.162s\n",
            "The CEO of Facebook is Mark Zuckerberg.\n"
          ]
        }
      ],
      "source": [
        "print(cache.ask('Who is the CEO of Facebook?'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b9a6a23-83d7-4688-b037-fc015f295e83",
      "metadata": {
        "collapsed": true,
        "id": "7b9a6a23-83d7-4688-b037-fc015f295e83"
      },
      "outputs": [],
      "source": [
        "print(cache.ask('Who is the current CEO of Google?'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2P3Tso8TTElH",
      "metadata": {
        "collapsed": true,
        "id": "2P3Tso8TTElH"
      },
      "outputs": [],
      "source": [
        "print(cache.ask('Is Sundar Pichai the CEO of Google?'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "015dd13f-9de9-409b-9273-6730fe173585",
      "metadata": {
        "collapsed": true,
        "id": "015dd13f-9de9-409b-9273-6730fe173585"
      },
      "outputs": [],
      "source": [
        "print(cache.ask('Best local food spots in Edinburgh for a couple?'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cf696d0-2660-4cae-99b1-583807e7e5f1",
      "metadata": {
        "collapsed": true,
        "id": "2cf696d0-2660-4cae-99b1-583807e7e5f1"
      },
      "outputs": [],
      "source": [
        "print(cache.ask('Best local food spots in Edinburgh?'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4e94626-1fd1-4493-8b8f-9550a1460e7a",
      "metadata": {
        "collapsed": true,
        "id": "b4e94626-1fd1-4493-8b8f-9550a1460e7a"
      },
      "outputs": [],
      "source": [
        "print(cache.ask('Best local food spots in London?'))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
