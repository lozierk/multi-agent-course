{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hamzafarooq/multi-agent-course/blob/main/Module_4/Knowledge_Graphs_Advanced_Version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "In the [basic notebook for Module 4](https://colab.research.google.com/github/hamzafarooq/multi-agent-course/blob/main/Module_4/Knowledge_Graphs_Basic_Version.ipynb) of [Enterprise RAG and Multi-Agent Applications](https://maven.com/boring-bot/advanced-llm), we explored the fundamentals of knowledge graph construction and Graph RAG. We built a hotel reviews knowledge graph, migrated it to Neo4j, and implemented a template-based retriever for answering natural language queries about our graph data.\n",
        "\n",
        "This advanced notebook builds on those foundations to explore more sophisticated Graph RAG techniques and hybrid approaches. While our basic notebook focused on core concepts and implementation, this notebook delves into methods that significantly enhance both the graph itself and our retrieval capabilities - including the pattern most frequently discussed in modern Graph RAG demonstrations, the LLM-driven extraction of structured triplets from unstructured text.\n",
        "\n"
      ],
      "metadata": {
        "id": "iXmdiUY7NlQA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Architecture Overview\n",
        "The key enhancements we'll cover in this notebook include:\n",
        "\n",
        "1. **Graph Enrichment**: Using LLMs to extract entities and relationships from unstructured text fields, expanding our knowledge graph beyond the structured data\n",
        "   \n",
        "2. **Vector Indexing**: Adding semantic search capabilities to our graph nodes, enabling similarity-based retrieval alongside structural queries\n",
        "   \n",
        "3. **Advanced Retrieval**: Implementing and comparing Text2Cypher, template-based, and vector retrieval\n",
        "   \n",
        "4. **Performance Analysis**: Systematically comparing different RAG strategies to understand when graph-based approaches outperform traditional vector RAG\n"
      ],
      "metadata": {
        "id": "0u7og19aUaKX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prerequisites\n",
        "\n",
        "* Completion of the basic Knowledge Graph RAG notebook\n",
        "* Access to the same Neo4j database used in the basic notebook\n",
        "* OpenAI API key"
      ],
      "metadata": {
        "id": "Ok5THI2zioaM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup\n",
        "\n",
        "Let's begin by setting up our connections and exploring how to enhance our knowledge graph with entities extracted from unstructured text.\n",
        "\n",
        "We will use the same Neo4j database instance that ingested our data in the basic notebook. Make sure that you have entered your NEO4J_URI and NEO4J_PASSWORD key-value pairs into your Colab Secrets before continuing."
      ],
      "metadata": {
        "id": "0G_POa8_yntz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install pyvis IPython cchardet datasets==2.16.0 langchain neo4j openai tiktoken langchain-community langchain-experimental json-repair\n",
        "\n",
        "from getpass import getpass\n",
        "import os\n",
        "from google.colab import userdata\n",
        "import json\n",
        "import pandas as pd\n",
        "from typing import Optional, List, Dict, Any\n",
        "from openai import OpenAI\n",
        "from neo4j import GraphDatabase\n",
        "from dataclasses import dataclass\n",
        "\n",
        "# Configure OpenAI API key\n",
        "if os.getenv(\"OPENAI_API_KEY\") is None:\n",
        "  try:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
        "  except (userdata.TimeoutException, userdata.SecretNotFoundError):\n",
        "    if any(['VSCODE' in x for x in os.environ.keys()]):\n",
        "      print('Please enter password in the VS Code prompt at the top of your VS Code window!')\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"\")\n",
        "\n",
        "assert os.getenv(\"OPENAI_API_KEY\", \"\").startswith(\"sk-\"), \"This doesn't look like a valid OpenAI API key\"\n",
        "print(\"OpenAI API key configured\")\n",
        "\n",
        "# Connect to Neo4j\n",
        "url = userdata.get('NEO4J_URI')\n",
        "username = \"neo4j\"\n",
        "password = userdata.get('NEO4J_PASSWORD')\n",
        "\n",
        "driver = GraphDatabase.driver(url, auth=(username, password))\n",
        "print(\"Connected to Neo4j database\")\n",
        "\n",
        "# Initialize OpenAI client\n",
        "openai_client = OpenAI()"
      ],
      "metadata": {
        "id": "ogboTato5Vq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Expanding the graph from unstructured text fields with LLMs and pre-defined schema types"
      ],
      "metadata": {
        "id": "5TAKgnG_fAnm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Graph RAG really shines when you have large amounts of unstructured text data which contain many and complex relationships. In fact, most publicly available examples and demos you see for Graph RAG involve unstructured text as the primary data sources. In our case, with the structured dataset of hotel reviews, we can turn to the hotel descriptions for such data. Let's now look at how to build a pipeline for entity and relationship extraction from unstructured text."
      ],
      "metadata": {
        "id": "pRk2Oktcbsac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional, List, Dict, Any\n",
        "from openai import OpenAI\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "class Neo4jGraphExtractor:\n",
        "    def __init__(self, openai_client: OpenAI, neo4j_driver: GraphDatabase, entity_types: List[str], rel_types: List[str], instruct_notes: List[str] = None, temperature: float = 0 ):\n",
        "        self.client = openai_client\n",
        "        self.temperature = temperature\n",
        "        self.driver = driver\n",
        "        self.entity_types = entity_types\n",
        "        self.rel_types = rel_types\n",
        "        self.instruct_notes = instruct_notes\n",
        "\n",
        "    def _create_prompt(self, hotel_name: str, text: str) -> str:\n",
        "        return f\"\"\"\n",
        "-Goal-\n",
        "Given a text document that contains the description of a specific hotel and a list of entity types, identify all entities of those types from the text and all relationships among the identified entities.\n",
        "\n",
        "-Steps-\n",
        "1. Identify all entities. There is a known root entity, which is the described hotel. For each identified entity, extract the following information:\n",
        "- entity_name: Name of the entity, capitalized\n",
        "- entity_type: One of the following types: [{self.entity_types}]\n",
        "- entity_description: Comprehensive description of the entity's attributes and activities\n",
        "\n",
        "2. From the entities identified in step 1, identify all pairs of (source_entity, target_entity) that are clearly related to each other.\n",
        "\n",
        "For each pair of related entities, extract:\n",
        "- source_entity: name of the source entity\n",
        "- target_entity: name of the target entity\n",
        "- relationship_type: One of the following types: {self.rel_types}\n",
        "\n",
        "3. Return output as a single JSON list containing all entities and relationships.\n",
        "\n",
        "-Real Data-\n",
        "######################\n",
        "Root hotel: {hotel_name}\n",
        "text: {text}\n",
        "######################\n",
        "output:\n",
        "\"\"\"\n",
        "\n",
        "    def _parse_llm_response(self, response: str) -> Dict[str, List[Dict[str, Any]]]:\n",
        "      try:\n",
        "          # Parse JSON\n",
        "          try:\n",
        "              data = json.loads(response)\n",
        "\n",
        "              # Handle both direct entity/relationship format and nested format\n",
        "              if isinstance(data, dict) and 'entities' in data and 'relationships' in data:\n",
        "                  return {\n",
        "                      \"entities\": data['entities'],\n",
        "                      \"relationships\": data['relationships']\n",
        "                  }\n",
        "              if isinstance(data, list) and len(data) > 0:\n",
        "                  # Original format handling\n",
        "                  print(\"List format received\")\n",
        "                  entities = [item for item in data if \"type\" in item]\n",
        "                  relationships = [item for item in data if \"relationship\" in item]\n",
        "                  return {\n",
        "                      \"entities\": entities,\n",
        "                      \"relationships\": relationships\n",
        "                  }\n",
        "              # Fallback if data is not in any expected shape\n",
        "              return {\"entities\": [], \"relationships\": []}\n",
        "\n",
        "          except json.JSONDecodeError as e:\n",
        "              # If standard format fails, try the alternative format\n",
        "              print(f\"JSONDecodeError thrown in inner block: {str(e)}\")\n",
        "              if str(e).startswith(\"Extra data\"):\n",
        "                try:\n",
        "                  # Assuming the format is [entities_list, relationships_list]\n",
        "                  wrapped = f\"[{response}]\"\n",
        "                  arr = json.loads(wrapped)\n",
        "                  if len(arr) == 2:\n",
        "                    entities_list, relationships_list = arr\n",
        "                    return {\n",
        "                        \"entities\": entities_list,\n",
        "                        \"relationships\": relationships_list\n",
        "                    }\n",
        "                except json.JSONDecodeError:\n",
        "                  raise e\n",
        "              else:\n",
        "                  # If neither format works, raise the original error\n",
        "                  raise e\n",
        "      except json.JSONDecodeError as e:\n",
        "          print(f\"Error parsing JSON response: {str(e)}\")\n",
        "          print(\"Response was:\", response)\n",
        "          return {\"entities\": [], \"relationships\": []}\n",
        "\n",
        "    def process_text(self, hotel_name: str, text: str) -> Dict[str, List[Dict[str, Any]]]:\n",
        "        \"\"\"\n",
        "        Process a hotel description text and extract entities and relationships.\n",
        "\n",
        "        Args:\n",
        "            hotel_name: Name of the hotel (root entity)\n",
        "            text: Hotel description text to process\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing lists of extracted entities and relationships\n",
        "        \"\"\"\n",
        "        prompt = self._create_prompt(hotel_name, text)\n",
        "\n",
        "        response = self.client.chat.completions.create(\n",
        "            model=\"gpt-4o\",  # or your preferred model\n",
        "            temperature=self.temperature,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that extracts entities and relationships from text and returns them in JSON format.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            response_format={\"type\": \"json_object\"}\n",
        "        )\n",
        "\n",
        "        # Extract the content from the response\n",
        "        result = response.choices[0].message.content\n",
        "\n",
        "        # Parse and return the structured data\n",
        "        return self._parse_llm_response(result)\n",
        "\n",
        "    def process_and_save(self, hotel_name: str, text: str) -> Dict[str, List[Dict[str, Any]]]:\n",
        "        \"\"\"\n",
        "        Process hotel description text, extract entities and relationships, and save to Neo4j.\n",
        "\n",
        "        Args:\n",
        "            hotel_name: Name of the hotel (root entity)\n",
        "            text: Hotel description text to process\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing lists of extracted entities and relationships\n",
        "        \"\"\"\n",
        "        try:\n",
        "            result = self.process_text(hotel_name, text)\n",
        "\n",
        "            if result == {\"entities\": [], \"relationships\": []}:\n",
        "                print(f\"No entities or relationships found for {hotel_name}, skipping upload\")\n",
        "                return result\n",
        "            print(f\"Processing and saving result {result}\")\n",
        "\n",
        "            # Save to Neo4j within a single session\n",
        "            with self.driver.session() as session:\n",
        "                # Add entities\n",
        "                for entity in result['entities']:\n",
        "                    cypher_query = \"\"\"\n",
        "                    MERGE (n:__Entity__ {name: $name})\n",
        "                    SET n.entity = $type,\n",
        "                        n.description = $description\n",
        "                    WITH n\n",
        "                    CALL apoc.create.addLabels(n, [$type]) YIELD node\n",
        "                    RETURN distinct 'done' AS result\n",
        "                    \"\"\"\n",
        "                    session.run(\n",
        "                        cypher_query,\n",
        "                        name=entity['entity_name'],\n",
        "                        type=entity['entity_type'].upper(),\n",
        "                        description=entity['entity_description']\n",
        "                    )\n",
        "\n",
        "                # Add relationships\n",
        "                for rel in result['relationships']:\n",
        "                    # Format the relationship type directly into the query\n",
        "                    cypher_query = f\"\"\"\n",
        "                    MATCH (a:__Entity__), (b:__Entity__)\n",
        "                    WHERE a.name = $source AND b.name = $target\n",
        "                    MERGE (a)-[r:{rel['relationship_type']}]->(b)\n",
        "                    RETURN distinct 'done' AS result\n",
        "                    \"\"\"\n",
        "                    session.run(\n",
        "                        cypher_query,\n",
        "                        source=rel['source_entity'],\n",
        "                        target=rel['target_entity']\n",
        "                    )\n",
        "\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing hotel {hotel_name}: {str(e)}\")\n",
        "            return {\"entities\": [], \"relationships\": []}\n",
        "\n",
        "\n",
        "\n",
        "    def process_hotels_from_neo4j(self, batch_size: Optional[int] = None):\n",
        "        \"\"\"Process hotels from existing Neo4j HOTEL nodes\"\"\"\n",
        "        with self.driver.session() as session:\n",
        "            # First, count total hotels\n",
        "            count_query = \"\"\"\n",
        "            MATCH (h:HOTEL)\n",
        "            RETURN count(h) as total\n",
        "            \"\"\"\n",
        "            total = session.run(count_query).single()['total']\n",
        "\n",
        "            # Then process in batches if specified\n",
        "            query = \"\"\"\n",
        "            MATCH (h:HOTEL)\n",
        "            RETURN h.name as name, h.description as description\n",
        "            \"\"\"\n",
        "            if batch_size:\n",
        "                query += f\" SKIP $skip LIMIT {batch_size}\"\n",
        "\n",
        "            processed = 0\n",
        "            while processed < total:\n",
        "                results = session.run(query, skip=processed)\n",
        "                for record in results:\n",
        "                    processed += 1\n",
        "                    if record['description'] is None:\n",
        "                        print(f\"Skipping hotel {processed}/{total}: {record['name']} (no description)\")\n",
        "                        continue\n",
        "                    print(f\"Processing hotel {processed}/{total}: {record['name']}\")\n",
        "                    self.process_and_save(\n",
        "                        hotel_name=record['name'],\n",
        "                        text=record['description']\n",
        "                    )\n",
        "\n",
        "                if batch_size:\n",
        "                    print(f\"Completed batch of {batch_size} hotels\")"
      ],
      "metadata": {
        "id": "Dz4fNeeu0LHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai_client = OpenAI()\n",
        "extractor = Neo4jGraphExtractor(openai_client=openai_client,\n",
        "                                neo4j_driver=driver,\n",
        "                                entity_types = [\"HOTEL\", \"AMENITY\", \"TOURIST_ATTRACTION\"],\n",
        "                                rel_types=[\"HAS_AMENITY\", \"LOCATED_NEARBY\"],\n",
        "                                )\n",
        "\n",
        "# Process from Neo4j:\n",
        "extractor.process_hotels_from_neo4j(batch_size=100)\n",
        "\n"
      ],
      "metadata": {
        "id": "WUDjQLNEw88T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our knowledge graph has now been augmented with additional fact triplets about the hotels. This enrichment allows us to capture more complex relationships between hotels, amenities, and tourist attractions."
      ],
      "metadata": {
        "id": "JnSzJXx0eQ5V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build database indices for the graph\n",
        "\n",
        "With the data in place, we can next create a vector index over the text properties, exposing all of our nodes to HNSW similarity search.\n",
        "\n",
        "Recall that in our basic KG notebook, we crafted our node MERGE Cypher statement to give all new nodes a base label `__Entity__` along with their type label from our hotels data model. In Neo4j, we can only a create a vector index for a single node label, but we might not know all the entity types we will need ahead of time. Hence why we gave all our nodes a base entity label to cover all potential entity types. Now that we've added new entity and relationship types to our KG, we're ready to create the vector index that will allow us to perform similarity search."
      ],
      "metadata": {
        "id": "0G5L4sucnLae"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain.vectorstores.neo4j_vector import Neo4jVector\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "vector_index = Neo4jVector.from_existing_graph(\n",
        "    OpenAIEmbeddings(),\n",
        "    url=url,\n",
        "    username=username,\n",
        "    password=password,\n",
        "    index_name='reviews',\n",
        "    node_label=\"__Entity__\",\n",
        "    text_node_properties=['name', 'description', 'text'],\n",
        "    embedding_node_property='embedding',\n",
        ")\n"
      ],
      "metadata": {
        "id": "EO1S7UM021eJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the vector search capability with a dash of Langchain\n",
        "response = vector_index.similarity_search(\n",
        "    \"What positive things are said about the Pera Palace Hotel?\"\n",
        ")\n",
        "\n",
        "print(\"Top similarity search result:\")\n",
        "print(response[0].page_content)"
      ],
      "metadata": {
        "id": "wA_-zSyb3-5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Retrieval"
      ],
      "metadata": {
        "id": "Dxh3h8cfsoTQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text2Cypher: Building a Flexible Retrieval Pattern\n",
        "\n",
        "In the basic notebook, we explored a template-based approach to graph retrieval, which works well for common query patterns with clear entities. However, template-based systems are limited to predefined patterns and can't handle novel queries or complex structural relationships.\n",
        "\n",
        "Text2Cypher is a more flexible retrieval pattern that uses an LLM to generate custom Cypher queries on the fly based on:\n",
        "1. The natural language query\n",
        "2. The graph schema\n",
        "3. Best practices for Cypher query construction\n",
        "\n",
        "Our applications gain several advantages from this approach:\n",
        "- **Flexibility**: Can handle arbitrary query patterns not covered by templates\n",
        "- **Structural Understanding**: Maintains awareness of the graph structure\n",
        "- **Complex Relationships**: Supports multi-hop traversals and complex filtering\n",
        "\n",
        "The implementation involves:\n",
        "1. Fetching the graph schema from Neo4j\n",
        "2. Using the schema to guide the LLM in generating valid Cypher\n",
        "3. Executing the generated Cypher and processing the results\n",
        "4. Generating a natural language answer from the structured results\n",
        "\n",
        "Let's implement a Text2Cypher retriever from scratch:"
      ],
      "metadata": {
        "id": "YFf5wV1wNSr6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text2Cypher retriever"
      ],
      "metadata": {
        "id": "8gFKE2hiZdTe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import List, Dict, Any, Optional, Union\n",
        "from openai import OpenAI\n",
        "import json\n",
        "from neo4j import GraphDatabase\n",
        "\n",
        "class Text2CypherRetriever:\n",
        "    \"\"\"\n",
        "    A retriever that converts natural language queries to Cypher queries using LLMs.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, neo4j_driver: GraphDatabase.driver, openai_client: OpenAI, schema: Optional[str] = None):\n",
        "        \"\"\"\n",
        "        Initialize the Text2Cypher retriever.\n",
        "\n",
        "        Args:\n",
        "            neo4j_driver: Neo4j database driver\n",
        "            openai_client: OpenAI client\n",
        "            schema: Optional schema information to guide Cypher generation.\n",
        "                   If None, the schema will be fetched from the database.\n",
        "        \"\"\"\n",
        "        self.driver = neo4j_driver\n",
        "        self.client = openai_client\n",
        "        self._schema = schema\n",
        "\n",
        "    @property\n",
        "    def schema(self) -> str:\n",
        "        \"\"\"Get the Neo4j schema information.\"\"\"\n",
        "        if self._schema is None:\n",
        "            self._schema = self._fetch_schema()\n",
        "        return self._schema\n",
        "\n",
        "    def _fetch_schema(self) -> str:\n",
        "        \"\"\"Fetch the graph schema information from the Neo4j database.\"\"\"\n",
        "        with self.driver.session() as session:\n",
        "            # Get node labels, properties, and relationship types\n",
        "            result = session.run(\"\"\"\n",
        "            CALL apoc.meta.schema()\n",
        "            YIELD value\n",
        "            RETURN value\n",
        "            \"\"\")\n",
        "\n",
        "            schema_data = result.single()[\"value\"]\n",
        "\n",
        "            # Format the schema in a way that's easy for the LLM to understand\n",
        "            formatted_schema = \"# Node Labels and Properties\\n\"\n",
        "\n",
        "            for node_label, node_data in schema_data.items():\n",
        "                if node_data.get(\"type\") == \"node\":\n",
        "                    formatted_schema += f\"\\n## {node_label}\\n\"\n",
        "                    formatted_schema += \"Properties:\\n\"\n",
        "\n",
        "                    for prop, prop_data in node_data.get(\"properties\", {}).items():\n",
        "                        if prop != \"embedding\":  # Skip embedding properties\n",
        "                            formatted_schema += f\"- {prop}: {prop_data.get('type', 'unknown')}\\n\"\n",
        "\n",
        "            formatted_schema += \"\\n# Relationship Types\\n\"\n",
        "            rels = set()\n",
        "\n",
        "            for node_data in schema_data.values():\n",
        "                if node_data.get(\"type\") == \"node\":\n",
        "                    for rel in node_data.get(\"relationships\", {}).values():\n",
        "                        rel_type = rel.get(\"type\")\n",
        "                        if rel_type:\n",
        "                            rels.add(rel_type)\n",
        "\n",
        "            for rel in sorted(rels):\n",
        "                formatted_schema += f\"- {rel}\\n\"\n",
        "\n",
        "            return formatted_schema\n",
        "\n",
        "    def generate_cypher(self, query: str) -> str:\n",
        "        \"\"\"\n",
        "        Generate a Cypher query from a natural language query.\n",
        "\n",
        "        Args:\n",
        "            query: Natural language query\n",
        "\n",
        "        Returns:\n",
        "            Generated Cypher query\n",
        "        \"\"\"\n",
        "        prompt = f\"\"\"You are a Neo4j Cypher query expert. Your task is to translate natural language questions into Cypher queries.\n",
        "\n",
        "Below is the schema of the Neo4j graph database:\n",
        "\n",
        "{self.schema}\n",
        "\n",
        "Important Guidelines:\n",
        "1. Generate only the Cypher query, with no explanations, comments, or markdown formatting.\n",
        "2. Always return nodes with all their properties to provide complete information.\n",
        "3. Avoid using the 'embedding' property in your queries - it contains vector data and is very large.\n",
        "4. Use appropriate aggregation functions (count, avg, collect) when grouping data.\n",
        "5. Limit results to a reasonable number (10-20 max) when returning many nodes.\n",
        "6. For more complex queries, consider using multiple MATCH clauses.\n",
        "7. Make sure to use the correct relationship directions in your query.\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Cypher Query:\"\"\"\n",
        "\n",
        "        response = self.client.chat.completions.create(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            temperature=0,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a Neo4j Cypher query generation assistant.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        cypher_query = response.choices[0].message.content.strip()\n",
        "\n",
        "        # Clean up any markdown formatting if present\n",
        "        if cypher_query.startswith(\"```\") and cypher_query.endswith(\"```\"):\n",
        "            cypher_query = cypher_query[3:-3].strip()\n",
        "        if cypher_query.startswith(\"```cypher\"):\n",
        "            cypher_query = cypher_query[9:].strip()\n",
        "            if cypher_query.endswith(\"```\"):\n",
        "                cypher_query = cypher_query[:-3].strip()\n",
        "\n",
        "        return cypher_query\n",
        "\n",
        "    def execute_cypher(self, cypher_query: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Execute a Cypher query against the Neo4j database.\n",
        "\n",
        "        Returns:\n",
        "            List of result records as dictionaries\n",
        "        \"\"\"\n",
        "        try:\n",
        "            with self.driver.session() as session:\n",
        "                result = session.run(cypher_query)\n",
        "                records = [dict(record) for record in result]\n",
        "                return records\n",
        "        except Exception as e:\n",
        "            print(f\"Error executing Cypher query: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "    def retrieve(self, query: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Retrieve information from the Neo4j database using a natural language query.\n",
        "        \"\"\"\n",
        "        # Generate Cypher query\n",
        "        cypher_query = self.generate_cypher(query)\n",
        "\n",
        "        # Execute the query\n",
        "        results = self.execute_cypher(cypher_query)\n",
        "\n",
        "        return {\n",
        "            \"query\": query,\n",
        "            \"cypher_query\": cypher_query,\n",
        "            \"results\": results\n",
        "        }\n",
        "\n",
        "    def generate_answer(self, query: str, results: List[Dict[str, Any]]) -> str:\n",
        "        \"\"\"\n",
        "        Generate a natural language answer based on query results.\n",
        "        \"\"\"\n",
        "        # Format results for the prompt\n",
        "        if not results:\n",
        "            results_text = \"No results were found for this query.\"\n",
        "        else:\n",
        "            # Convert results to a clean text representation\n",
        "            items = []\n",
        "            for i, item in enumerate(results[:10]):  # Limit to 10 items to keep prompt size reasonable\n",
        "                item_str = f\"Result {i+1}:\\n\"\n",
        "                for k, v in item.items():\n",
        "                    if k == \"embedding\":\n",
        "                        continue  # Skip embedding vectors\n",
        "                    if isinstance(v, (list, set)):\n",
        "                        v_str = \", \".join(str(x) for x in v if x is not None)\n",
        "                        item_str += f\"  {k}: {v_str}\\n\"\n",
        "                    else:\n",
        "                        item_str += f\"  {k}: {v}\\n\"\n",
        "                items.append(item_str)\n",
        "\n",
        "            if len(results) > 10:\n",
        "                items.append(f\"... and {len(results) - 10} more results.\")\n",
        "\n",
        "            results_text = \"\\n\".join(items)\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "The user asked: \"{query}\"\n",
        "\n",
        "I retrieved the following information from the graph database:\n",
        "{results_text}\n",
        "\n",
        "Based on this information, provide a helpful, conversational response to the user's query.\n",
        "Make sure to address all aspects of their question if possible.\n",
        "If the information retrieved doesn't fully answer their query, acknowledge this limitation.\n",
        "\"\"\"\n",
        "\n",
        "        response = self.client.chat.completions.create(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            temperature=0.7,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that provides accurate information based on database query results.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        return response.choices[0].message.content\n",
        "\n",
        "    def query(self, query: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Process a natural language query end-to-end.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with answer, retrieval info, and intermediate steps\n",
        "        \"\"\"\n",
        "        # Retrieve information\n",
        "        retrieval_result = self.retrieve(query)\n",
        "\n",
        "        # Generate answer\n",
        "        answer = self.generate_answer(query, retrieval_result[\"results\"])\n",
        "\n",
        "        return {\n",
        "            \"query\": query,\n",
        "            \"answer\": answer,\n",
        "            \"retrieval_info\": {\n",
        "                \"cypher_query\": retrieval_result[\"cypher_query\"],\n",
        "                \"result_count\": len(retrieval_result[\"results\"])\n",
        "            },\n",
        "            \"results\": retrieval_result[\"results\"]\n",
        "        }\n",
        "\n"
      ],
      "metadata": {
        "id": "eCu0R80vswZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's initialize and test our Text2Cypher retriever:\n"
      ],
      "metadata": {
        "id": "LR685eZNltfR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text2cypher_retriever = Text2CypherRetriever(\n",
        "    neo4j_driver=driver,\n",
        "    openai_client=openai_client\n",
        ")"
      ],
      "metadata": {
        "id": "mfSnUKLjetdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text2cypher_retriever.query(\"What tourist attractions are LOCATED NEARBY the Grant Plaza Hotel?\")"
      ],
      "metadata": {
        "id": "4LfwCS9Se_hP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Vector Similarity Retriever Implementation"
      ],
      "metadata": {
        "id": "7RPKUF8jZ2Ll"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's implement a retriever that uses vector similarity to find relevant nodes in the graph:"
      ],
      "metadata": {
        "id": "9DkSRSqsmE3M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VectorSimilarityRetriever:\n",
        "    def __init__(self, neo4j_driver: GraphDatabase.driver, openai_client: OpenAI, index_name: str = \"reviews\"):\n",
        "        \"\"\"\n",
        "        Initialize the vector similarity retriever.\n",
        "\n",
        "        Args:\n",
        "            neo4j_driver: Neo4j database driver\n",
        "            openai_client: OpenAI client\n",
        "            index_name: Name of the vector index in Neo4j\n",
        "        \"\"\"\n",
        "        self.driver = neo4j_driver\n",
        "        self.client = openai_client\n",
        "        self.index_name = index_name\n",
        "        self.embeddings_model = \"text-embedding-ada-002\"\n",
        "\n",
        "    def get_embedding(self, text: str) -> List[float]:\n",
        "        \"\"\"\n",
        "        Get an embedding vector for the given text.\n",
        "\n",
        "        Args:\n",
        "            text: Text to embed\n",
        "\n",
        "        Returns:\n",
        "            Embedding vector\n",
        "        \"\"\"\n",
        "        response = self.client.embeddings.create(\n",
        "            model=self.embeddings_model,\n",
        "            input=text\n",
        "        )\n",
        "        return response.data[0].embedding\n",
        "\n",
        "    def retrieve(self, query: str, limit: int = 10, node_labels: Optional[List[str]] = None) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Retrieve nodes from the graph based on vector similarity.\n",
        "\n",
        "        Args:\n",
        "            query: Query text\n",
        "            limit: Maximum number of results to return\n",
        "            node_labels: Optional list of node labels to filter by\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing the results and query information\n",
        "        \"\"\"\n",
        "        # Get embedding for the query\n",
        "        query_embedding = self.get_embedding(query)\n",
        "\n",
        "        # Construct Neo4j query for vector search with more targeted results\n",
        "        cypher_query = f\"\"\"\n",
        "        CALL db.index.vector.queryNodes($index_name, $limit, $query_embedding)\n",
        "        YIELD node, score\n",
        "        \"\"\"\n",
        "\n",
        "        # Add label filter if provided, otherwise prioritize REVIEW nodes for text queries\n",
        "        if node_labels:\n",
        "            label_filters = []\n",
        "            for label in node_labels:\n",
        "                label_filters.append(f\"node:{label}\")\n",
        "            cypher_query += f\"WHERE {' OR '.join(label_filters)}\\n\"\n",
        "        else:\n",
        "            # For review-specific queries, prioritize REVIEW nodes\n",
        "            if \"review\" in query.lower() or \"said\" in query.lower() or \"sentiment\" in query.lower():\n",
        "                cypher_query += \"WHERE node:REVIEW\\n\"\n",
        "\n",
        "        cypher_query += \"\"\"\n",
        "        RETURN node, score\n",
        "        ORDER BY score DESC\n",
        "        \"\"\"\n",
        "\n",
        "        # Execute query\n",
        "        try:\n",
        "            with self.driver.session() as session:\n",
        "                result = session.run(\n",
        "                    cypher_query,\n",
        "                    index_name=self.index_name,\n",
        "                    limit=limit,\n",
        "                    query_embedding=query_embedding\n",
        "                )\n",
        "\n",
        "                # Transform the results into a more usable format\n",
        "                records = []\n",
        "                for record in result:\n",
        "                    node = record[\"node\"]\n",
        "                    score = record[\"score\"]\n",
        "\n",
        "                    # Extract all node properties\n",
        "                    props = dict(node)\n",
        "                    if \"embedding\" in props:\n",
        "                        del props[\"embedding\"]  # Skip embedding vectors\n",
        "\n",
        "                    # Add score and labels\n",
        "                    props[\"similarity_score\"] = score\n",
        "                    props[\"labels\"] = list(node.labels)\n",
        "\n",
        "                    records.append(props)\n",
        "\n",
        "                return {\n",
        "                    \"query\": query,\n",
        "                    \"results\": records\n",
        "                }\n",
        "        except Exception as e:\n",
        "            print(f\"Error executing vector search: {str(e)}\")\n",
        "            return {\n",
        "                \"query\": query,\n",
        "                \"error\": str(e),\n",
        "                \"results\": []\n",
        "            }\n",
        "\n",
        "    def generate_answer(self, query: str, results: List[Dict[str, Any]]) -> str:\n",
        "        \"\"\"\n",
        "        Generate a natural language answer based on query results.\n",
        "\n",
        "        Args:\n",
        "            query: Original natural language query\n",
        "            results: Query results\n",
        "\n",
        "        Returns:\n",
        "            Natural language answer\n",
        "        \"\"\"\n",
        "        # Format results for the prompt\n",
        "        if not results:\n",
        "            results_text = \"No results were found for this query.\"\n",
        "        else:\n",
        "            # Convert results to a clean text representation\n",
        "            items = []\n",
        "            for i, item in enumerate(results[:5]):  # Limit to 5 items\n",
        "                item_str = f\"Result {i+1} (Similarity: {item.get('similarity_score', 'N/A')}):\\n\"\n",
        "                for k, v in item.items():\n",
        "                    if k in ['embedding', 'similarity_score']:\n",
        "                        continue  # Skip embedding vectors and already displayed score\n",
        "                    if isinstance(v, (list, set)):\n",
        "                        v_str = \", \".join(str(x) for x in v if x is not None)\n",
        "                        item_str += f\"  {k}: {v_str}\\n\"\n",
        "                    else:\n",
        "                        item_str += f\"  {k}: {v}\\n\"\n",
        "                items.append(item_str)\n",
        "\n",
        "            if len(results) > 10:\n",
        "                items.append(f\"... and {len(results) - 10} more results.\")\n",
        "\n",
        "            results_text = \"\\n\".join(items)\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "The user asked: \"{query}\"\n",
        "\n",
        "I retrieved the following information using semantic similarity search:\n",
        "{results_text}\n",
        "\n",
        "Based on this information, provide a helpful, conversational response to the user's query.\n",
        "Focus on the most relevant information from the results.\n",
        "If the information retrieved doesn't fully answer their query, acknowledge this limitation.\n",
        "\"\"\"\n",
        "\n",
        "        response = self.client.chat.completions.create(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            temperature=0.7,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that provides accurate information based on semantic search results.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        return response.choices[0].message.content\n",
        "\n",
        "    def query(self, query: str, limit: int = 10, node_labels: Optional[List[str]] = None) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Process a natural language query end-to-end.\n",
        "\n",
        "        Args:\n",
        "            query: Natural language query\n",
        "            limit: Maximum number of results to return\n",
        "            node_labels: Optional list of node labels to filter by\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with answer, retrieval info, and results\n",
        "        \"\"\"\n",
        "        # Retrieve information\n",
        "        retrieval_result = self.retrieve(query, limit, node_labels)\n",
        "\n",
        "        # Generate answer\n",
        "        answer = self.generate_answer(query, retrieval_result[\"results\"])\n",
        "\n",
        "        return {\n",
        "            \"query\": query,\n",
        "            \"answer\": answer,\n",
        "            \"retrieval_info\": {\n",
        "                \"retrieval_method\": \"vector_similarity\",\n",
        "                \"result_count\": len(retrieval_result[\"results\"])\n",
        "            },\n",
        "            \"results\": retrieval_result[\"results\"]\n",
        "        }"
      ],
      "metadata": {
        "id": "YrZfIuf5Z3Tj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_retriever = VectorSimilarityRetriever(\n",
        "    neo4j_driver=driver,\n",
        "    openai_client=openai_client\n",
        ")\n",
        "\n",
        "vector_retriever.query(\"What positive things are said about the Sirdeci Mansion Hotel?\")"
      ],
      "metadata": {
        "id": "wJewftZyqv0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Router Retriever: Homework Assignment\n",
        "Now that we've implemented three different retrieval strategies, we need a router that can intelligently select the most appropriate strategy for each query. This is where your homework assignment comes in!\n",
        "Below is a skeleton implementation of a RouterRetriever class. Your task is to complete the implementation by filling in the missing parts:"
      ],
      "metadata": {
        "id": "nTiw470vaQ7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RouterRetriever:\n",
        "    \"\"\"\n",
        "    A router that selects the appropriate retrieval strategy based on query characteristics.\n",
        "\n",
        "    This is a skeleton implementation for you to complete as part of the homework assignment.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        text2cypher_retriever: Text2CypherRetriever,\n",
        "        template_retriever: SimpleGraphRAG, # copy from basic notebook\n",
        "        vector_retriever: VectorSimilarityRetriever,\n",
        "        openai_client: OpenAI\n",
        "    ):\n",
        "        \"\"\"Initialize with all retriever implementations.\"\"\"\n",
        "        self.text2cypher_retriever = text2cypher_retriever\n",
        "        self.template_retriever = template_retriever\n",
        "        self.vector_retriever = vector_retriever\n",
        "        self.client = openai_client\n",
        "\n",
        "    def route_query(self, query: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Analyze the query and determine which retrieval strategy to use.\n",
        "\n",
        "        Args:\n",
        "            query: The natural language query\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with the selected strategy and reasoning\n",
        "\n",
        "        TODO: Implement this method to select the most appropriate retrieval strategy.\n",
        "        \"\"\"\n",
        "        # TODO: Implement query analysis and strategy selection\n",
        "\n",
        "        # Default implementation (replace with your own)\n",
        "        return {\n",
        "            \"strategy\": \"text2cypher\",  # Default fallback\n",
        "            \"reasoning\": \"Default strategy - replace with actual reasoning\"\n",
        "        }\n",
        "\n",
        "    def query(self, query: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Route the query to the appropriate retriever and return results.\n",
        "\n",
        "        Args:\n",
        "            query: The natural language query\n",
        "\n",
        "        Returns:\n",
        "            Results from the selected retriever with routing information\n",
        "\n",
        "        TODO: Complete this method to execute the query using the selected strategy.\n",
        "        \"\"\"\n",
        "        # TODO: Implement query routing\n",
        "        # 1. Call route_query to determine the best strategy\n",
        "        # 2. Execute the query using the selected retriever\n",
        "        # 3. Return the results with added routing information\n",
        "\n",
        "        # Default implementation (replace with your own)\n",
        "        strategy = \"text2cypher\"  # Replace with actual routing logic\n",
        "        result = self.text2cypher_retriever.query(query)\n",
        "\n",
        "        # TODO: Add routing information to the result\n",
        "\n",
        "        return result\n",
        "\n"
      ],
      "metadata": {
        "id": "HfMdx3G3aU50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Assignment Guidelines:\n",
        "\n",
        "1. **Strategy Selection Logic:** Implement route_query to intelligently analyze the query and select the most appropriate retrieval strategy.\n",
        "\n",
        "  * Document your reasoning process for different query types\n",
        "\n",
        "\n",
        "2. **Router Implementation:** Complete the query method to route queries to the appropriate retriever.\n",
        "\n",
        "  * Handle errors gracefully if a strategy fails\n",
        "\n",
        "\n",
        "3. **Testing and Evaluation:**\n",
        "\n",
        "  * Test your router with a variety of queries\n",
        "  * Compare the results from different strategies\n",
        "\n",
        "\n",
        "4. **Extra Credit:**\n",
        "\n",
        "  * Implement a hybrid approach (potentially even a dedicated hybrid retriever) that combines results from multiple strategies\n",
        "\n",
        "\n",
        "### Decision Criteria for Your Router\n",
        "When implementing your router, consider these factors to determine the best strategy:\n",
        "\n",
        "1. **Query Structure:**\n",
        "\n",
        "  * Template patterns: Common patterns with clear entities (template)\n",
        "  * Complex relationships: Multi-hop traversals (text2cypher or template) or aggregations (text2cypher)\n",
        "  * Semantic/conceptual: Opinion or concept-based queries (vector)\n",
        "\n",
        "\n",
        "2. **Entity Presence:**\n",
        "\n",
        "  * Clear entity mentions: Specific hotel names, locations, etc. (template or text2cypher)\n",
        "  * Conceptual descriptions: \"Luxury\", \"family-friendly\", etc. (vector)\n",
        "\n",
        "\n",
        "3. **Query Intent:**\n",
        "\n",
        "  * Fact retrieval: \"How many reviews does hotel X have?\" (text2cypher)\n",
        "  * Opinion extraction: \"What do guests say about X?\" (vector)\n",
        "  * Common lookup patterns: \"Tell me about hotel X\" (template)"
      ],
      "metadata": {
        "id": "a7JF7TOuBY2a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparing Vector RAG vs Graph RAG: A Practical Analysis\n",
        "\n",
        "When implementing RAG systems, it's crucial to understand when graph-based approaches offer meaningful advantages over traditional vector RAG. Let's examine some real queries against our hotel reviews knowledge graph to understand these tradeoffs.\n",
        "\n",
        "## Case Study 1: Multi-Hop Queries\n",
        "### Query: \"What highly-rated hotels near Fisherman's Wharf offer free Wi-Fi and easy access to cable cars?\"\n",
        "\n",
        "#### Vector RAG Response:\n",
        "> *Two highly-rated hotels near Fisherman's Wharf that offer free Wi-Fi and easy access to cable cars are\n",
        "San Francisco Marriott Fisherman's Wharf and Hotel Riu Plaza Fisherman's Wharf.*\n",
        "\n",
        "#### Graph RAG Response:\n",
        "Using the following Cypher for the retrieval:\n",
        "```cypher\n",
        "MATCH (h:HOTEL)-[:LOCATED_NEARBY]->(fw:TOURIST_ATTRACTION {name: 'Fisherman\\'s Wharf'}),\n",
        "      (h)-[:HAS_AMENITY]->(wifi:AMENITY {name: 'Free Wi-Fi'}),\n",
        "      (h)-[:LOCATED_NEARBY]->(cc:TOURIST_ATTRACTION {name: 'San Francisco Cable Car line'}),\n",
        "      (h)-[:HAS_REVIEW]->(r:REVIEW)\n",
        "WHERE r.rating_value >= 4.0\n",
        "RETURN DISTINCT h.name\n",
        "```\n",
        "\n",
        ">*The Hotel Riu Plaza Fishermans Wharf, located in San Francisco, offers free Wi-Fi and easy access to the San Francisco Cable Car line.*\n",
        "\n",
        "#### Analysis:\n",
        "1. **Accuracy**: Vector RAG incorrectly includes SF Marriott, which doesn't mention free Wi-Fi in its description. Graph RAG correctly identifies only Hotel Riu by following explicit relationships.\n",
        "\n",
        "2. **Query Processing**:\n",
        "   - Vector RAG attempts to infer relationships from text proximity\n",
        "   - Graph RAG follows verified relationship paths\n",
        "   - Multi-hop queries strongly favor graph approach\n",
        "\n",
        "3. **Confidence**: Graph RAG can guarantee the existence of relationships, while vector RAG makes best guesses\n",
        "\n",
        "## Case Study 2: Amenity Comparisons\n",
        "### Query: \"Compare amenities and ratings for hotels near Fisherman's Wharf and Pier 39\"\n",
        "\n",
        "Vector RAG Response:\n",
        "```\n",
        "Based on the information provided:\n",
        "\n",
        "1. **San Francisco Marriott Fisherman's Wharf**:\n",
        "   - Amenities: Fitness Center, Handicap Accessible Rooms, Safety Deposit Boxes, Iron, Ironing Board, Hairdryers.\n",
        "   - Rating: No specific rating mentioned.\n",
        "\n",
        "2. **Hotel Riu Plaza Fishermans Wharf**:\n",
        "   - Amenities: Free WiFi, Outdoor Pool, Gym, Conference Rooms, Satellite TV, Mini-fridge, Coffee Machine, AC, Buffet Breakfast, Grab & Go Bar.\n",
        "   - Rating: No specific rating mentioned.\n",
        "\n",
        "3. **Hotel Zephyr San Francisco**:\n",
        "   - Amenities: Not specified in the provided information.\n",
        "   - Rating: No specific rating mentioned.\n",
        "\n",
        "4. **PIER 39**:\n",
        "   - Amenities: Shops, Restaurants, Sea lions.\n",
        "   - Rating: No specific rating mentioned.\n",
        "\n",
        "Unfortunately, the information provided does not include specific ratings for the hotels near Fisherman's Wharf and Pier 39, so I cannot provide a direct comparison in terms of ratings. If you have a specific rating range in mind or other criteria for comparison, please let me know.\n",
        "```\n",
        "\n",
        "Graph RAG Response:\n",
        "\n",
        "```\n",
        "The Hyatt Centric Fisherman's Wharf San Francisco and Hotel Riu Plaza Fishermans Wharf are two hotels near both Fisherman's Wharf and PIER 39.\n",
        "\n",
        "The Hyatt Centric offers amenities such as Brick & Beam, a Fitness Center, Free Wi-Fi, Meeting Rooms, and a Relaxing Pool. It has an average rating of 4.33.\n",
        "\n",
        "The Hotel Riu Plaza provides amenities like a Buffet Breakfast Service, a Bar, a Minibar, a TV with Free Movies, and Coffee and Tea Facilities. It has an average rating of 4.0.\n",
        "```\n",
        "\n",
        "#### Analysis:\n",
        "1. **Structured Data**: Graph RAG excels at comparing discrete attributes\n",
        "2. **Relationship Context**: Understanding nearness to multiple landmarks\n",
        "3. **Aggregation**: Can compute statistics across relationship patterns\n",
        "\n",
        "## Key Findings\n",
        "\n",
        "### When to Use Graph RAG:\n",
        "1. **Relationship-Critical Queries**\n",
        "   - Multiple hops required (e.g., \"hotels near X with amenity Y\")\n",
        "   - Relationship accuracy matters\n",
        "   - Complex pattern matching\n",
        "\n",
        "2. **Structured Comparisons**\n",
        "   - Comparing entities across relationships\n",
        "   - Aggregating across relationship patterns\n",
        "   - Need for verified connections\n",
        "\n",
        "3. **Hybrid Questions**\n",
        "   - Combining factual relationships with semantic search\n",
        "   - Need both structured and unstructured insights\n",
        "\n",
        "### When Traditional Vector RAG Suffices:\n",
        "1. **Simple Semantic Queries**\n",
        "   - Single-entity questions\n",
        "   - General descriptions or summaries\n",
        "   - No relationship traversal needed\n",
        "\n",
        "2. **Fuzzy Matching**\n",
        "   - When exact relationship matching isn't critical\n",
        "   - Flexible interpretation acceptable\n",
        "   - General sentiment or topic analysis\n",
        "\n",
        "## Implementation Considerations\n",
        "\n",
        "1. **Data Quality Requirements**\n",
        "   - Graph RAG requires explicit relationship modeling\n",
        "   - Higher upfront cost in knowledge graph construction\n",
        "   - Need for relationship maintenance/updates\n",
        "\n",
        "2. **Query Complexity**\n",
        "   - Graph queries can be more complex to construct\n",
        "   - Need for query optimization\n",
        "   - Hybrid approaches often optimal\n",
        "\n",
        "3. **System Architecture**\n",
        "   - Graph databases add operational complexity\n",
        "   - Vector indices still valuable for semantic search\n",
        "   - Consider hybrid architectures for complex applications\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UGYAPbek2KAm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XSkqDJZd2PXk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}