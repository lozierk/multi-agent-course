{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hamzafarooq/multi-agent-course/blob/main/Module_6/DSPy/DSPy%20Introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nP_GkfK7SFKn"
      },
      "source": [
        "# Intro to [DSPy](https://github.com/stanfordnlp/dspy?tab=readme-ov-file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NSBpA9mSqWw"
      },
      "source": [
        "## 1. Installing the requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90otxiC15_rS"
      },
      "outputs": [],
      "source": [
        "!pip install dspy-ai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHFDfaOb4sjV"
      },
      "outputs": [],
      "source": [
        "import dspy\n",
        "import sys\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwVlRLXb4rk5"
      },
      "source": [
        "\n",
        "## 2. Setting up your LM and RM\n",
        "\n",
        "We'll start by setting up the language model (LM) and retrieval model (RM).\n",
        "\n",
        "In this notebook, we'll work with GPT-4o and the retriever ColBERTv2.\n",
        "\n",
        "To make things easy, we've set up a ColBERTv2 server hosting a Wikipedia 2017 \"abstracts\" search index (i.e., containing first paragraph of each article from this 2017 dump).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6sQSOeb4t6F"
      },
      "outputs": [],
      "source": [
        "turbo = dspy.LM(model = 'gpt-4o-mini')\n",
        "colbertv2_wiki17_abstracts = dspy.ColBERTv2(url = 'http://20.102.90.50:2017/wiki17_abstracts')\n",
        "dspy.settings.configure(lm = turbo, rm = colbertv2_wiki17_abstracts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vyx5eNT85hjq"
      },
      "outputs": [],
      "source": [
        "from dspy.datasets import HotPotQA #HotPotQA dataset is used to benchmark multi-hop QA\n",
        "\n",
        "# Load the dataset.\n",
        "dataset = HotPotQA(train_seed=1, train_size=20, eval_seed=2023, dev_size=50, test_size=0) #Notice the size of training and dev dataset! Teeny tiny compared to other ML models.\n",
        "\n",
        "# Tell DSPy that the 'question' field is the input. Any other fields are labels and/or metadata.\n",
        "trainset = [x.with_inputs('question') for x in dataset.train]\n",
        "devset = [x.with_inputs('question') for x in dataset.dev]\n",
        "\n",
        "len(trainset), len(devset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCdr93BS68pp"
      },
      "source": [
        "Let's check some examples!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhHc8SsW6MxG"
      },
      "outputs": [],
      "source": [
        "for i in range(5):\n",
        "  train_example = trainset[i]\n",
        "  print(f\"Question: {train_example.question}\")\n",
        "  print(f\"Answer: {train_example.answer}\", '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZI6Ii5AC7Zst"
      },
      "source": [
        "As you see, not all questions are multi-hop, e.g. the very first one. But, the second question is one such question as it requires breaking up the question into pieces in order to provide an answer.\n",
        "\n",
        "Let's check an example from the development dataset. While we will not touch this for training, we will use this for metric evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vFOowQc7I_3"
      },
      "outputs": [],
      "source": [
        "dev_example = devset[18]\n",
        "print(f\"Question: {dev_example.question}\")\n",
        "print(f\"Answer: {dev_example.answer}\")\n",
        "print(f\"Relevant Wikipedia Titles: {dev_example.gold_titles}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8uKwr3Q77qn"
      },
      "outputs": [],
      "source": [
        "#This cell instructs how the data is presented to the model\n",
        "print(f\"For this dataset, training examples have input keys {train_example.inputs().keys()} and label keys {train_example.labels().keys()}\")\n",
        "print(f\"For this dataset, dev examples have input keys {dev_example.inputs().keys()} and label keys {dev_example.labels().keys()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDjcsa-58joe"
      },
      "source": [
        "## 3. Defining simple Signature and Predictor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AhswCIKU8LEp"
      },
      "outputs": [],
      "source": [
        "class BasicQA(dspy.Signature):\n",
        "    \"\"\"Answer questions with short factoid answers.\"\"\"\n",
        "\n",
        "    question = dspy.InputField()\n",
        "    answer = dspy.OutputField(desc=\"often between 1 and 5 words\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NoWqmdxK8w6I"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = 'your api key'  #my key redacted - add your own key here\n",
        "\n",
        "# Define the predictor.\n",
        "generate_answer = dspy.Predict(BasicQA)\n",
        "\n",
        "# Call the predictor on a particular input.\n",
        "pred = generate_answer(question=dev_example.question)\n",
        "\n",
        "# Print the input and the prediction.\n",
        "print(f\"Question: {dev_example.question}\")\n",
        "print(f\"Predicted Answer: {pred.answer}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5671uwhEjTq"
      },
      "source": [
        "^ If this gives an error complaining about not having and API_Key, click on the link and get a key. And run the following command:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dom7P1s4HJy2"
      },
      "source": [
        "**Wrong answer**. The chef is [Robert Irvine](https://en.wikipedia.org/wiki/Robert_Irvine), who is in fact British.\n",
        "\n",
        "\n",
        "We can explore the history of this answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwiZjN1s9Mgn"
      },
      "outputs": [],
      "source": [
        "turbo.inspect_history(n=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aj2VUsnPH-rU"
      },
      "source": [
        "There is no reasoning or chain of thought in the history of the LLM provided above. Instead of using `Predict`, we will use the `ChainOfThought` module of the `DSPy`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1SEfO2EJk0b"
      },
      "outputs": [],
      "source": [
        "dev_example = devset[18]\n",
        "print(f\"Question: {dev_example.question}\")\n",
        "print(f\"Answer: {dev_example.answer}\")\n",
        "print(f\"Relevant Wikipedia Titles: {dev_example.gold_titles}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FGluwcMHzIu"
      },
      "outputs": [],
      "source": [
        "# Replacing the dspy.Predict(BasicQA) with dspy.ChainOfThought(BasicQA) -> Notice that the BasicQA signature in untouched.\n",
        "generate_answer_with_chain_of_thought = dspy.ChainOfThought(BasicQA)\n",
        "\n",
        "# Call the predictor on the same input.\n",
        "pred = generate_answer_with_chain_of_thought(question=dev_example.question)\n",
        "\n",
        "# Print the input, the chain of thought, and the prediction.\n",
        "print(f\"Question: {dev_example.question}\")\n",
        "print(f\"Predicted Answer: {pred.answer}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "buKiPt6iJU4h"
      },
      "outputs": [],
      "source": [
        "turbo.inspect_history(n=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3iVJeMvKkZ1"
      },
      "source": [
        "## 4. Retrieval and basic RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9zmZ2gmKWZL"
      },
      "outputs": [],
      "source": [
        "retrieve = dspy.Retrieve(k=3)\n",
        "topK_passages = None\n",
        "while True:\n",
        "    try:\n",
        "        topK_passages = retrieve(dev_example.question).passages\n",
        "        break\n",
        "    except Exception as e:\n",
        "        continue\n",
        "\n",
        "print(f\"Top {retrieve.k} passages for question: {dev_example.question} \\n\", '-' * 30, '\\n')\n",
        "\n",
        "for idx, passage in enumerate(topK_passages):\n",
        "    print(f'{idx+1}]', passage, '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkAx6M6XKpi6"
      },
      "outputs": [],
      "source": [
        "topK_passages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5N_xZITKv97"
      },
      "outputs": [],
      "source": [
        "#check 3 passages for the same question\n",
        "for i in range(3):\n",
        "  while True:\n",
        "    try:\n",
        "      print(retrieve(dev_example.question).passages[i], '\\n')\n",
        "      break\n",
        "    except Exception as e:\n",
        "      continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcmFLE_5K6yd"
      },
      "outputs": [],
      "source": [
        "class GenerateAnswer(dspy.Signature):\n",
        "    \"\"\"Answer questions with short factoid answers.\"\"\"\n",
        "\n",
        "    context = dspy.InputField(desc=\"may contain relevant facts\")\n",
        "    question = dspy.InputField()\n",
        "    answer = dspy.OutputField(desc=\"often between 1 and 5 words\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kK0SFvR7LVC4"
      },
      "outputs": [],
      "source": [
        "class RAG(dspy.Module):\n",
        "    def __init__(self, num_passages=3):\n",
        "        super().__init__()\n",
        "\n",
        "        self.retrieve = dspy.Retrieve(k=num_passages)\n",
        "        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\n",
        "\n",
        "    def forward(self, question):\n",
        "        context = self.retrieve(question).passages\n",
        "        prediction = self.generate_answer(context=context, question=question)\n",
        "        return dspy.Prediction(context=context, answer=prediction.answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhXl_fqOLmdh"
      },
      "source": [
        "Time for optimizer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDHWZXPGLbVO"
      },
      "outputs": [],
      "source": [
        "from dspy.teleprompt import BootstrapFewShot\n",
        "\n",
        "# Validation logic: check that the predicted answer is correct.\n",
        "# Also check that the retrieved context does actually contain that answer.\n",
        "def validate_context_and_answer(example, pred, trace=None):\n",
        "    answer_EM = dspy.evaluate.answer_exact_match(example, pred) #This metric is Exact Match\n",
        "    return answer_EM\n",
        "\n",
        "# Set up a basic teleprompter, which will compile our RAG program.\n",
        "teleprompter = BootstrapFewShot(metric=validate_context_and_answer) #This line bootstraps few-shot examples\n",
        "\n",
        "# Compile!\n",
        "compiled_rag = None\n",
        "while True:\n",
        "    try:\n",
        "      compiled_rag = teleprompter.compile(RAG(), trainset=trainset)\n",
        "      break\n",
        "    except Exception as e:\n",
        "      print(f\"Exception: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bcD3a9wMH14"
      },
      "source": [
        "^ It'll stop once it has reached some performance threshold.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9K2iXjS8L5rg"
      },
      "outputs": [],
      "source": [
        "# Ask any question you like to this simple RAG program.\n",
        "my_question = \"What castle did David Gregory inherit?\"\n",
        "\n",
        "# Get the prediction. This contains `pred.context` and `pred.answer`.\n",
        "pred = None\n",
        "while True:\n",
        "    try:\n",
        "      pred = compiled_rag(my_question)\n",
        "      break\n",
        "    except Exception as e:\n",
        "      continue\n",
        "\n",
        "# Print the contexts and the answer.\n",
        "print(f\"Question: {my_question}\")\n",
        "print(f\"Predicted Answer: {pred.answer}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MM4BdyHwMTXX"
      },
      "outputs": [],
      "source": [
        "# Let's check the retrieved passage\n",
        "\n",
        "print(f\"Retrieved Contexts (truncated): {[c[:200] + '...' for c in pred.context]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-PbKNA8MrzG"
      },
      "source": [
        "**For Readability:**\n",
        "\n",
        "Retrieved Contexts (truncated): ['David Gregory (physician) | David Gregory (20 December 1625 – 1720) was a Scottish physician and inventor. His surname is sometimes spelt as Gregorie, the original Scottish spelling. He inherited Kinn...', 'Gregory Tarchaneiotes | Gregory Tarchaneiotes (Greek: Γρηγόριος Ταρχανειώτης , Italian: \"Gregorio Tracanioto\" or \"Tracamoto\" ) was a \"protospatharius\" and the long-reigning catepan of Italy from 998 t...', 'David Gregory (mathematician) | David Gregory (originally spelt Gregorie) FRS (? 1659 – 10 October 1708) was a Scottish mathematician and astronomer. He was professor of mathematics at the University ...']\n",
        "\n",
        "**And the wikipedia link**: [David Gregory](https://en.wikipedia.org/wiki/David_Gregory_(physician)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4eUDURbhMn6-"
      },
      "outputs": [],
      "source": [
        "turbo.inspect_history(n=1) #To see the last context the LLM has seen. If you wanna see the previous context, you can set \"n\" to that number"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjpt3I3VOVM3"
      },
      "source": [
        "Inspect the parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52FbvsWnOXxY"
      },
      "outputs": [],
      "source": [
        "for name, parameter in compiled_rag.named_predictors():\n",
        "  print(name)\n",
        "  print(parameter.demos[0], '\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATq8RCc1O2a6"
      },
      "source": [
        "## 5. Evaluate the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NiowmB0SO5bJ"
      },
      "outputs": [],
      "source": [
        "from dspy.evaluate import Evaluate\n",
        "\n",
        "# Set up the `evaluate_on_hotpotqa` function.\n",
        "evaluate_on_hotpotqa = Evaluate(devset=devset, num_threads=1, display_progress=True, display_table=5)\n",
        "\n",
        "# Evaluate the `compiled_rag` program with the `answer_exact_match` metric.\n",
        "metric = None\n",
        "while True:\n",
        "    try:\n",
        "        metric = dspy.evaluate.answer_exact_match\n",
        "        evaluate_on_hotpotqa(compiled_rag, metric=metric)\n",
        "        break\n",
        "    except Exception as e:\n",
        "        continue\n",
        "\n",
        "evaluate_on_hotpotqa(compiled_rag, metric=metric)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def gold_passages_retrieved(example, pred, trace=None):\n",
        "    gold_titles = set(map(dspy.evaluate.normalize_text, example['gold_titles']))\n",
        "    found_titles = set(map(dspy.evaluate.normalize_text, [c.split(' | ')[0] for c in pred.context]))\n",
        "\n",
        "    return gold_titles.issubset(found_titles)\n",
        "compiled_rag_retrieval_score = None\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        compiled_rag_retrieval_score = evaluate_on_hotpotqa(compiled_rag, metric=gold_passages_retrieved)\n",
        "        break\n",
        "    except Exception as e:\n",
        "        continue"
      ],
      "metadata": {
        "id": "YAZTK1FpTI-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For more advanced topics, refer to https://github.com/stanfordnlp/dspy/tree/main"
      ],
      "metadata": {
        "id": "P7_sHAdNTPI0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQrtScsOTmAs"
      },
      "source": [
        "\n",
        "<h3 align=\"center\"></h3>\n",
        "\n",
        "\n",
        "<h3 align=\"center\">---Son---</h3>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}